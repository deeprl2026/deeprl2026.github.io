
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="This page provides detailed notes on value-based methods in reinforcement learning, including Bellman equations, dynamic programming, Monte Carlo methods, and temporal difference learning.">
      
      
      
        <link rel="canonical" href="https://DeepRL2026.github.io/course_notes/value-based2/">
      
      
      
      
        
      
      
      <link rel="icon" href="../../assets/favicon/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.3">
    
    
      
        <title>Week 7: Value-based Methods - Deep RL Course</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300,300i,400,400i,700,700i%7CRed+Hat+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Ubuntu";--md-code-font:"Red Hat Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
      <link rel="stylesheet" href="../../stylesheets/vazirmatn.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-Q1YT8K39WE"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-Q1YT8K39WE",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-Q1YT8K39WE",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
  
<meta property="og:type" content="website" />
<meta property="og:title" content="Week 7: Value-based Methods - Deep RL Course" />
<meta property="og:description" content="This page provides detailed notes on value-based methods in reinforcement learning, including Bellman equations, dynamic programming, Monte Carlo methods, and temporal difference learning." />
<meta property="og:image" content="https://DeepRL2026.github.io/assets/images/social/course_notes/value-based2.png" />
<meta property="og:image:type" content="image/png" />
<meta property="og:image:width" content="1200" />
<meta property="og:image:height" content="630" />
<meta property="og:url" content="https://DeepRL2026.github.io/course_notes/value-based2/" />
<meta property="twitter:card" content="summary_large_image" />
<meta property="twitter:title" content="Week 7: Value-based Methods - Deep RL Course" />
<meta property="twitter:description" content="This page provides detailed notes on value-based methods in reinforcement learning, including Bellman equations, dynamic programming, Monte Carlo methods, and temporal difference learning." />
<meta property="twitter:image" content="https://DeepRL2026.github.io/assets/images/social/course_notes/value-based2.png" />
</head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#week-7-value-based-methods" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Deep RL Course" class="md-header__button md-logo" aria-label="Deep RL Course" data-md-component="logo">
      
  <img src="../../assets/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Deep RL Course
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Week 7: Value-based Methods
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="red"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="red"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  Home

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  Course Notes

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../lectures/lecture1/" class="md-tabs__link">
          
  
  
  Lectures

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  TA Sessions

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  Homeworks

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  Projects

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../exams/" class="md-tabs__link">
          
  
  
  Exams

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../resources/" class="md-tabs__link">
          
  
  
  Resources

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
    
      

    

    

        

            
                  
            

        

            

        
    

    


    <!-- Navigation -->
    
        
        <div
            class="md-sidebar md-sidebar--primary"
            data-md-component="sidebar"
            data-md-type="navigation"
            
        >
            <div class="md-sidebar__scrollwrap">
            <div class="md-sidebar__inner">
                <!--
  Copyright (c) 2016-2025 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->



<!-- Determine classes -->


  


  


<!-- Navigation -->
<nav
  class="md-nav md-nav--primary md-nav--lifted"
  aria-label="Navigation"
  data-md-level="0"
>

  <!-- Site title -->
  <label class="md-nav__title" for="__drawer">
    <a
      href="../.."
      title="Deep RL Course"
      class="md-nav__button md-logo"
      aria-label="Deep RL Course"
      data-md-component="logo"
    >
      
  <img src="../../assets/logo.png" alt="logo">

    </a>
    Deep RL Course
  </label>

  <!-- Repository information -->
  

  <!-- Navigation list -->
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Home
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Course Notes
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Course Notes
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Lectures
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Lectures
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../lectures/lecture1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Lecture1
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../lectures/lecture2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Lecture2
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    TA Sessions
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    TA Sessions
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Homeworks
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    Homeworks
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Projects
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    Projects
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Exams
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            
  
    Exams
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../exams/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Previous Semesters
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Resources
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            
  
    Resources
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../resources/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Resources
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
            </div>
            </div>
        </div>
    

    <!-- Table of contents -->
    
        
        <div
            class="md-sidebar md-sidebar--secondary"
            data-md-component="sidebar"
            data-md-type="toc"
            
        >
            <div class="md-sidebar__scrollwrap">
            <div class="md-sidebar__inner">
                <!--
  Copyright (c) 2016-2025 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->

<!-- Determine title -->




<!-- Table of contents -->
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  

  <!--
    Check whether the content starts with a level 1 headline. If it does, the
    top-level anchor must be skipped, since it would be redundant to the link
    to the current page that is located just above the anchor. Therefore we
    directly continue with the children of the anchor.
  -->
  
  
    
  

  <!-- Table of contents title and list -->
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-bellman-equations-and-value-functions" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. Bellman Equations and Value Functions
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. Bellman Equations and Value Functions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-state-value-function-vs" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.1. State Value Function \(V(s)\)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1.1. State Value Function \(V(s)\)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#definition" class="md-nav__link">
    <span class="md-ellipsis">
      
        Definition:
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bellman-expectation-equation-for-vpis" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bellman Expectation Equation for \(V^\pi(s)\):
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-action-value-function-qs-a" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.2. Action Value Function \(Q(s, a)\)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1.2. Action Value Function \(Q(s, a)\)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#definition_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Definition:
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bellman-expectation-equation-for-qpis-a" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bellman Expectation Equation for \(Q^\pi(s, a)\):
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bellman-optimality-equation-for-qs-a" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bellman Optimality Equation for \(Q^*(s, a)\):
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-dynamic-programming" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Dynamic Programming
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#21-value-iteration" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.1. Value Iteration
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.1. Value Iteration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bellman-optimality-equation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bellman Optimality Equation:
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#value-iteration-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      
        Value Iteration Algorithm:
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convergence" class="md-nav__link">
    <span class="md-ellipsis">
      
        Convergence:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-policy-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.2. Policy Evaluation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-policy-improvement" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.3. Policy Improvement
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#24-policy-iteration" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.4. Policy Iteration
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.4. Policy Iteration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#convergence_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Convergence:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#25-comparison-of-policy-iteration-and-value-iteration" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.5. Comparison of Policy Iteration and Value Iteration
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-monte-carlo-methods" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Monte Carlo Methods
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Monte Carlo Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-planning-vs-learning-in-rl" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.1. Planning vs. Learning in RL
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3.1. Planning vs. Learning in RL">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#planning-model-based-rl" class="md-nav__link">
    <span class="md-ellipsis">
      
        Planning (Model-Based RL)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-model-free-rl" class="md-nav__link">
    <span class="md-ellipsis">
      
        Learning (Model-Free RL)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-introduction-to-monte-carlo" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.2. Introduction to Monte Carlo
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3.2. Introduction to Monte Carlo">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-estimating-pi" class="md-nav__link">
    <span class="md-ellipsis">
      
        Example: Estimating \(\pi\)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Example: Estimating \(\pi\)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#steps" class="md-nav__link">
    <span class="md-ellipsis">
      
        Steps:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-integration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Example: Integration
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-monte-carlo-prediction" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.3. Monte Carlo Prediction
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3.3. Monte Carlo Prediction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#estimating-vpis" class="md-nav__link">
    <span class="md-ellipsis">
      
        Estimating \(V^\pi(s)\)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34-monte-carlo-control" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.4. Monte Carlo Control
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3.4. Monte Carlo Control">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      
        Algorithm:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#35-first-visit-vs-every-visit-monte-carlo" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.5. First-Visit vs. Every-Visit Monte Carlo
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3.5. First-Visit vs. Every-Visit Monte Carlo">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#first-visit-monte-carlo" class="md-nav__link">
    <span class="md-ellipsis">
      
        First-Visit Monte Carlo:
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="First-Visit Monte Carlo:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#algorithm_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Algorithm:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#every-visit-monte-carlo" class="md-nav__link">
    <span class="md-ellipsis">
      
        Every-Visit Monte Carlo:
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Every-Visit Monte Carlo:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#algorithm_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Algorithm:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#comparison" class="md-nav__link">
    <span class="md-ellipsis">
      
        Comparison:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#36-incremental-monte-carlo-policy" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.6. Incremental Monte Carlo Policy
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-temporal-difference-td-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Temporal Difference (TD) Learning
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. Temporal Difference (TD) Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-td-prediction" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.1. TD Prediction
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43-on-policy-vs-off-policy-td-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.3. On-Policy vs. Off-Policy TD Learning
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4.3. On-Policy vs. Off-Policy TD Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sarsa-algorithm-on-policy" class="md-nav__link">
    <span class="md-ellipsis">
      
        SARSA Algorithm (On-Policy)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q-learning-algorithm-off-policy" class="md-nav__link">
    <span class="md-ellipsis">
      
        Q-Learning Algorithm (Off-Policy)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#44-exploitation-vs-exploration" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.4. Exploitation vs Exploration
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4.4. Exploitation vs Exploration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#balancing-exploration-and-exploitation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Balancing Exploration and Exploitation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-summary-of-key-concepts-and-methods" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. Summary of Key Concepts and Methods
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Summary of Key Concepts and Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-model-based-vs-model-free-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.1. Model-Based vs. Model-Free Learning
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.1. Model-Based vs. Model-Free Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#model-based-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Model-Based Learning:
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-free-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Model-Free Learning:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52-on-policy-vs-off-policy-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.2. On-Policy vs. Off-Policy Learning
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.2. On-Policy vs. Off-Policy Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#on-policy-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        On-Policy Learning:
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#off-policy-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Off-Policy Learning:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#53-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.3. Comparison
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      
        References
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
            </div>
            </div>
        </div>
    

          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="week-7-value-based-methods">Week 7: Value-based Methods</h1>
<h2 id="1-bellman-equations-and-value-functions">1. Bellman Equations and Value Functions</h2>
<h3 id="11-state-value-function-vs">1.1. State Value Function <span class="arithmatex">\(V(s)\)</span></h3>
<h4 id="definition"><strong>Definition:</strong></h4>
<p>The <strong>state value function</strong> <span class="arithmatex">\(V^\pi(s)\)</span> measures the expected return when an agent starts in state <span class="arithmatex">\(s\)</span> and follows a policy <span class="arithmatex">\(\pi\)</span>. It provides a scalar value for each state that reflects the desirability of that state under the given policy. Formally, it is defined as:</p>
<div class="arithmatex">\[
V^\pi(s) = \mathbb{E} \left[ G_t \mid s_t = s \right]
\]</div>
<p>Where <span class="arithmatex">\(G_t\)</span> represents the return (total reward) from time step <span class="arithmatex">\(t\)</span> onwards:</p>
<div class="arithmatex">\[
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots
\]</div>
<ul>
<li><span class="arithmatex">\(R_t\)</span> is the reward received at time step <span class="arithmatex">\(t\)</span>.</li>
<li><span class="arithmatex">\(\gamma\)</span> is the discount factor (<span class="arithmatex">\(0 \leq \gamma \leq 1\)</span>), controlling how much future rewards are valued compared to immediate rewards.</li>
</ul>
<h4 id="bellman-expectation-equation-for-vpis"><strong>Bellman Expectation Equation for <span class="arithmatex">\(V^\pi(s)\)</span>:</strong></h4>
<p>The Bellman Expectation Equation for the state value function expresses the value of a state <span class="arithmatex">\(s\)</span> in terms of the expected immediate reward and the discounted value of the next state. It is written as:</p>
<div class="arithmatex">\[
V^\pi(s) = \mathbb{E} \left[ R_{t+1} + \gamma V^\pi(s_{t+1}) \mid s_t = s \right]
\]</div>
<p>Using the transition probabilities of the environment, this can be expanded as:</p>
<div class="arithmatex">\[
V^\pi(s) = \sum_{s'} P(s'|s, \pi(s)) \left[ R(s, \pi(s), s') + \gamma V^\pi(s') \right]
\]</div>
<p>Where:
- <span class="arithmatex">\(P(s'|s, \pi(s))\)</span> is the probability of transitioning from state <span class="arithmatex">\(s\)</span> to state <span class="arithmatex">\(s'\)</span> when following action <span class="arithmatex">\(\pi(s)\)</span>.
- <span class="arithmatex">\(R(s, \pi(s), s')\)</span> is the reward for transitioning from state <span class="arithmatex">\(s\)</span> to <span class="arithmatex">\(s'\)</span> under action <span class="arithmatex">\(\pi(s)\)</span>.</p>
<p>This equation allows for the iterative computation of state values in a model-based setting.</p>
<hr />
<h3 id="12-action-value-function-qs-a">1.2. Action Value Function <span class="arithmatex">\(Q(s, a)\)</span></h3>
<h4 id="definition_1"><strong>Definition:</strong></h4>
<p>The <strong>action value function</strong> <span class="arithmatex">\(Q^\pi(s, a)\)</span> represents the expected return when an agent starts in state <span class="arithmatex">\(s\)</span>, takes action <span class="arithmatex">\(a\)</span>, and then follows policy <span class="arithmatex">\(\pi\)</span>:</p>
<div class="arithmatex">\[
Q^\pi(s, a) = \mathbb{E} \left[ G_t \mid s_t = s, a_t = a \right]
\]</div>
<p>Where <span class="arithmatex">\(G_t\)</span> is the return starting at time <span class="arithmatex">\(t\)</span>.</p>
<h4 id="bellman-expectation-equation-for-qpis-a">Bellman Expectation Equation for <span class="arithmatex">\(Q^\pi(s, a)\)</span>:</h4>
<p>The Bellman Expectation Equation for the action value function is similar to the one for the state value function but includes both the action and the subsequent states and actions. It is given by:</p>
<div class="arithmatex">\[
Q^\pi(s, a) = \mathbb{E} \left[ R_{t+1} + \gamma Q^\pi(s_{t+1}, a_{t+1}) \mid s_t = s, a_t = a \right]
\]</div>
<p>Expanding this into a sum over possible next states, we get:</p>
<div class="arithmatex">\[
Q^\pi(s, a) = \sum_{s'} P(s'|s, a) \left[ R(s, a, s') + \gamma \sum_{a'} \pi(a'|s') Q^\pi(s', a') \right]
\]</div>
<p>Where:
- <span class="arithmatex">\(P(s'|s, a)\)</span> is the transition probability from state <span class="arithmatex">\(s\)</span> to state <span class="arithmatex">\(s'\)</span> under action <span class="arithmatex">\(a\)</span>.
- <span class="arithmatex">\(\pi(a'|s')\)</span> is the probability of taking action <span class="arithmatex">\(a'\)</span> in state <span class="arithmatex">\(s'\)</span> under policy <span class="arithmatex">\(\pi\)</span>.</p>
<h4 id="bellman-optimality-equation-for-qs-a">Bellman Optimality Equation for <span class="arithmatex">\(Q^*(s, a)\)</span>:</h4>
<p>The <strong>Bellman Optimality Equation</strong> for <span class="arithmatex">\(Q^*(s, a)\)</span> expresses the optimal action value function. It is given by:</p>
<div class="arithmatex">\[
Q^*(s, a) = \mathbb{E} \left[ R_{t+1} + \gamma \max_{a'} Q^*(s_{t+1}, a') \mid s_t = s, a_t = a \right]
\]</div>
<p>This shows that the optimal action value at each state-action pair is the immediate reward plus the discounted maximum expected value from the next state, where the next action is chosen optimally.</p>
<hr />
<h3 id="2-dynamic-programming">2. Dynamic Programming</h3>
<p>Dynamic Programming (DP) is a powerful technique used to solve reinforcement learning problems where the environment is fully known (i.e., the model is available). DP algorithms compute the optimal policy and value functions by iteratively updating estimates based on a model of the environment. </p>
<h3 id="21-value-iteration">2.1. Value Iteration</h3>
<h5 id="bellman-optimality-equation">Bellman Optimality Equation:</h5>
<p>The Bellman Optimality Equation for the value function is:</p>
<div class="arithmatex">\[
V_{k+1}(s) = \max_a \left[ R(s, a) + \gamma \sum_{s'} P(s'|s, a) V_k(s') \right]
\]</div>
<p>Where:
- <span class="arithmatex">\(\max_a\)</span> selects the action <span class="arithmatex">\(a\)</span> that maximizes the expected return from state <span class="arithmatex">\(s\)</span>.</p>
<h5 id="value-iteration-algorithm">Value Iteration Algorithm:</h5>
<ol>
<li><strong>Initialize</strong> the value function <span class="arithmatex">\(V_0(s)\)</span> arbitrarily.</li>
<li>
<p><strong>Repeat</strong> until convergence:</p>
<ul>
<li>For each state <span class="arithmatex">\(s\)</span>, update the value function:</li>
</ul>
<div class="arithmatex">\[
V_{k+1}(s) = \max_a \left[ R(s, a) + \gamma \sum_{s'} P(s'|s, a) V_k(s') \right]
\]</div>
</li>
<li>
<p>Once the value function converges, the optimal policy <span class="arithmatex">\(\pi^*(s)\)</span> can be derived by selecting the action that maximizes the expected return:</p>
</li>
</ol>
<div class="arithmatex">\[
\pi^*(s) = \arg\max_a \left[ R(s, a) + \gamma \sum_{s'} P(s'|s, a) V^*(s') \right]
\]</div>
<h5 id="convergence">Convergence:</h5>
<p>Value Iteration is guaranteed to converge to the optimal value function and policy. The number of iterations required depends on the problem's dynamics, but it typically converges faster than Policy Iteration in terms of the number of iterations, though it may require more computation per iteration.</p>
<hr />
<h3 id="22-policy-evaluation">2.2. Policy Evaluation</h3>
<p>Policy Evaluation calculates the state value function <span class="arithmatex">\(V^\pi(s)\)</span> for a given policy <span class="arithmatex">\(\pi\)</span> by iteratively updating the value function using the Bellman Expectation Equation:</p>
<div class="arithmatex">\[
V^\pi(s) = R(s, \pi(s)) + \gamma \sum_{s'} P(s'|s, \pi(s)) V^\pi(s')
\]</div>
<p>This process is repeated until <span class="arithmatex">\(V^\pi(s)\)</span> converges to a fixed point for all s.</p>
<hr />
<h3 id="23-policy-improvement">2.3. Policy Improvement</h3>
<p>Policy Improvement refines a policy <span class="arithmatex">\(\pi\)</span> by making it greedy with respect to the current value function:</p>
<div class="arithmatex">\[
\pi'(s) = \arg\max_a \left[ R(s, a) + \gamma \sum_{s'} P(s'|s, a) V^\pi(s') \right]
\]</div>
<p>It is proven that the new policys value function is at least as good as the previous one:</p>
<div class="arithmatex">\[
V^{\pi'}(s) \geq V^\pi(s), \quad \forall s.
\]</div>
<p>By repeating policy evaluation and improvement, the policy converges to the optimal one.</p>
<ol>
<li>
<p><strong>Single-Step Improvement:</strong>  </p>
<ul>
<li>Modify the policy at <strong>only</strong> <span class="arithmatex">\(t = 0\)</span>, keeping the rest unchanged.  </li>
<li>This new policy achieves a higher or equal value:  </li>
</ul>
<div class="arithmatex">\[
V^{\pi_{(k+1)}^{(1)}}(s) \geq V^{\pi_k}(s), \quad \forall s.
\]</div>
</li>
<li>
<p><strong>Extending to Multiple Steps:</strong>  </p>
<ul>
<li>Modify the policy at <strong><span class="arithmatex">\(t = 0\)</span> and <span class="arithmatex">\(t = 1\)</span></strong>, keeping the rest unchanged.  </li>
<li>Again, the value function improves: </li>
</ul>
<div class="arithmatex">\[
V^{\pi_{(k+1)}^{(2)}}(s) \geq V^{\pi_{(k+1)}^{(1)}}(s) \geq V^{\pi_k}(s).
\]</div>
</li>
<li>
<p><strong>Repeating for All Steps:</strong>  </p>
<ul>
<li>After applying this to all time steps, the final policy matches the fully improved one:  </li>
</ul>
<div class="arithmatex">\[
\pi_{(k+1)}^{(\infty)}(s) = \pi_{k+1}(s).
\]</div>
<ul>
<li>This ensures:  </li>
</ul>
<div class="arithmatex">\[
V^{\pi_{k+1}}(s) \geq V^{\pi_k}(s), \quad \forall s.
\]</div>
</li>
</ol>
<p>The value function <strong>never decreases</strong> with each update.  </p>
<hr />
<h3 id="24-policy-iteration">2.4. Policy Iteration</h3>
<p>Policy Iteration alternates between <strong>policy evaluation</strong> and <strong>policy improvement</strong> to compute the optimal policy.</p>
<ol>
<li><strong>Initialize policy <span class="arithmatex">\(\pi_0\)</span></strong> randomly.</li>
<li><strong>Policy Evaluation:</strong> Compute the value function <span class="arithmatex">\(V^{\pi_k}(s)\)</span> for the current policy <span class="arithmatex">\(\pi_k\)</span> using the Bellman Expectation Equation.</li>
<li><strong>Policy Improvement:</strong> Update the policy <span class="arithmatex">\(\pi_{k+1}(s)\)</span> by making it greedy with respect to the current value function:</li>
</ol>
<div class="arithmatex">\[ 
\pi_{k+1}(s) = \arg\max_a \left[ R(s, a) + \gamma \sum_{s'} P(s'|s, a) V^{\pi_k}(s') \right] 
\]</div>
<ol>
<li>Repeat the above steps until the policy converges (i.e., <span class="arithmatex">\(\pi_k = \pi_{k+1}\)</span>).</li>
</ol>
<h5 id="convergence_1">Convergence:</h5>
<p>Each policy update ensures that the value function <strong>does not decrease</strong>.</p>
<p>Since there are only a <strong>finite number of deterministic policies</strong> in a finite Markov Decision Process (MDP), the sequence of improving policies must eventually reach an policy <span class="arithmatex">\(\pi^*\)</span>, where further improvement do not change it.</p>
<p>The value function of the fixed point <span class="arithmatex">\(\pi^*\)</span> satisfy the <strong>Bellman Optimality Equation</strong>:</p>
<div class="arithmatex">\[
V^*(s) = \max_a \left[ R(s, a) + \gamma \sum_{s'} P(s'|s, a) V^*(s') \right]
\]</div>
<p>This confirms that the final policy <span class="arithmatex">\(\pi^*\)</span> is optimal.</p>
<hr />
<h3 id="25-comparison-of-policy-iteration-and-value-iteration">2.5. Comparison of Policy Iteration and Value Iteration</h3>
<p>Policy Iteration and Value Iteration are two dynamic programming methods for finding the optimal policy in an MDP. Both rely on iterative updates but differ in <strong>efficiency</strong> and <strong>computation</strong>.</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Policy Iteration</th>
<th>Value Iteration</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Update Method</strong></td>
<td>Alternates between policy evaluation and improvement</td>
<td>Updates value function directly</td>
</tr>
<tr>
<td><strong>Computational Cost Per Iteration</strong></td>
<td><span class="arithmatex">\(O(\|S\|^3)\)</span> (solving linear equations)</td>
<td><span class="arithmatex">\(O(\|S\| \cdot \|A\|)\)</span> (maximization over actions)</td>
</tr>
<tr>
<td><strong>Number of Iterations</strong></td>
<td>Fewer iterations, but each is expensive</td>
<td>More iterations, but each is cheaper</td>
</tr>
<tr>
<td><strong>Best for</strong></td>
<td>Small state spaces, deterministic transitions</td>
<td>Large state spaces, stochastic transitions</td>
</tr>
</tbody>
</table>
<p><strong>Policy Iteration</strong> explicitly computes the value function for a given policy, requiring <strong>solving a system of equations</strong>. Each iteration is <strong>computationally expensive</strong> but results in a significant improvement, leading to <strong>faster convergence</strong> in terms of iterations.</p>
<p><strong>Value Iteration</strong> avoids solving a system of equations by updating the value function <strong>incrementally</strong>. Each iteration is computationally <strong>cheaper</strong>, but because the value function is updated gradually, <strong>more iterations</strong> are needed for convergence.</p>
<p>Thus, <strong>Policy Iteration takes fewer iterations but is computationally heavy per step, while Value Iteration takes more iterations but is computationally lighter per step</strong>.</p>
<p><a href="https://www.youtube.com/watch?v=sJIFUTITfBc">Watch on YouTube</a></p>
<hr />
<h2 id="3-monte-carlo-methods">3. Monte Carlo Methods</h2>
<h3 id="31-planning-vs-learning-in-rl">3.1. Planning vs. Learning in RL</h3>
<p>Reinforcement learning can be approached in two ways: <strong>planning</strong> and <strong>learning</strong>. The main difference is that <strong>planning relies on a model of the environment</strong>, while <strong>learning uses real-world interactions</strong> to improve decision-making.</p>
<h5 id="planning-model-based-rl">Planning (Model-Based RL)</h5>
<ul>
<li>Uses a model to predict state transitions and rewards.</li>
<li>The agent can simulate future actions without interacting with the environment.</li>
<li>Examples: <strong>Dynamic Programming (DP), Monte Carlo Tree Search (MCTS).</strong></li>
</ul>
<h5 id="learning-model-free-rl">Learning (Model-Free RL)</h5>
<ul>
<li>No access to a model; the agent learns by interacting with the environment.</li>
<li>The agent updates value estimates based on observed rewards.</li>
<li>Examples: <strong>Monte Carlo, Temporal Difference (TD), Q-Learning.</strong></li>
</ul>
<p>Planning is efficient when a reliable model is available, but learning is necessary when the model is unknown or too complex to compute.</p>
<p>Monte Carlo methods fall under <strong>Model-Free RL</strong>, where the agent improves through experience. The following sections introduce how <strong>Monte Carlo Sampling</strong> is used to estimate value functions without needing a model.</p>
<hr />
<h3 id="32-introduction-to-monte-carlo">3.2. Introduction to Monte Carlo</h3>
<p>Monte Carlo methods use <strong>random sampling</strong> to estimate numerical results, especially when direct computation is infeasible or the underlying distribution is unknown. These methods are widely applied in <strong>physics, finance, optimization, and reinforcement learning</strong>.</p>
<p>Monte Carlo estimates an expectation:</p>
<div class="arithmatex">\[
I = \mathbb{E}[f(X)] = \int f(x) p(x) dx
\]</div>
<p>using <strong>sample averaging</strong>:</p>
<div class="arithmatex">\[
\hat{I}_N = \frac{1}{N} \sum_{i=1}^{N} f(x_i),
\]</div>
<p>where $ x_i $ are <strong>independent</strong> samples drawn from $ p(x) $. The <strong>Law of Large Numbers (LLN)</strong> ensures that as <span class="arithmatex">\(N \to \infty\)</span>:</p>
<div class="arithmatex">\[
\hat{I}_N \to I.
\]</div>
<p>This guarantees <strong>convergence</strong>, but the <strong>speed of convergence</strong> depends on the variance of the samples.</p>
<p>Monte Carlo estimates become more accurate as <span class="arithmatex">\(N\)</span> increases, but <strong>independent samples</strong> are crucial for unbiased estimation.</p>
<p>By the <strong>Central Limit Theorem (CLT)</strong>, for large <span class="arithmatex">\(N\)</span>, the Monte Carlo estimate follows a normal distribution:</p>
<div class="arithmatex">\[
\hat{I}_N \approx \mathcal{N} \left(I, \frac{\sigma^2}{N} \right).
\]</div>
<p>This shows that the <strong>variance decreases at a rate of</strong> <span class="arithmatex">\(O(1/N)\)</span>, meaning that as the number of independent samples increases, the estimate becomes more stable. However, this reduction is slow, requiring a large number of samples to achieve high precision.</p>
<hr />
<h4 id="example-estimating-pi">Example: Estimating <span class="arithmatex">\(\pi\)</span></h4>
<p>Monte Carlo methods can estimate <strong><span class="arithmatex">\(\pi\)</span></strong> by randomly sampling points and analyzing their distribution relative to a known geometric shape.</p>
<h5 id="steps">Steps:</h5>
<ol>
<li>Generate <span class="arithmatex">\(N\)</span> random points <span class="arithmatex">\((x, y)\)</span> where <span class="arithmatex">\(x, y \sim U(-1,1)\)</span>, meaning they are uniformly sampled in the square <span class="arithmatex">\([-1,1] \times [-1,1]\)</span>.</li>
<li>Define an <strong>indicator function</strong> <span class="arithmatex">\(I(x, y)\)</span> that takes the value:</li>
</ol>
<div class="arithmatex">\[
I(x, y) =
\begin{cases}
1, &amp; \text{if } x^2 + y^2 \leq 1 \quad \text{(inside the circle)} \\
0, &amp; \text{otherwise}.
\end{cases}
\]</div>
<p>Since each point is either inside or outside the circle, the variable <span class="arithmatex">\(I(x, y)\)</span> follows a <strong>Bernoulli distribution</strong> with probability <span class="arithmatex">\(p = \frac{\pi}{4}\)</span>.</p>
<ol>
<li>Compute the proportion of points inside the circle. The expectation of <span class="arithmatex">\(I(x, y)\)</span> gives:</li>
</ol>
<div class="arithmatex">\[
\mathbb{E}[I] = P(I = 1) = \frac{\pi}{4}.
\]</div>
<p>By the <strong>Law of Large Numbers (LLN)</strong>, the sample mean of <span class="arithmatex">\(I(x, y)\)</span> over <span class="arithmatex">\(N\)</span> points converges to this expected value:</p>
<div class="arithmatex">\[
\frac{\text{Points inside the circle}}{\text{Total points}} \approx \frac{\pi}{4}.
\]</div>
<center> 
<img src="\assets\images\course_notes\value-based\pi-estimation.png"
    alt="pi estimation with monte carlo"
    style="float: center; margin-right: 10px;" 
    /> 
    </center>

<hr />
<h4 id="example-integration">Example: Integration</h4>
<p>Monte Carlo methods can also estimate definite integrals using random sampling. Given an integral:</p>
<div class="arithmatex">\[
I = \int_a^b f(x) dx,
\]</div>
<p>we approximate it using Monte Carlo sampling:</p>
<div class="arithmatex">\[
\hat{I}_N = \frac{b-a}{N} \sum_{i=1}^{N} f(x_i),
\]</div>
<p>where <span class="arithmatex">\(x_i\)</span> are sampled uniformly from <span class="arithmatex">\([a, b]\)</span>. By the <strong>LLN</strong>, as <span class="arithmatex">\(N \to \infty\)</span>, the estimate <span class="arithmatex">\(\hat{I}_N\)</span> converges to the true integral.</p>
<center> 
<img src="\assets\images\course_notes\value-based\integral.png"
    alt="integration with monte carlo"
    style="float: center; margin-right: 10px;" 
    /> 
    </center>

<p><a href="https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/monte-carlo-methods-in-practice/monte-carlo-integration.html">Source</a></p>
<p><a href="https://www.youtube.com/watch?v=7TqhmX92P6U&amp;t=173s">Watch on YouTube</a></p>
<hr />
<h3 id="33-monte-carlo-prediction">3.3. Monte Carlo Prediction</h3>
<p>In reinforcement learning, an <strong>episode</strong> is a sequence of states, actions, and rewards that starts from an <strong>initial state</strong> and ends in a <strong>terminal state</strong>. Each episode represents a <strong>complete trajectory</strong> of the agents interaction with the environment.</p>
<p>The <strong>return</strong> for a time step <span class="arithmatex">\(t\)</span> in an episode is the <strong>cumulative discounted reward</strong>:</p>
<div class="arithmatex">\[
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots
\]</div>
<h4 id="estimating-vpis">Estimating <span class="arithmatex">\(V^\pi(s)\)</span></h4>
<p>Monte Carlo methods estimate the <strong>state value function</strong> <span class="arithmatex">\(V^\pi(s)\)</span> by <strong>averaging the returns</strong> observed after visiting state <span class="arithmatex">\(s\)</span> in multiple episodes.</p>
<p>The estimate of <span class="arithmatex">\(V^\pi(s)\)</span> is:</p>
<div class="arithmatex">\[
V^\pi(s) = \frac{1}{N(s)} \sum_{i=1}^{N(s)} G_i
\]</div>
<p>where:
- <span class="arithmatex">\(N(s)\)</span> is the number of times state <span class="arithmatex">\(s\)</span> has been visited.
- <span class="arithmatex">\(G_i\)</span> is the return observed from the <span class="arithmatex">\(i\)</span>-th visit to state <span class="arithmatex">\(s\)</span>.</p>
<p>Since Monte Carlo methods rely entirely on <strong>sampled episodes</strong>, they do <strong>not require knowledge of transition probabilities or rewards</strong> so they learn directly from experience.</p>
<hr />
<h3 id="34-monte-carlo-control">3.4. Monte Carlo Control</h3>
<p>In <strong>Monte Carlo Control</strong>, the goal is to improve the policy <span class="arithmatex">\(\pi\)</span> by optimizing it based on the action-value function <span class="arithmatex">\(Q^\pi(s, a)\)</span>. </p>
<h4 id="algorithm"><strong>Algorithm:</strong></h4>
<ol>
<li><strong>Generate Episodes</strong>: Generate episodes by interacting with the environment under the current policy <span class="arithmatex">\(\pi\)</span>.</li>
<li><strong>Compute Returns</strong>: For each state-action pair <span class="arithmatex">\((s, a)\)</span> in the episode, compute the return <span class="arithmatex">\(G_t\)</span> from that time step onward.</li>
<li><strong>Update Action-Value Function</strong>: For each state-action pair, update the action-value function as:</li>
</ol>
<div class="arithmatex">\[ 
Q^\pi(s, a) = \frac{1}{N(s, a)} \sum_{i=1}^{N(s, a)} G_i \]</div>
<p>Where <span class="arithmatex">\(N(s, a)\)</span> is the number of times the state-action pair <span class="arithmatex">\((s, a)\)</span> has been visited.
4. <strong>Policy Improvement</strong>: After updating the action-value function, improve the policy by selecting the action that maximizes <span class="arithmatex">\(Q^\pi(s, a)\)</span> for each state <span class="arithmatex">\(s\)</span>:</p>
<div class="arithmatex">\[ 
\pi'(s) = \arg\max_a Q^\pi(s, a) 
\]</div>
<p>This method is used to optimize the policy iteratively, improving it by making the policy greedy with respect to the current action-value function.</p>
<hr />
<h3 id="35-first-visit-vs-every-visit-monte-carlo">3.5. First-Visit vs. Every-Visit Monte Carlo</h3>
<p>There are two main variations of Monte Carlo methods for estimating value functions: <strong>First-Visit Monte Carlo</strong> and <strong>Every-Visit Monte Carlo</strong>.</p>
<h4 id="first-visit-monte-carlo">First-Visit Monte Carlo:</h4>
<p>In <strong>First-Visit Monte Carlo</strong>, the return for a state is only updated the first time it is visited in an episode. This approach helps avoid over-counting and ensures that the value estimate for each state is updated only once per episode.</p>
<h5 id="algorithm_1">Algorithm:</h5>
<ol>
<li>Initialize <span class="arithmatex">\(N(s) = 0\)</span> and <span class="arithmatex">\(G(s) = 0\)</span> for all states.</li>
<li>For each episode, visit each state <span class="arithmatex">\(s\)</span> for the first time, and when it is first visited, add the return <span class="arithmatex">\(G_t\)</span> to <span class="arithmatex">\(G(s)\)</span> and increment <span class="arithmatex">\(N(s)\)</span>.</li>
<li>After the episode, update the value estimate for each state as:  </li>
</ol>
<div class="arithmatex">\[
V^\pi(s) = \frac{G(s)}{N(s)}
\]</div>
<h4 id="every-visit-monte-carlo">Every-Visit Monte Carlo:</h4>
<p>In <strong>Every-Visit Monte Carlo</strong>, the return for each state is updated every time it is visited in an episode. This approach uses all occurrences of a state to update its value function, which can sometimes lead to more stable estimates.</p>
<h5 id="algorithm_2">Algorithm:</h5>
<ol>
<li>Initialize <span class="arithmatex">\(N(s) = 0\)</span> and <span class="arithmatex">\(G(s) = 0\)</span> for all states.</li>
<li>For each episode, visit each state <span class="arithmatex">\(s\)</span>, and every time it is visited, add the return <span class="arithmatex">\(G_t\)</span> to <span class="arithmatex">\(G(s)\)</span> and increment <span class="arithmatex">\(N(s)\)</span>.</li>
<li>After the episode, update the value estimate for each state as:</li>
</ol>
<div class="arithmatex">\[
V^\pi(s) = \frac{G(s)}{N(s)}
\]</div>
<h4 id="comparison">Comparison:</h4>
<p>First-Visit Monte Carlo updates the value function only the first time a state is encountered in an episode, ensuring an <strong>unbiased</strong> estimate but using <strong>fewer samples</strong>, which can result in <strong>higher variance</strong> and <strong>slower</strong> learning. In contrast, Every-Visit Monte Carlo updates the value function on all occurrences of a state within an episode, <strong>reducing variance</strong> and improving <strong>sample efficiency</strong> by utilizing more data. Although it may introduce <strong>bias</strong>, it often converges faster, making it more practical in many applications.</p>
<hr />
<h3 id="36-incremental-monte-carlo-policy">3.6. Incremental Monte Carlo Policy</h3>
<p>In addition to First-Visit and Every-Visit Monte Carlo, an alternative approach is the <strong>Incremental Monte Carlo Policy</strong>, which updates the value function <strong>incrementally after each visit</strong> instead of computing an average over all episodes. This method is <strong>more memory-efficient</strong> and allows real-time updates without storing past returns.</p>
<p>Given the return <span class="arithmatex">\(G_{i,t}\)</span> observed for state <span class="arithmatex">\(s\)</span> at time <span class="arithmatex">\(t\)</span> in episode <span class="arithmatex">\(i\)</span>, we update the value function as:</p>
<div class="arithmatex">\[
V^\pi(s) = V^\pi(s) \frac{N(s) - 1}{N(s)} + \frac{G_{i,t}}{N(s)}
\]</div>
<p>which can be rewritten as:</p>
<div class="arithmatex">\[
V^\pi(s) = V^\pi(s) + \frac{1}{N(s)} (G_{i,t} - V^\pi(s))
\]</div>
<ul>
<li>The update formula behaves like a <strong>running average</strong>, gradually incorporating new information.</li>
</ul>
<p>This approach ensures <strong>smooth updates</strong>, avoids storing all past returns, and is <strong>more computationally efficient</strong>, especially in long episodes or large state spaces.</p>
<hr />
<h2 id="4-temporal-difference-td-learning">4. Temporal Difference (TD) Learning</h2>
<p>Temporal Difference (TD) Learning is a method for estimating value functions in reinforcement learning. </p>
<h3 id="41-td-prediction">4.1. TD Prediction</h3>
<p>TD Learning updates the value function using the Bellman equation. It differs from Monte Carlo methods in that it updates after each step rather than waiting for the entire episode to finish. The general TD update rule for state-value function <span class="arithmatex">\(V^\pi(s_t)\)</span> is:</p>
<div class="arithmatex">\[
V^\pi(s_t) = V^\pi(s_t) + \alpha \left[ R_{t+1} + \gamma V^\pi(s_{t+1}) - V^\pi(s_t) \right]
\]</div>
<p>This approach is called <strong>bootstrapping</strong> since it estimates future rewards based on the current value function rather than waiting for the full return.</p>
<p>Just like for state-value functions, we can extend TD Learning to action-value functions (Q-values). The TD update rule for <span class="arithmatex">\(Q(s_t, a_t)\)</span> is:</p>
<div class="arithmatex">\[
Q(s_t, a_t) = Q(s_t, a_t) + \alpha \left[ R_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]
\]</div>
<p>This allows TD learning to be applied to <strong>control</strong> tasks, where the agent needs to improve its policy while learning. </p>
<h3 id="43-on-policy-vs-off-policy-td-learning">4.3. On-Policy vs. Off-Policy TD Learning</h3>
<p>TD methods can be used for both <strong>on-policy</strong> and <strong>off-policy</strong> learning:</p>
<ul>
<li><strong>SARSA (On-Policy TD Control)</strong>: Updates the action-value function based on the agents actual policy.</li>
<li><strong>Q-Learning (Off-Policy TD Control)</strong>: Updates based on the optimal action, regardless of the agents current policy.</li>
</ul>
<h4 id="sarsa-algorithm-on-policy">SARSA Algorithm (On-Policy)</h4>
<p>In SARSA, the agent chooses the next action <span class="arithmatex">\(a_{t+1}\)</span> according to its current policy and updates the Q-value based on the immediate reward and the Q-value for the next state-action pair.</p>
<pre><code class="language-pseudo">Initialize Q(s, a) arbitrarily, for all s  S, a  A(s), and Q(terminal-state, ) = 0
Repeat (for each episode):
    Initialize S
    Choose A from S using policy derived from Q (e.g., -greedy)
    Repeat (for each step of episode):
         Take action A, observe R and next state S'
         Choose A' from S' using policy derived from Q (e.g., -greedy)

         Update Q-value:
         Q(S, A)  Q(S, A) + [R +  * Q(S', A') - Q(S, A)]

         Set S  S', A  A'

    until S is terminal
</code></pre>
<!--  
![alt text](IMG_20250223_163420.png) -->

<h4 id="q-learning-algorithm-off-policy">Q-Learning Algorithm (Off-Policy)</h4>
<p>Q-learning is an off-policy algorithm that learns the best action-value function, no matter what behavior policy the agent used to gather the data.</p>
<pre><code class="language-pseudo">Initialize Q(s, a) arbitrarily, for all s  S, a  A(s), and Q(terminal-state, ) = 0
Repeat (for each episode):
    Initialize S

    Repeat (for each step of episode):
         Choose A from S using policy derived from Q (e.g., -greedy)

         Take action A, observe R and next state S'

         Update Q-value:
         Q(S, A)  Q(S, A) + [R +  * max_a Q(S', A') - Q(S, A)]

         Set S  S'

    until S is terminal
</code></pre>
<!-- 
![alt text](IMG_20250223_163452-1.png) -->

<h3 id="44-exploitation-vs-exploration">4.4. Exploitation vs Exploration</h3>
<h4 id="balancing-exploration-and-exploitation">Balancing Exploration and Exploitation</h4>
<p>In reinforcement learning, an agent needs to balance <strong>exploration</strong> (trying new actions) and <strong>exploitation</strong> (using known actions that give good rewards). To do this, we use an <strong><span class="arithmatex">\(\epsilon\)</span>-greedy policy</strong>:</p>
<div class="arithmatex">\[
\pi(a_t | s_t) = 
\begin{cases} 
\arg\max_a Q(s_t, a) &amp; \text{with probability } 1 - \epsilon \\
\text{random action} &amp; \text{with probability } \epsilon
\end{cases}
\]</div>
<p>At the start of learning, <strong><span class="arithmatex">\(\epsilon\)</span></strong> is high to encourage exploration. As the agent learns more about the environment, <strong><span class="arithmatex">\(\epsilon\)</span></strong> decreases, allowing the agent to focus more on exploiting the best actions it has learned. This process is called <strong>epsilon decay</strong>.</p>
<p>Common ways to decay <strong><span class="arithmatex">\(\epsilon\)</span></strong> include:</p>
<ul>
<li><strong>Linear Decay</strong>: </li>
</ul>
<div class="arithmatex">\[
\epsilon_t = \frac{1}{t}
\]</div>
<p>where <span class="arithmatex">\(t\)</span> is the time step.</p>
<ul>
<li><strong>Exponential Decay</strong>:</li>
</ul>
<div class="arithmatex">\[
\epsilon_t = \epsilon_0 \cdot \text{decay_rate}^t
\]</div>
<p><a href="https://www.youtube.com/watch?v=0iqz4tcKN58">Watch on YouTube</a></p>
<h2 id="5-summary-of-key-concepts-and-methods">5. Summary of Key Concepts and Methods</h2>
<p>In reinforcement learning, various methods are used to estimate value functions and find optimal policies. These methods can be broadly categorized into <strong>Model-Based</strong> and <strong>Model-Free</strong> learning, as well as <strong>On-Policy</strong> and <strong>Off-Policy</strong> learning. Below is a concise summary of these key concepts and a comparison of different approaches.</p>
<h3 id="51-model-based-vs-model-free-learning">5.1. Model-Based vs. Model-Free Learning</h3>
<h4 id="model-based-learning">Model-Based Learning:</h4>
<ul>
<li><strong>Definition</strong>: The agent uses a model of the environment to predict future states and rewards. The model allows the agent to simulate actions and outcomes.</li>
<li><strong>Example</strong>: <strong>Dynamic Programming (DP)</strong> relies on a complete model of the environment.</li>
<li><strong>Advantages</strong>: Efficient when the model is available and provides exact solutions when the environment is known.</li>
</ul>
<h4 id="model-free-learning">Model-Free Learning:</h4>
<ul>
<li><strong>Definition</strong>: The agent learns directly from interactions with the environment by estimating value functions based on observed rewards, without needing a model.</li>
<li><strong>Examples</strong>: <strong>Monte Carlo (MC)</strong>, <strong>Temporal Difference (TD)</strong>.</li>
<li><strong>Advantages</strong>: More flexible, applicable when the model is unknown or too complex to compute.</li>
</ul>
<h3 id="52-on-policy-vs-off-policy-learning">5.2. On-Policy vs. Off-Policy Learning</h3>
<h4 id="on-policy-learning">On-Policy Learning:</h4>
<ul>
<li><strong>Definition</strong>: The agent learns about and improves the policy it is currently following. The policy that generates the data is the same as the one being evaluated and improved.</li>
<li><strong>Example</strong>: <strong>SARSA</strong> updates based on actions taken under the current policy.</li>
<li><strong>Advantages</strong>: Simpler and guarantees that the agent learns from its own actions.</li>
</ul>
<h4 id="off-policy-learning">Off-Policy Learning:</h4>
<ul>
<li><strong>Definition</strong>: The agent learns about an optimal policy while following a different behavior policy. The target policy is updated while the agent explores using a behavior policy.</li>
<li><strong>Example</strong>: <strong>Q-Learning</strong> updates based on the optimal action, independent of the behavior policy.</li>
<li><strong>Advantages</strong>: More flexible, allows for learning from past experiences and using different exploration strategies.</li>
</ul>
<h3 id="53-comparison">5.3. Comparison</h3>
<table>
<thead>
<tr>
<th>Feature</th>
<th><strong>TD (Temporal Difference)</strong></th>
<th><strong>Monte Carlo (MC)</strong></th>
<th><strong>Dynamic Programming (DP)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Model Requirement</strong></td>
<td>No model required (model-free)</td>
<td>No model required (model-free)</td>
<td>Requires a full model of the environment</td>
</tr>
<tr>
<td><strong>Learning Method</strong></td>
<td>Updates based on current estimates (bootstrapping)</td>
<td>Updates after complete episode (no bootstrapping)</td>
<td>Updates based on exact model (transition probabilities)</td>
</tr>
<tr>
<td><strong>Update Frequency</strong></td>
<td>After each step</td>
<td>After each episode</td>
<td>After each step or full sweep over states</td>
</tr>
<tr>
<td><strong>Efficiency</strong></td>
<td>More sample efficient (incremental learning)</td>
<td>Less efficient (requires full episodes)</td>
<td>Very efficient, but needs a model</td>
</tr>
<tr>
<td><strong>Convergence</strong></td>
<td>Converges with sufficient exploration</td>
<td>Converges with sufficient exploration</td>
<td>Converges to optimal policy with a known model</td>
</tr>
<tr>
<td><strong>Suitability</strong></td>
<td>Works well in ongoing tasks</td>
<td>Works well for episodic tasks</td>
<td>Works well in fully known environments</td>
</tr>
</tbody>
</table>
<p>The choice of method depends on the environment, the availability of a model, and the trade-off between exploration and exploitation.</p>
<h3 id="references">References</h3>
<ul>
<li>Sutton, R.S., &amp; Barto, A.G. (2018). <em>Reinforcement Learning: An Introduction</em> (2nd ed.). MIT Press.</li>
<li><a href="https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/monte-carlo-methods-in-practice/monte-carlo-integration.html?url=mathematics-physics-for-computer-graphics/monte-carlo-methods-in-practice/monte-carlo-integration">monte-carlo for integration</a></li>
</ul>
<!-- ## Author(s)

<div class="grid cards" markdown>
-   ![Instructor Avatar](/assets/images/staff/Ghazal-Hosseini.jpg){align=left width="150"}
    <span class="description">
        <p>**Ghazal Hosseini**</p>
        <p>Teaching Assistant</p>
        <p>[ghazaldesu@gmail.com](mailto:ghazaldesu@gmail.com)</p>
        <p>
        [:fontawesome-brands-linkedin-in:](https://www.linkedin.com/in/ghazal-hosseini-mighan-8b911823a){:target="_blank"}
        </p>
    </span>
</div> -->







  
  



  



  <form class="md-feedback" name="feedback" hidden>
    <fieldset>
      <legend class="md-feedback__title">
        Was this page helpful?
      </legend>
      <div class="md-feedback__inner">
        <div class="md-feedback__list">
          
            <button class="md-feedback__icon md-icon" type="submit" title="This page was helpful" data-md-value="1">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m7 0c0 .8-.7 1.5-1.5 1.5S14 10.3 14 9.5 14.7 8 15.5 8s1.5.7 1.5 1.5m-5 7.73c-1.75 0-3.29-.73-4.19-1.81L9.23 14c.45.72 1.52 1.23 2.77 1.23s2.32-.51 2.77-1.23l1.42 1.42c-.9 1.08-2.44 1.81-4.19 1.81"/></svg>
            </button>
          
            <button class="md-feedback__icon md-icon" type="submit" title="This page could be improved" data-md-value="0">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10m-6.5-4c.8 0 1.5.7 1.5 1.5s-.7 1.5-1.5 1.5-1.5-.7-1.5-1.5.7-1.5 1.5-1.5M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m2 4.5c1.75 0 3.29.72 4.19 1.81l-1.42 1.42C14.32 16.5 13.25 16 12 16s-2.32.5-2.77 1.23l-1.42-1.42C8.71 14.72 10.25 14 12 14"/></svg>
            </button>
          
        </div>
        <div class="md-feedback__note">
          
            <div data-md-value="1" hidden>
              
              
                
              
              
              
                
                
              
              Thanks for your feedback!
            </div>
          
            <div data-md-value="0" hidden>
              
              
                
              
              
              
                
                
              
              Thanks for your feedback! Help us improve this page by using our <a href="..." target="_blank" rel="noopener">feedback form</a>.
            </div>
          
        </div>
      </div>
    </fieldset>
  </form>


<script src="https://giscus.app/client.js"
        data-repo="DeepRLCourse/DeepRLCourse.github.io"
        data-repo-id="R_kgDOMPOFbQ"
        data-category="Announcements"
        data-category-id="DIC_kwDOMPOFbc4CmGY6"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="1"
        data-input-position="top"
        data-theme="preferred_color_scheme"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>

                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Made with  in Robust and Interpretable Machine Learning Lab
    </div>
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.top", "toc.integrate"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>