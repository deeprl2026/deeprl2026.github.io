
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="This page delves into advanced reinforcement learning methods, focusing on actor-critic algorithms such as PPO, DDPG, and SAC. It explores key concepts like Reward-to-Go, Advantage Estimation, and Generalized Advantage Estimation (GAE), providing theoretical insights and practical applications. The document also highlights the challenges and solutions in continuous action spaces, policy optimization, and exploration strategies, making it a comprehensive guide for mastering advanced RL techniques.">
      
      
      
        <link rel="canonical" href="https://DeepRL2026.github.io/course_notes/advanced/">
      
      
      
      
        
      
      
      <link rel="icon" href="../../assets/favicon/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.3">
    
    
      
        <title>Week 4: Advanced Methods - Deep RL Course</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300,300i,400,400i,700,700i%7CRed+Hat+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Ubuntu";--md-code-font:"Red Hat Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
      <link rel="stylesheet" href="../../stylesheets/vazirmatn.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-Q1YT8K39WE"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-Q1YT8K39WE",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-Q1YT8K39WE",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
  
<meta property="og:type" content="website" />
<meta property="og:title" content="Week 4: Advanced Methods - Deep RL Course" />
<meta property="og:description" content="This page delves into advanced reinforcement learning methods, focusing on actor-critic algorithms such as PPO, DDPG, and SAC. It explores key concepts like Reward-to-Go, Advantage Estimation, and Generalized Advantage Estimation (GAE), providing theoretical insights and practical applications. The document also highlights the challenges and solutions in continuous action spaces, policy optimization, and exploration strategies, making it a comprehensive guide for mastering advanced RL techniques." />
<meta property="og:image" content="https://DeepRL2026.github.io/assets/images/social/course_notes/advanced.png" />
<meta property="og:image:type" content="image/png" />
<meta property="og:image:width" content="1200" />
<meta property="og:image:height" content="630" />
<meta property="og:url" content="https://DeepRL2026.github.io/course_notes/advanced/" />
<meta property="twitter:card" content="summary_large_image" />
<meta property="twitter:title" content="Week 4: Advanced Methods - Deep RL Course" />
<meta property="twitter:description" content="This page delves into advanced reinforcement learning methods, focusing on actor-critic algorithms such as PPO, DDPG, and SAC. It explores key concepts like Reward-to-Go, Advantage Estimation, and Generalized Advantage Estimation (GAE), providing theoretical insights and practical applications. The document also highlights the challenges and solutions in continuous action spaces, policy optimization, and exploration strategies, making it a comprehensive guide for mastering advanced RL techniques." />
<meta property="twitter:image" content="https://DeepRL2026.github.io/assets/images/social/course_notes/advanced.png" />
</head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#week-4-advanced-methods" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Deep RL Course" class="md-header__button md-logo" aria-label="Deep RL Course" data-md-component="logo">
      
  <img src="../../assets/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Deep RL Course
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Week 4: Advanced Methods
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="red"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="red"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  Home

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  Course Notes

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../lectures/lecture1/" class="md-tabs__link">
          
  
  
  Lectures

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  TA Sessions

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  Homeworks

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  Projects

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../exams/" class="md-tabs__link">
          
  
  
  Exams

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../resources/" class="md-tabs__link">
          
  
  
  Resources

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
    
      

    

    

        

            
                  
            

        

            

        
    

    


    <!-- Navigation -->
    
        
        <div
            class="md-sidebar md-sidebar--primary"
            data-md-component="sidebar"
            data-md-type="navigation"
            
        >
            <div class="md-sidebar__scrollwrap">
            <div class="md-sidebar__inner">
                <!--
  Copyright (c) 2016-2025 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->



<!-- Determine classes -->


  


  


<!-- Navigation -->
<nav
  class="md-nav md-nav--primary md-nav--lifted"
  aria-label="Navigation"
  data-md-level="0"
>

  <!-- Site title -->
  <label class="md-nav__title" for="__drawer">
    <a
      href="../.."
      title="Deep RL Course"
      class="md-nav__button md-logo"
      aria-label="Deep RL Course"
      data-md-component="logo"
    >
      
  <img src="../../assets/logo.png" alt="logo">

    </a>
    Deep RL Course
  </label>

  <!-- Repository information -->
  

  <!-- Navigation list -->
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Home
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Course Notes
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Course Notes
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Lectures
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Lectures
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../lectures/lecture1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Lecture1
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../lectures/lecture2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Lecture2
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    TA Sessions
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    TA Sessions
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Homeworks
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    Homeworks
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Projects
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    Projects
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Exams
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            
  
    Exams
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../exams/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Previous Semesters
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Resources
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            
  
    Resources
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../resources/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Resources
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
            </div>
            </div>
        </div>
    

    <!-- Table of contents -->
    
        
        <div
            class="md-sidebar md-sidebar--secondary"
            data-md-component="sidebar"
            data-md-type="toc"
            
        >
            <div class="md-sidebar__scrollwrap">
            <div class="md-sidebar__inner">
                <!--
  Copyright (c) 2016-2025 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->

<!-- Determine title -->




<!-- Table of contents -->
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  

  <!--
    Check whether the content starts with a level 1 headline. If it does, the
    top-level anchor must be skipped, since it would be redundant to the link
    to the current page that is located just above the anchor. Therefore we
    directly continue with the children of the anchor.
  -->
  
  
    
  

  <!-- Table of contents title and list -->
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      
        Introduction
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Introduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#actor-critic" class="md-nav__link">
    <span class="md-ellipsis">
      
        Actor-Critic
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Actor-Critic">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#reward-to-go" class="md-nav__link">
    <span class="md-ellipsis">
      
        Reward to Go:
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#advantage-value" class="md-nav__link">
    <span class="md-ellipsis">
      
        Advantage Value:
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Advantage Value:">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#why-use-the-advantage-value" class="md-nav__link">
    <span class="md-ellipsis">
      
        Why Use the Advantage Value?
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#estimating-the-advantage-value" class="md-nav__link">
    <span class="md-ellipsis">
      
        Estimating the Advantage Value
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#generalized-advantage-estimation-gae" class="md-nav__link">
    <span class="md-ellipsis">
      
        Generalized Advantage Estimation (GAE)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#actor-critic-algorihtms" class="md-nav__link">
    <span class="md-ellipsis">
      
        Actor-Critic Algorihtms
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Actor-Critic Algorihtms">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#batch-actor-critic-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      
        Batch actor-critic algorithm
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#online-actor-critic-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      
        Online actor-critic algorithm
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parallel-actor-critic-online" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parallel Actor-Critic (Online)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#off-policy-actor-critic-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      
        Off-Policy Actor-Critic Algorithm
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#issues-with-standard-policy-gradient-methods" class="md-nav__link">
    <span class="md-ellipsis">
      
        Issues with Standard Policy Gradient Methods
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#proximal-policy-optimization-ppo" class="md-nav__link">
    <span class="md-ellipsis">
      
        Proximal Policy Optimization (PPO)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Proximal Policy Optimization (PPO)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-ppo-enhances-on-policy-actor-critic-methods" class="md-nav__link">
    <span class="md-ellipsis">
      
        How PPO Enhances On-Policy Actor-Critic Methods
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-intuition-behind-ppo" class="md-nav__link">
    <span class="md-ellipsis">
      
        The intuition behind PPO
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-clipped-surrogate-objective" class="md-nav__link">
    <span class="md-ellipsis">
      
        the Clipped Surrogate Objective
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="the Clipped Surrogate Objective">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-ratio-function" class="md-nav__link">
    <span class="md-ellipsis">
      
        The ratio Function
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-clipped-part" class="md-nav__link">
    <span class="md-ellipsis">
      
        The clipped part
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-unclipped-part" class="md-nav__link">
    <span class="md-ellipsis">
      
        The unclipped part
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#visualize-the-clipped-surrogate-objective" class="md-nav__link">
    <span class="md-ellipsis">
      
        Visualize the Clipped Surrogate Objective
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#challenges-of-ppo-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      
        Challenges of PPO algorithm
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deep-deterministic-policy-gradients-ddpg" class="md-nav__link">
    <span class="md-ellipsis">
      
        Deep Deterministic Policy Gradients (DDPG)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Deep Deterministic Policy Gradients (DDPG)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#handling-continuous-action-spaces" class="md-nav__link">
    <span class="md-ellipsis">
      
        Handling Continuous Action Spaces
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Handling Continuous Action Spaces">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#why-is-this-a-problem" class="md-nav__link">
    <span class="md-ellipsis">
      
        Why is this a problem?
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-does-ddpg-solve-it" class="md-nav__link">
    <span class="md-ellipsis">
      
        How does DDPG solve it?
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ddpg-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      
        DDPG Architecture
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="DDPG Architecture">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#why-use-target-networks" class="md-nav__link">
    <span class="md-ellipsis">
      
        Why Use Target Networks?
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#breakdown-of-ddpg-components" class="md-nav__link">
    <span class="md-ellipsis">
      
        Breakdown of DDPG Components
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Breakdown of DDPG Components">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#replay-buffer" class="md-nav__link">
    <span class="md-ellipsis">
      
        Replay Buffer
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#actor-policy-critic-value-network-updates" class="md-nav__link">
    <span class="md-ellipsis">
      
        Actor (Policy) &amp; Critic (Value) Network Updates
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#target-network-updates" class="md-nav__link">
    <span class="md-ellipsis">
      
        Target Network Updates
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exploration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Exploration
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#algorithm-ddpg-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      
        Algorithm: DDPG Algorithm
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Algorithm: DDPG Algorithm">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#soft-actor-critic-sac" class="md-nav__link">
    <span class="md-ellipsis">
      
        Soft Actor-Critic (SAC)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Soft Actor-Critic (SAC)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#challenges-and-motivation-of-sac" class="md-nav__link">
    <span class="md-ellipsis">
      
        Challenges and motivation of SAC
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#maximum-entropy-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Maximum Entropy Reinforcement Learning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#overcoming-exploration-bias-in-multimodal-q-functions" class="md-nav__link">
    <span class="md-ellipsis">
      
        Overcoming Exploration Bias in Multimodal Q-Functions
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Overcoming Exploration Bias in Multimodal Q-Functions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#standard-rl-approach" class="md-nav__link">
    <span class="md-ellipsis">
      
        Standard RL Approach
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#improved-exploration-strategy" class="md-nav__link">
    <span class="md-ellipsis">
      
        Improved Exploration Strategy
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#soft-policy" class="md-nav__link">
    <span class="md-ellipsis">
      
        Soft Policy
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#soft-policy-iteration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Soft Policy Iteration
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Soft Policy Iteration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#soft-policy-evaluation-critic-update" class="md-nav__link">
    <span class="md-ellipsis">
      
        Soft Policy Evaluation (Critic Update)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#soft-policy-improvement-actor-update" class="md-nav__link">
    <span class="md-ellipsis">
      
        Soft Policy Improvement (Actor Update)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convergence-of-soft-policy-iteration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Convergence of Soft Policy Iteration
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#soft-actor-critic" class="md-nav__link">
    <span class="md-ellipsis">
      
        Soft Actor-Critic
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Soft Actor-Critic">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#soft-value-function-v_psis" class="md-nav__link">
    <span class="md-ellipsis">
      
        Soft Value function (\(V_{\psi}(s)\))
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#soft-q-funciton-q_thetas-a" class="md-nav__link">
    <span class="md-ellipsis">
      
        Soft Q-funciton (\(Q_{\theta}(s, a)\))
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#policy-network-pi_phias" class="md-nav__link">
    <span class="md-ellipsis">
      
        Policy network (\(\pi_{\phi}(a|s)\))
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    <span class="md-ellipsis">
      
        Conclusion
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Conclusion">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#reward-to-go_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Reward-to-Go
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#advantage-estimation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Advantage Estimation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#generalized-advantage-estimation-gae_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Generalized Advantage Estimation (GAE)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#comparison-of-reward-to-go-advantage-estimation-and-gae" class="md-nav__link">
    <span class="md-ellipsis">
      
        Comparison of Reward-to-Go, Advantage Estimation, and GAE
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Comparison of Reward-to-Go, Advantage Estimation, and GAE">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#proximal-policy-optimization-ppo_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Proximal Policy Optimization (PPO)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deep-deterministic-policy-gradient-ddpg" class="md-nav__link">
    <span class="md-ellipsis">
      
        Deep Deterministic Policy Gradient (DDPG)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#soft-actor-critic-sac_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Soft Actor-Critic (SAC)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#comparison-table-ppo-vs-ddpg-vs-sac" class="md-nav__link">
    <span class="md-ellipsis">
      
        Comparison Table: PPO vs. DDPG vs. SAC
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#final-thoughts" class="md-nav__link">
    <span class="md-ellipsis">
      
        Final Thoughts
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#authors" class="md-nav__link">
    <span class="md-ellipsis">
      
        Author(s)
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
            </div>
            </div>
        </div>
    

          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="week-4-advanced-methods">Week 4: Advanced Methods</h1>
<h2 id="introduction">Introduction</h2>
<p>Reinforcement Learning (RL) has significantly evolved with the development of <strong>actor-critic methods</strong>, which combine the benefits of policy-based and value-based approaches. These methods utilize <strong>policy gradients</strong> for optimizing the agentâ€™s actions while employing a <strong>critic network</strong> to estimate value functions, leading to improved stability.</p>
<p>As we explored last week, key concepts like Reward-to-Go and Advantage Estimation lay the foundation for understanding and enhancing these methods. Building on that conversation, this document revisits these ideas, emphasizing their role in refining policy updates and stabilizing training.</p>
<p>In actor-critic methods, we explore key concepts that enhance learning efficiency:</p>
<ul>
<li><strong>Reward-to-Go</strong>: A method for computing future. </li>
<li><strong>Advantage Estimation</strong>: A technique to quantify how much better an action is compared to the expected return.  </li>
<li><strong>Generalized Advantage Estimation (GAE)</strong>: A framework that balances bias and variance in advantage computation, making training more stable.  </li>
</ul>
<p>This document covers three widely used actor-critic algorithms:</p>
<ul>
<li><strong>Proximal Policy Optimization (PPO)</strong>: A popular on-policy algorithm that stabilizes policy updates through a clipped objective function.  </li>
<li><strong>Deep Deterministic Policy Gradient (DDPG)</strong>: An off-policy algorithm designed for continuous action spaces, incorporating experience replay and target networks.  </li>
<li><strong>Soft Actor-Critic (SAC)</strong>: A state-of-the-art off-policy method that introduces entropy maximization to improve exploration and robustness.  </li>
</ul>
<p>Each section delves into these algorithms, their theoretical foundations, and their practical advantages in reinforcement learning tasks.</p>
<hr />
<h4 id="actor-critic">Actor-Critic</h4>
<p>The variance of policy methods can originate from two sources:</p>
<ol>
<li>high variance in the cumulative reward estimate</li>
<li>high variance in the gradient estimate. </li>
</ol>
<p>For both problems, a solution has been developed: bootstrapping for better reward estimates and baseline subtraction to lower the variance of gradient estimates.</p>
<p>In the next seciton we will review the concepts bootstrapping (<strong><em>reward to go</em></strong>), using baseline (<strong><em>Advantage value</em></strong>) and <strong><em>Generalized Advantage Estimation</em></strong> (GAE).</p>
<h5 id="reward-to-go">Reward to Go:</h5>
<p><strong><em>A cumulative reward from state <span class="arithmatex">\(s_t\)</span> to the end of the episode by applying policy <span class="arithmatex">\(\pi_\theta\)</span>.</em></strong></p>
<p>As mentioned earlier, in the <em>policy gradient</em> method, we update our policy weights with the learning rate <span class="arithmatex">\(\alpha\)</span> as follows:</p>
<div class="arithmatex">\[
\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta),
\]</div>
<p>where</p>
<div class="arithmatex">\[
\nabla_\theta J(\theta) \approx \frac{1}{N} \sum^N_{i=1} \sum^T_{t=1} \nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t})\cdot r(s_{i,t},a_{i,t}).
\]</div>
<p>In this equation, the term <span class="arithmatex">\(r(s_{i,t}, a_{i,t})\)</span> is the primary source of variance and noise. We use the <em>causality trick</em> to mitigate this issue by multiplying the policy gradient at state <span class="arithmatex">\(s_t\)</span> with its future rewards. It is important to note that the policy at state <span class="arithmatex">\(s_t\)</span> can only affect future rewards, not past ones. The causality trick is represented as follows:</p>
<div class="arithmatex">\[
\nabla_\theta J(\theta) \approx \frac{1}{N} \sum^N_{i=1}\bigg( \sum^T_{t=1} \nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t})\bigg) \bigg( \sum^T_{t=1}r(s_{i,t},a_{i,t}) \bigg) \approx \frac{1}{N} \sum^N_{i=1} \sum^T_{t=1} \nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t})\bigg( \sum^T_{t'=t}r(s_{i,t'},a_{i,t'}) \bigg).
\]</div>
<p>The term <span class="arithmatex">\(\sum^T_{t'=t}r(s_{i,t'},a_{i,t'})\)</span> is known as <strong><em>reward to go</em></strong>, which is calculated in a Monte Carlo manner. It represents the total expected reward from a given state by applying policy <span class="arithmatex">\(\pi_\theta\)</span>, starting from time <span class="arithmatex">\(t\)</span> to the end of the episode.</p>
<p>To further reduce variance, we can approximate the <em>reward to go</em> with the <em>Q-value</em>, which conveys a similar meaning. Thus, we can rewrite <span class="arithmatex">\(\nabla_\theta J(\theta)\)</span> as:</p>
<div class="arithmatex">\[
\nabla_\theta J(\theta) \approx \frac{1}{N} \sum^N_{i=1} \sum^T_{t=1} \nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t}) Q(s_{i,t},a_{i,t}).
\]</div>
<h5 id="advantage-value">Advantage Value:</h5>
<p><strong><em>Measures how much an action is better than the average of other actions in a given state.</em></strong></p>
<center> 
<img src="\assets\images\course_notes\advanced\Actor_Critic.jpg"
    alt="Advantage value"
    style="float: center; margin-right: 10px;" 
    /> 
    </center>

<h6 id="why-use-the-advantage-value">Why Use the Advantage Value?</h6>
<p>We can further reduce variance by subtracting a baseline from <span class="arithmatex">\(Q(s_{i,t}, a_{i,t})\)</span> without altering the expectation of <span class="arithmatex">\(\nabla_\theta J(\theta)\)</span>, making it an unbiased estimator:</p>
<div class="arithmatex">\[
\nabla_\theta J(\theta) \approx \frac{1}{N} \sum^N_{i=1} \sum^T_{t=1} \nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t}) \bigg( Q(s_{i,t},a_{i,t}) - b_t \bigg).
\]</div>
<p>A reasonable choice for the baseline is the expected reward. Although it is not optimal, it significantly reduces variance.</p>
<p>We define:</p>
<div class="arithmatex">\[
Q(s_{i,t},a_{i,t}) = \sum_{t'=t}^T E_{\pi_\theta}[r(s_{t'}, a_{t'})|s_t,a_t].
\]</div>
<p>To ensure the baseline is independent of the action taken, we compute the expectation of <span class="arithmatex">\(Q(s_{i,t}, a_{i,t})\)</span> over all actions sampled from the policy:</p>
<div class="arithmatex">\[
E_{a_t \sim \pi_\theta(a_{i,t}|s_{i,t})} [Q(s_{i,t},a_{i,t})] = V(s_t) = b_t.
\]</div>
<p>Thus, the variance-reduced policy gradient equation becomes:</p>
<div class="arithmatex">\[
\nabla_\theta J(\theta) \approx \frac{1}{N} \sum^N_{i=1} \sum^T_{t=1} \nabla_\theta \log\pi_\theta(a_{i,t}|s_{i,t}) \bigg( Q(s_{i,t},a_{i,t}) - V(s_t) \bigg).
\]</div>
<p>We define the <em>advantage function</em> as:</p>
<div class="arithmatex">\[
A(s_t,a_t) = Q(s_{i,t},a_{i,t}) - V(s_t).
\]</div>
<div class="admonition example">
<p class="admonition-title">Example of Understanding the Advantage Function</p>
<p>Consider a penalty shootout game to illustrate the concept of the advantage function and Q-values in reinforcement learning.</p>
<p><center> 
<img src="\assets\images\course_notes\advanced\Football.png"
    alt="penalty shootout game"
    style="float: center; margin-right: 10px;" 
    /> 
    </center></p>
<ul>
<li><strong>Game Setup</strong>:<ol>
<li><em>Goalie Strategy</em>:  A goalie always jumps to the right to block the shot.</li>
<li><em>Kicker Strategy</em>: A kicker can shoot either left or right with equal probability (0.5 each), defining the kicker's policy <span class="arithmatex">\(\pi_k\)</span>.</li>
</ol>
</li>
</ul>
<p>The reward matrix for the game is:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Kicker / Goalie</th>
<th style="text-align: center;">Right (jumps right)</th>
<th style="text-align: center;">Left (jumps left)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Right (shoots right)</td>
<td style="text-align: center;">0,1</td>
<td style="text-align: center;">1,0</td>
</tr>
<tr>
<td style="text-align: center;">Left (shoots left)</td>
<td style="text-align: center;">1,0</td>
<td style="text-align: center;">0,1</td>
</tr>
</tbody>
</table>
<ul>
<li>
<p><strong>Expected Reward</strong>:</p>
<p>Since the kicker selects left and right with equal probability, the expected reward is:</p>
<div class="arithmatex">\[
V^{\pi_k}(s_t) = 0.5 \times 1 + 0.5 \times 0 = 0.5.
\]</div>
</li>
<li>
<p><strong>Q-Value Calculation</strong>:
    The Q-value is expressed as:</p>
<div class="arithmatex">\[
Q^{\pi_k}(s_{i,t},a_{i,t}) = V^{\pi_k}(s_t) + A^{\pi_k}(s_t,a_t).
\]</div>
<ul>
<li>If the kicker shoots right, the shot is always saved (<span class="arithmatex">\(Q^{\pi_k}(s_{i,t},r) = 0\)</span>).</li>
<li>If the kicker shoots left, the shot is always successful (<span class="arithmatex">\(Q^{\pi_k}(s_{i,t},l) = 1\)</span>).</li>
</ul>
</li>
<li>
<p><strong>Advantage Calculation</strong>:</p>
<p>The advantage function <span class="arithmatex">\(A^{\pi_k}(s_t,a_t)\)</span> measures how much better or worse an action is compared to the expected reward.</p>
<ul>
<li>If the kicker shoots left, he scores (reward = 1), which is 0.5 more than the expected reward <span class="arithmatex">\(V^{\pi_k}(s_t)\)</span>. Thus, the advantage of shooting left is:</li>
</ul>
<div class="arithmatex">\[
1 = 0.5 + A^{\pi_k}(s_t,l) \Rightarrow A^{\pi_k}(s_t,l) = 0.5.
\]</div>
<ul>
<li>If the kicker shoots right, he fails (reward = 0), which is 0.5 less than the expected reward. Thus, the advantage of shooting right is:</li>
</ul>
<div class="arithmatex">\[
0 = 0.5 + A^{\pi_k}(s_t,r) \Rightarrow A^{\pi_k}(s_t,r) = -0.5.
\]</div>
</li>
</ul>
</div>
<h6 id="estimating-the-advantage-value">Estimating the Advantage Value</h6>
<p>Instead of maintaining separate networks for estimating <span class="arithmatex">\(V(s_t)\)</span> and <span class="arithmatex">\(Q(s_{i,t}, a_{i,t})\)</span>, we approximate <span class="arithmatex">\(Q(s_{i,t}, a_{i,t})\)</span> using <span class="arithmatex">\(V(s_t)\)</span>:</p>
<div class="arithmatex">\[
Q(s_{i,t},a_{i,t}) = r(s_t, a_t) + \sum_{t'=t+1}^T E_{\pi_\theta}[r(s_{t'}, a_{t'})|s_t,a_t] \approx r(s_t, a_t) + V(s_{t+1}).
\]</div>
<p>Thus, we estimate the advantage function as:</p>
<div class="arithmatex">\[
A(s_{i,t},a_{i,t}) \approx r(s_t, a_t) + V(s_{t+1}) - V(s_t).
\]</div>
<p>We can also, consider the advantage function with discount factor as:</p>
<div class="arithmatex">\[
A(s_{i,t},a_{i,t}) \approx r(s_t, a_t) + \gamma V(s_{t+1}) - V(s_t).
\]</div>
<p>To train the value estimator, we use Monte Carlo estimation.</p>
<h5 id="generalized-advantage-estimation-gae">Generalized Advantage Estimation (GAE)</h5>
<p>To have a good  balance between variance and bias, we can use the concept of GAE, which is firstly introduced in <a href="https://arxiv.org/abs/1506.02438">High-Dimensional Continuous Control Using Generalized Advantage Estimation</a>. </p>
<p>At the first, we define <span class="arithmatex">\(\hat{A}^{(k)}(s_{i,t},a_{i,t})\)</span> to understand this the GAE concept.</p>
<div class="arithmatex">\[
\hat{A}^{(k)}(s_{i,t},a_{i,t}) = r(s_t, a_t) + \dots + \gamma^{k-1}r(s_{t+k-1}, a_{t+k-1}) + \gamma^k V(s_{t+k})- V(s_t).
\]</div>
<p>So, we can write the <span class="arithmatex">\(\hat{A}^{(k)}(s_{i,t},a_{i,t})\)</span> for <span class="arithmatex">\(k \in \{1, \infty\}\)</span> as:</p>
<div class="arithmatex">\[
\hat{A}^{(1)}(s_{i,t},a_{i,t}) = r(s_t, a_t) + \gamma V(s_{t+1}) - V(s_t)
\]</div>
<div class="arithmatex">\[
\hat{A}^{(2)}(s_{i,t},a_{i,t}) = r(s_t, a_t) + \gamma r(s_{t+1}, a_{t+1}) + \gamma^2 V(s_{t+2}) - V(s_t)
\]</div>
<div class="arithmatex">\[
.\\
.\\
.\\
\]</div>
<div class="arithmatex">\[
\hat{A}^{(\infty)}(s_{i,t},a_{i,t}) = r(s_t, a_t) + \gamma r(s_{t+1}, a_{t+1}) + \gamma^2 r(s_{t+2}, a_{t+2})+ \dots - V(s_t)\\
\]</div>
<p><span class="arithmatex">\(\hat{A}^{(1)}(s_{i,t},a_{i,t})\)</span> is high bias, low variance, whilst <span class="arithmatex">\(\hat{A}^{(\infty)}(s_{i,t},a_{i,t})\)</span> is unbiased, high variance.</p>
<p>We take a weighted average of all <span class="arithmatex">\(\hat{A}^{(k)}(s_{i,t},a_{i,t})\)</span> for <span class="arithmatex">\(k \in \{1, \infty\}\)</span> with weight <span class="arithmatex">\(w_k = \lambda^{k-1}\)</span> to balance bias and variance. This is called Generalized Advantage Estimation (GAE). </p>
<div class="arithmatex">\[
\hat{A}^{(GAE)}(s_{i,t},a_{i,t}) = \frac{\sum_{k =1}^T  w_k \hat{A}^{(k)}(s_{i,t},a_{i,t})}{\sum_k w_k}= \frac{\sum_{k =1}^T \lambda^{k-1} \hat{A}^{(k)}(s_{i,t},a_{i,t})}{\sum_k w_k}
\]</div>
<h5 id="actor-critic-algorihtms">Actor-Critic Algorihtms</h5>
<h6 id="batch-actor-critic-algorithm">Batch actor-critic algorithm</h6>
<p>The first algorithm is <em>Actor-Critic with Bootstrapping and Baseline Subtraction</em>.
In this algorithm, the simulator runs for an entire episode before updating the policy.</p>
<p><strong>Batch actor-critic algorithm:</strong></p>
<ol>
<li><strong>for</strong> each episode <strong>do</strong>:</li>
<li>&emsp;<strong>for</strong> each step <strong>do</strong>:</li>
<li>&emsp;&emsp;Take action <span class="arithmatex">\(a_t \sim \pi_{\theta}(a_t | s_t)\)</span>, get <span class="arithmatex">\((s_t,a_t,s'_t,r_t)\)</span>.</li>
<li>&emsp;Fit <span class="arithmatex">\(\hat{V}(s_t)\)</span> with sampled rewards.</li>
<li>&emsp;Evaluate the advantage function: <span class="arithmatex">\(A({s_t, a_t})\)</span></li>
<li>&emsp;Compute the policy gradient: <span class="arithmatex">\(\nabla_{\theta} J(\theta) \approx \sum_{i} \nabla_{\theta} \log \pi_{\theta}(a_i | s_i) A({s_t})\)</span></li>
<li>&emsp;Update the policy parameters:  <span class="arithmatex">\(\theta \gets \theta + \alpha \nabla_{\theta} J(\theta)\)</span></li>
</ol>
<p>Running full episodes for a single update is inefficient as it requires a significant amount of time. To address this issue, the <strong>online actor-critic algorithm</strong> is proposed.</p>
<h6 id="online-actor-critic-algorithm">Online actor-critic algorithm</h6>
<p>In this algorithm, we take an action in the environment and immediately apply an update using that action.</p>
<p><strong>Online actor-critic algorithm</strong></p>
<ol>
<li><strong>for</strong> each episode <strong>do</strong>:</li>
<li>&emsp;<strong>for</strong> each step <strong>do</strong>:</li>
<li>&emsp;&emsp;Take action <span class="arithmatex">\(a_t \sim \pi_{\theta}(a_t | s_t)\)</span>, get <span class="arithmatex">\((s_t,a_t,s'_t,r_t)\)</span>.</li>
<li>&emsp;&emsp;Fit <span class="arithmatex">\(\hat{V}(s_t)\)</span> with the sampled reward.</li>
<li>&emsp;&emsp;Evaluate the advantage function: <span class="arithmatex">\(A({s,a})\)</span></li>
<li>&emsp;&emsp;Compute the policy gradient: <span class="arithmatex">\(\nabla_{\theta} J(\theta) \approx  \nabla_{\theta} \log \pi_{\theta}(a | s) A({s,a})\)</span></li>
<li>&emsp;&emsp;Update the policy parameters: <span class="arithmatex">\(\theta \gets \theta + \alpha \nabla_{\theta} J(\theta)\)</span></li>
</ol>
<p>Training neural networks with a batch size of 1 leads to high variance, making the training process unstable.</p>
<p>To mitigate this issue, two main solutions are commonly used:</p>
<ol>
<li><strong>Parallel Actor-Critic (Online)</strong></li>
<li><strong>Off-Policy Actor-Critic</strong></li>
</ol>
<h6 id="parallel-actor-critic-online">Parallel Actor-Critic (Online)</h6>
<p>Many high-performance implementations are based on the actor critic approach. For large problems, the algorithm is typically parallelized and implemented on a large cluster computer.</p>
<center> 
<img src="\assets\images\course_notes\advanced\Parallel.jpg"
    alt="Parallel Actor-Critic framework"
    style="float: center; margin-right: 10px;" 
    /> 
    </center>

<p>To reduce variance, multiple actors are used to update the policy. There are two main approaches:</p>
<ul>
<li><strong>Synchronized Parallel Actor-Critic:</strong> All actors run synchronously, and updates are applied simultaneously. However, this introduces synchronization overhead, making it impractical in many cases.</li>
<li><strong>Asynchronous Parallel Actor-Critic:</strong> Each actor applies its updates independently, reducing synchronization constraints and improving computational efficiency. It also, uses asynchronous (parallel and distributed) gradient descent for optimization of deep neural network controllers.</li>
</ul>
<details class="tip">
<summary>Resources &amp; Links</summary>
<p><a href="https://arxiv.org/abs/1602.01783">Asynchronous Methods for Deep Reinforcement Learning</a></p>
<p><a href="https://danieltakeshi.github.io/2018/06/28/a2c-a3c/">Actor-Critic Methods: A3C and A2C</a></p>
<p><a href="https://theaisummer.com/Actor_critics/">The idea behind Actor-Critics and how A2C and A3C improve them</a></p>
</details>
<h6 id="off-policy-actor-critic-algorithm">Off-Policy Actor-Critic Algorithm</h6>
<p>In the off-policy approach, we maintain a replay buffer to store past experiences, allowing us to train the model using previously collected data rather than relying solely on the most recent experience.</p>
<center> 
<img src="\assets\images\course_notes\advanced\offpolicy.png"
    alt="Replay buffer"
    style="float: center; margin-right: 10px;" 
    /> 
    </center>

<p><strong>Off-policy actor-critic algorithm:</strong></p>
<ol>
<li><strong>for</strong> each episode <strong>do</strong>:</li>
<li>&emsp;<strong>for</strong> multiple steps <strong>do</strong>:</li>
<li>&emsp;&emsp;Take action <span class="arithmatex">\(a \sim \pi_{\theta}(a | s)\)</span>, get <span class="arithmatex">\((s,a,s',r)\)</span>, store in <span class="arithmatex">\(\mathcal{R}\)</span>.</li>
<li>&emsp;Sample a batch <span class="arithmatex">\(\{s_i, a_i, r_i, s'_i \}\)</span> for buffer <span class="arithmatex">\(\mathcal{R}\)</span>.</li>
<li>&emsp;Fit <span class="arithmatex">\(\hat{Q}^{\pi}(s_i, a_i)\)</span> for each <span class="arithmatex">\(s_i, a_i\)</span>.</li>
<li>&emsp;Compute the policy gradient: <span class="arithmatex">\(\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i} \nabla_{\theta} \log \pi_{\theta}(a^{\pi}_i | s_i) \hat{Q}^{\pi}(s_i, a^{\pi}_i)\)</span></li>
<li>&emsp;Update the policy parameters: <span class="arithmatex">\(\theta \gets \theta + \alpha \nabla_{\theta} J(\theta)\)</span></li>
</ol>
<p>To work with off-policy methods, we use the Q-value instead of the V-value in step 3. In step 4, rather than using the advantage function, we directly use <span class="arithmatex">\(\hat{Q}^{\pi}(s_i, a^{\pi}_i)\)</span>, where <span class="arithmatex">\(a^{\pi}_i\)</span>  is sampled from the policy <span class="arithmatex">\(\pi\)</span>. By using the Q-value instead of the advantage function, we do not encounter the high-variance problem typically associated with single-step updates. This is because we sample a batch from the replay buffer, which inherently reduces variance. As a result, there is no need to compute an explicit advantage function for variance reduction.</p>
<h5 id="issues-with-standard-policy-gradient-methods">Issues with Standard Policy Gradient Methods</h5>
<p>Earlier policy gradient methods, such as Vanilla Policy Gradient (VPG) or REINFORCE, suffer from high variance and instability in training. A key problem is that large updates to the policy can lead to drastic performance degradation.</p>
<p>To address these issues, Trust Region Policy Optimization (TRPO) was introduced, enforcing a constraint on how much the policy can change in a single update. However, TRPO is computationally expensive because it requires solving a constrained optimization problem.
PPO is a simpler and more efficient alternative to TRPO, designed to ensure stable policy updates without requiring complex constraints.</p>
<hr />
<h4 id="proximal-policy-optimization-ppo">Proximal Policy Optimization (PPO)</h4>
<h5 id="how-ppo-enhances-on-policy-actor-critic-methods">How PPO Enhances On-Policy Actor-Critic Methods</h5>
<p><strong>PPO (Proximal Policy Optimization)</strong> is an on-policy actor-critic algorithm. It combines a policy network (the actor) and a value network (the critic) and employs a clipped surrogate objective to restrict excessive policy updates, thereby promoting training stability.</p>
<p>PPO addresses several problems inherent in on-policy actor-critic methods in the following ways:</p>
<ul>
<li>
<p><strong>Stabilizing Policy Updates:</strong><br />
  PPO introduces a <em>clipping mechanism</em> in its objective function that constrains the policy update by ensuring the new policy doesn't deviate too far from the old policy.This clipping prevents overly large updates, thereby stabilizing the learning process and reducing sensitivity.</p>
</li>
<li>
<p><strong>Improving Sample Efficiency:</strong><br />
  Although PPO is an on-policy algorithm, it reuses a fixed batch of data for multiple epochs of mini-batch updates. This means that each set of interactions with the environment can contribute more to learning, mitigating the need for excessive sampling.</p>
</li>
<li>
<p><strong>Reducing Variance in Gradient Estimates:</strong><br />
  By incorporating advanced advantage estimation techniques (e.g., Generalized Advantage Estimation), PPO reduces the high variance typically associated with policy gradients, leading to more reliable and stable updates.</p>
</li>
<li>
<p><strong>Implicitly Maintaining a Trust Region:</strong><br />
  The clipping mechanism acts like an implicit trust region constraint. It ensures that updates remain within a safe boundary around the current policy, which helps in achieving monotonic improvements and prevents performance collapse.</p>
</li>
</ul>
<h5 id="the-intuition-behind-ppo">The intuition behind PPO</h5>
<p>The idea with Proximal Policy Optimization (PPO) is that we want to improve the training stability of the policy by limiting the change you make to the policy at each training epoch: <strong>we want to avoid having too large policy updates</strong>.  why?</p>
<ol>
<li>We know empirically that smaller policy updates during training are more likely to converge to an optimal solution.</li>
<li>If we change the policy too much, we may end up with a bad policy that cannot be improved.</li>
</ol>
<center> 
<img src="\assets\images\course_notes\advanced\cliff.jpg"
    alt="Replay buffer"
    style="float: center; margin-right: 10px;" 
    /> 
    </center>

<p><em>soruce: <a href="https://huggingface.co/blog/deep-rl-ppo">Unit 8, of the Deep Reinforcement Learning Class with Hugging Face</a></em></p>
<p>Therefore, in order not to allow the current policy to change much compared to the previous policy, we limit the ratio of these two policies to  <span class="arithmatex">\([1 - \epsilon, 1 + \epsilon]\)</span>.</p>
<h5 id="the-clipped-surrogate-objective">the Clipped Surrogate Objective</h5>
<div class="arithmatex">\[
L^{CLIP}(\theta) = \hat{\mathbb{E}}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip} \left( r_t(\theta), 1 - \epsilon, 1 + \epsilon \right) \hat{A}_t \right) \right]
\]</div>
<h6 id="the-ratio-function"><em>The ratio Function</em></h6>
<div class="arithmatex">\[
r_t(\theta) = \frac{\pi_{\theta}(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)}
\]</div>
<p><span class="arithmatex">\(r_{\theta}\)</span> denotes the probability ratio between the current and old policy. if <span class="arithmatex">\(r_{\theta} &gt; 1\)</span>, then the probability of doing action <span class="arithmatex">\(a_t\)</span> at <span class="arithmatex">\(s_t\)</span> in current policy is higher than the old policy and vice versa.</p>
<p>So this probability ratio is an easy way to estimate the divergence between old and current policy.</p>
<h6 id="the-clipped-part"><em>The clipped part</em></h6>
<div class="arithmatex">\[
\text{clip} \left( r_t(\theta), 1 - \epsilon, 1 + \epsilon \right) \hat{A}_t
\]</div>
<p>If the current policy is updated significantly, such that the new policy parameters <span class="arithmatex">\(\theta'\)</span>  diverge greatly from the previous ones, the probability ratio between the new and old policies is clipped to the bounds 
<span class="arithmatex">\(1 - \epsilon\)</span>, <span class="arithmatex">\(1 + \epsilon\)</span>. At this point, the derivative of the objective function becomes zero, effectively preventing further updates. </p>
<h6 id="the-unclipped-part"><em>The unclipped part</em></h6>
<div class="arithmatex">\[
r_t(\theta) \hat{A}_t
\]</div>
<p>In the context of optimization, if the initial starting point is not idealâ€”i.e., if the probability ratio between the new and old policies is outside the range of <span class="arithmatex">\(1 - \epsilon\)</span> and <span class="arithmatex">\(1 + \epsilon\)</span>â€”the ratio is clipped to these bounds. This clipping results in the derivative of the objective function becoming zero, meaning no gradient is available for updates. </p>
<p>In this formulation, the optimization is performed with respect to the new policy parameters <span class="arithmatex">\(\theta'\)</span>, and <span class="arithmatex">\(A\)</span> represents the advantage function, which indicates how much better or worse the action performed is compared to the average return.</p>
<div class="admonition example">
<p class="admonition-title">Example of PPO objective function</p>
<ul>
<li>Case 1: <strong>Positive Advantage (<span class="arithmatex">\(A &gt; 0\)</span>)</strong></li>
</ul>
<p>if the Advantage <span class="arithmatex">\(A\)</span> is positive (indicating that the action taken has a higher return than the expected return), and <span class="arithmatex">\(\frac{\pi_{\theta}(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)} &lt; 1-\epsilon\)</span> , the unclipped part is less than the clipped part and then it is minimized, so we have gradient to update the policy. This allows the policy to increase the probability of the action, aiming for the ratio to reach <span class="arithmatex">\(1 + \epsilon\)</span> without violating the clipping constraint.</p>
<ul>
<li>Case 2: <strong>Negative Advantage (<span class="arithmatex">\(A &lt; 0\)</span>)</strong></li>
</ul>
<p>On the other hand, if the Advantage <span class="arithmatex">\(A\)</span> is negative (meaning the action taken is worse than the average return), and <span class="arithmatex">\(\frac{\pi_{\theta}(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)} &gt; 1+\epsilon\)</span>, the unclipped objective is again minimized and the gradient is non-zero, leading to an update. In this case, since the Advantage is negative, the policy is adjusted to reduce the probability of selecting that action, bringing the ratio closer to the boundary (<span class="arithmatex">\(1-\epsilon\)</span>), while ensuring that the new policy does not deviate too much from the old one.</p>
</div>
<h5 id="visualize-the-clipped-surrogate-objective">Visualize the Clipped Surrogate Objective</h5>
<center> 
<img src="\assets\images\course_notes\advanced\recap.jpg"
    alt="recap"
    style="float: center; margin-right: 10px;" 
    /> 
    </center>

<p><strong>Algorithm: PPO-Clip</strong> </p>
<ol>
<li>Input: initial policy parameters <span class="arithmatex">\(\theta_0\)</span>, initial value function parameters <span class="arithmatex">\(\phi_0\)</span>   </li>
<li><strong>for</strong> <span class="arithmatex">\(k = 0, 1, 2, \dots\)</span> <strong>do</strong>  </li>
<li>&emsp; Collect set of trajectories <span class="arithmatex">\(\mathcal{D}_k = \{\tau_i\}\)</span> by running policy <span class="arithmatex">\(\pi_k = \pi(\theta_k)\)</span> in the environment.  </li>
<li>&emsp; Compute rewards-to-go <span class="arithmatex">\(\hat{R}_t\)</span>.  </li>
<li>&emsp; Compute advantage estimates, <span class="arithmatex">\(\hat{A}_t\)</span> (using any method of advantage estimation) based on the current value function <span class="arithmatex">\(V_{\phi_k}\)</span>.  </li>
<li>
<p>&emsp; Update the policy by maximizing the PPO-Clip objective:  </p>
<div class="arithmatex">\[
\theta_{k+1} = \arg \max_{\theta} \frac{1}{|\mathcal{D}_k| T} \sum_{\tau \in \mathcal{D}_k} \sum_{t=0}^{T} \min \left( \frac{\pi_{\theta}(a_t | s_t)}{\pi_{\theta_k}(a_t | s_t)} A^{\pi_{\theta_k}}(s_t, a_t), \, g(\epsilon, A^{\pi_{\theta_k}}(s_t, a_t)) \right)
\]</div>
<p>typically via stochastic gradient ascent with Adam.  </p>
</li>
<li>
<p>&emsp; Fit value function by regression on mean-squared error:  </p>
<div class="arithmatex">\[
\phi_{k+1} = \arg \min_{\phi} \frac{1}{|\mathcal{D}_k| T} \sum_{\tau \in \mathcal{D}_k} \sum_{t=0}^{T} \left( V_{\phi}(s_t) - \hat{R}_t \right)^2
\]</div>
</li>
<li>
<p><strong>end for</strong></p>
</li>
</ol>
<h5 id="challenges-of-ppo-algorithm">Challenges of PPO algorithm</h5>
<p>PPO requires a significant amount of interactions with the environment to converge. This can be problematic in real-world applications where data is expensive or difficult to collect. In fact it is a sample inefficient algorithm.</p>
<ol>
<li>
<p><strong>Hyperparameter Sensitivity</strong>: 
 PPO requires careful tuning of hyperparameters such as the clipping parameter (epsilon), learning rate, and the number of updates per iteration. Poorly chosen hyperparameters can lead to suboptimal performance or even failure to converge.</p>
</li>
<li>
<p><strong>Sample Efficiency</strong>: Although PPO is more sample-efficient than some other policy gradient methods, it still requires a significant amount of data to achieve good performance. This can be problematic in environments where data collection is expensive or time-consuming.</p>
</li>
<li>
<p><strong>Exploration-Exploitation Tradeoff</strong>: PPO, like other policy gradient methods, can struggle with balancing exploration and exploitation. It may prematurely converge to suboptimal policies if it fails to explore sufficiently.</p>
</li>
</ol>
<details class="tip">
<summary>Helpful links</summary>
<p><a href="https://huggingface.co/blog/deep-rl-ppo">Unit 8, of the Deep Reinforcement Learning Class with Hugging Face</a></p>
<p><a href="https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b">Proximal Policy Optimization (PPO) Explained</a></p>
<p><a href="https://www.youtube.com/watch?v=TjHH_--7l8g">Proximal Policy Optimization (PPO) - How to train Large Language Models</a></p>
<p><a href="https://spinningup.openai.com/en/latest/algorithms/ppo.html">Proximal Policy Optimization</a></p>
<p><a href="https://medium.com/@chris.p.hughes10/understanding-ppo-a-game-changer-in-ai-decision-making-explained-for-rl-newcomers-913a0bc98d2b">Understanding PPO: A Game-Changer in AI Decision-Making Explained for RL Newcomers</a></p>
<p><a href="https://dilithjay.com/blog/ppo">Proximal Policy Optimization (PPO) - Explained</a></p>
</details>
<hr />
<h4 id="deep-deterministic-policy-gradients-ddpg">Deep Deterministic Policy Gradients (DDPG)</h4>
<h5 id="handling-continuous-action-spaces">Handling Continuous Action Spaces</h5>
<h6 id="why-is-this-a-problem">Why is this a problem?</h6>
<p>In discrete action spaces, methods like <strong>DQN (Deep Q-Networks)</strong> can use an action-value function <span class="arithmatex">\(Q(s, a)\)</span> to select the best action. However, in continuous action spaces, selecting the optimal action requires solving a high-dimensional optimization problem at every step, which is computationally expensive.</p>
<h6 id="how-does-ddpg-solve-it">How does DDPG solve it?</h6>
<p>DDPG uses a deterministic policy network <span class="arithmatex">\(\pi(s)\)</span>, which directly maps states to actions, eliminating the need for iterative optimization over action values.</p>
<h5 id="ddpg-architecture">DDPG Architecture</h5>
<p>DDPG uses four neural networks:</p>
<ul>
<li>A <strong>Q network</strong>  </li>
<li>A <strong>deterministic policy network</strong>  </li>
<li>A <strong>target Q network</strong>  </li>
<li>A <strong>target policy network</strong>  </li>
</ul>
<p>The Q network and policy network are similar to Advantage Actor-Critic (A2C), but in DDPG, the <strong>Actor directly maps states to actions</strong> (the output of the network directly represents the action) instead of outputting a probability distribution over a discrete action space.</p>
<p>The <strong>target networks</strong> are time-delayed copies of their original networks that <strong>slowly track the learned networks</strong>. Using these target value networks greatly improves stability in learning.  </p>
<h6 id="why-use-target-networks">Why Use Target Networks?</h6>
<p>In methods without target networks, the update equations of the network depend on the network's own calculated values, making it <strong>prone to divergence</strong>.  </p>
<p>For example, if we update the Q-values directly using the current network, errors can compound, leading to instability. The target networks help mitigate this issue by <strong>providing more stable targets</strong> for updates. </p>
<div class="arithmatex">\[
    Q(s_t, a_t) \leftarrow r_t + \gamma Q(s_{t+1}, \arg\max_{a'} Q(s_{t+1}, a'))
\]</div>
<h5 id="breakdown-of-ddpg-components">Breakdown of DDPG Components</h5>
<ol>
<li><strong>Experience Replay</strong>  </li>
<li><strong>Actor &amp; Critic Network Updates</strong>  </li>
<li><strong>Target Network Updates</strong>  </li>
<li><strong>Exploration</strong>  </li>
</ol>
<h6 id="replay-buffer"><strong>Replay Buffer</strong></h6>
<p>As used in <strong>Deep Q-Learning</strong> and other RL algorithms, DDPG also utilizes a <strong>replay buffer</strong> to store experience tuples:  </p>
<div class="arithmatex">\[
(state, action, reward, next\_state)
\]</div>
<p>These tuples are stored in a <strong>finite-sized cache</strong> (replay buffer). During training, <strong>random mini-batches</strong> are sampled from this buffer to update the value and policy networks.</p>
<details class="note" open="open">
<summary>Why Use Experience Replay?</summary>
<p>In optimization tasks, we want <strong>data to be independently distributed</strong>. However, in an <strong>on-policy</strong> learning process, the collected data is highly correlated.  </p>
<p>By storing experience in a <strong>replay buffer</strong> and sampling random mini-batches for training, we break correlations and improve learning stability.</p>
</details>
<h6 id="actor-policy-critic-value-network-updates"><strong>Actor (Policy) &amp; Critic (Value) Network Updates</strong></h6>
<p>The value network is updated similarly to Q-learning. The updated Q-value is obtained using the Bellman equation:</p>
<div class="arithmatex">\[
y_i = r_i + \gamma Q' \left (s_{i+1}, \mu' (s_{i+1}|\theta^{\mu'})|\theta^{Q'} \right)
\]</div>
<p>However, in DDPG, the next-state Q-values are calculated using the target Q network and target policy network.  </p>
<p>Then, we minimize the mean squared error (MSE) loss between the updated Q-value and the original Q-value:</p>
<div class="arithmatex">\[
\mathcal{L} = \frac{1}{N} \sum_{i} \left(y_i -Q(s_i, a_i|\theta^Q) \right)^2
\]</div>
<ul>
<li><em>Note: The original Q-value is calculated using the learned Q-network, not the target Q-network.</em></li>
</ul>
<p>The policy function aims to maximize the expected return:</p>
<div class="arithmatex">\[
J(\theta) = \mathbb{E} \left[ Q(s, a) \mid s = s_t, a_t = \mu(s_t) \right]
\]</div>
<p>The <strong>policy loss</strong> is computed by differentiating the objective function with respect to the policy parameters:</p>
<div class="arithmatex">\[
\nabla_{\theta^\mu} J(\theta) = \nabla_a Q(s, a) |_{a = \mu(s)} \nabla_{\theta^\mu} \mu(s|\theta^\mu)
\]</div>
<p>But since we are updating the policy in an off-policy way with batches of experience, we take the mean of the sum of gradients calculated from the mini-batch:</p>
<div class="arithmatex">\[
\nabla_{\theta_\mu} J(\theta) \approx \frac{1}{N} \sum_i \left[ 
\left. \nabla_a Q(s, a \mid \theta^Q) \right|_{s = s_i, a = \mu(s_i)} 
\nabla_{\theta_\mu} \mu(s \mid \theta^\mu) \Big|_{s = s_i} 
\right]
\]</div>
<h6 id="target-network-updates"><strong>Target Network Updates</strong></h6>
<p>The <strong>target networks</strong> are updated via <strong>soft updates</strong> instead of direct copying:</p>
<div class="arithmatex">\[
\theta^{Q'} \leftarrow \tau \theta^{Q} + (1 - \tau) \theta^{Q'}
\]</div>
<div class="arithmatex">\[
\theta^{\mu'} \leftarrow \tau \theta^{\mu} + (1 - \tau) \theta^{\mu'}
\]</div>
<p>where <span class="arithmatex">\(\tau\)</span> is a small value , ensuring smooth updates that prevent instability.</p>
<h6 id="exploration"><strong>Exploration</strong></h6>
<p>In RL for discrete action spaces, exploration is often done using epsilon-greedy or Boltzmann exploration.  </p>
<p>However, in continuous action spaces, exploration is done by adding noise to the action itself.  </p>
<ul>
<li>Ornstein-Uhlenbeck Process<br />
The DDPG paper proposes adding <strong>Ornstein-Uhlenbeck (OU) noise</strong> to the actions.  </li>
</ul>
<p>The <strong>OU Process</strong> generates <strong>temporally correlated noise</strong>, preventing the noise from canceling out or "freezing" the action dynamics.</p>
<div class="arithmatex">\[
\mu^{'}(s_t) = \mu(s_t|\theta^\mu_t) + \mathcal{N}
\]</div>
<details class="tip" open="open">
<summary>Diagram Of DDPG Algorithms</summary>
<p><center> 
<img src="\assets\images\course_notes\advanced\DDPG.png"
    alt="DDPG"
    style="float: center; margin-right: 10px;" 
    /> 
    </center>
<center> 
<img src="\assets\images\course_notes\advanced\DDPG_alg.png"
    alt="DDPG_alg"
    style="float: center; margin-right: 10px;" 
    /> 
    </center></p>
</details>
<h2 id="algorithm-ddpg-algorithm"><strong>Algorithm: DDPG Algorithm</strong></h2>
<p>Randomly initialize critic network <span class="arithmatex">\( Q(s, a | \theta^Q) \)</span> and actor <span class="arithmatex">\( \mu(s | \theta^\mu) \)</span> with weights <span class="arithmatex">\( \theta^Q \)</span> and <span class="arithmatex">\( \theta^\mu \)</span>.<br />
Initialize target networks <span class="arithmatex">\( Q' \)</span> and <span class="arithmatex">\( \mu' \)</span> with weights <span class="arithmatex">\( \theta^{Q'} \gets \theta^Q, \quad \theta^{\mu'} \gets \theta^\mu \)</span>.<br />
Initialize replay buffer <span class="arithmatex">\( R \)</span>.  </p>
<p><strong>for</strong> episode = 1 to <span class="arithmatex">\( M \)</span> <strong>do</strong><br />
&nbsp;&nbsp;Initialize a random process <span class="arithmatex">\( \mathcal{N} \)</span> for action exploration.<br />
&nbsp;&nbsp;Receive initial observation state <span class="arithmatex">\( s_1 \)</span>.<br />
&nbsp;&nbsp;<strong>for</strong> <span class="arithmatex">\( t = 1 \)</span> to <span class="arithmatex">\( T \)</span> <strong>do</strong><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Select action <span class="arithmatex">\( a_t = \mu(s_t | \theta^\mu) + \mathcal{N}_t \)</span> according to the current policy and exploration noise.<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Execute action <span class="arithmatex">\( a_t \)</span> and observe reward <span class="arithmatex">\( r_t \)</span> and new state <span class="arithmatex">\( s_{t+1} \)</span>.<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Store transition <span class="arithmatex">\( (s_t, a_t, r_t, s_{t+1}) \)</span> in <span class="arithmatex">\( R \)</span>.<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sample a random minibatch of <span class="arithmatex">\( N \)</span> transitions <span class="arithmatex">\( (s_i, a_i, r_i, s_{i+1}) \)</span> from <span class="arithmatex">\( R \)</span>.<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Set:  </p>
<div class="arithmatex">\[  
y_i = r_i + \gamma Q'(s_{i+1}, \mu'(s_{i+1} | \theta^{\mu'}) | \theta^{Q'})  
\]</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Update critic by minimizing the loss:  </p>
<div class="arithmatex">\[  
L = \frac{1}{N} \sum_i (y_i - Q(s_i, a_i | \theta^Q))^2  
\]</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Update actor policy using the sampled policy gradient: </p>
<div class="arithmatex">\[  
\nabla_{\theta^\mu} J \approx \frac{1}{N} \sum_i \nabla_a Q(s, a | \theta^Q) |_{s=s_i, a=\mu(s_i)} \nabla_{\theta^\mu} \mu(s | \theta^\mu) |_{s_i}  
\]</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Update the target networks:  </p>
<div class="arithmatex">\[  
\theta^{Q'} \gets \tau \theta^Q + (1 - \tau) \theta^{Q'}  
\]</div>
<div class="arithmatex">\[  
\theta^{\mu'} \gets \tau \theta^\mu + (1 - \tau) \theta^{\mu'}  
\]</div>
<p>&nbsp;&nbsp;<strong>end for</strong><br />
<strong>end for</strong>  </p>
<hr />
<h4 id="soft-actor-critic-sac">Soft Actor-Critic (SAC)</h4>
<h5 id="challenges-and-motivation-of-sac">Challenges and motivation of SAC</h5>
<ol>
<li>Previous Off-policy methods like DDPG often struggle with exploration , leading to suboptimal policies. SAC overcomes this by introducing entropy maximization, which encourages the agent to explore more efficiently.</li>
<li>Sample inefficiency is a major issue in on-policy algorithms like Proximal Policy Optimization (PPO), which require a large number of interactions with the environment. SAC, being an off-policy algorithm, reuses past experiences stored in a replay buffer, making it significantly more sample-efficient.</li>
<li>Another challenge is instability in learning, as methods like DDPG can suffer from overestimation of Q-values. SAC mitigates this by employing twin Q-functions (similar to TD3) and incorporating entropy regularization, leading to more stable and robust learning.</li>
</ol>
<p>In essence, SAC seeks to maximize the entropy in policy, in addition to the expected reward from the environment. The entropy in policy can be interpreted as randomness in the policy.</p>
<details class="note" open="open">
<summary>what is entropy?</summary>
<p>We can think of entropy as how unpredictable a random variable is. If a random variable always takes a single value then it has zero entropy because itâ€™s not unpredictable at all. If a random variable can be any Real Number with equal probability then it has very high entropy as it is very unpredictable.
<center> 
<img src="\assets\images\course_notes\advanced\entropy.jpg"
    alt="entropy"
    style="float: center; margin-right: 10px;" 
    /> 
    </center></p>
<p><em>probability distributions with low entropy have a tendency to greedily sample certain values, as the probability mass is distributed relatively unevenly</em>.</p>
</details>
<h5 id="maximum-entropy-reinforcement-learning">Maximum Entropy Reinforcement Learning</h5>
<p>In Maximum Entropy RL, the agent tries to optimize the policy to choose the right action that can receive the highest sum of reward and long term sum of entropy. This enables the agent to explore more and avoid converging to local optima.</p>
<div class="admonition note">
<p class="admonition-title">reason</p>
<p>We want a high entropy in our policy to encourage the policy to assign equal probabilities to actions that have same or nearly equal Q-values(allow the policy to capture multiple modes of good policies), and also to ensure that it does not collapse into repeatedly selecting a particular action that could exploit some inconsistency in the approximated Q function. Therefore, SAC overcomes the  problem by encouraging the policy network to explore and not assign a very high probability to any one part of the range of actions.</p>
</div>
<p>The objective function of the Maximum entropy RL is as shown below:</p>
<div class="arithmatex">\[
J(\pi_{\theta}) = \mathbb{E}_{\pi_{\theta}} \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) + \alpha H(\pi(\cdot | s_t)) \right]
\]</div>
<p>and the optimal policy is:</p>
<div class="arithmatex">\[
\pi^* = \arg\max_{\pi_{\theta}} \mathbb{E}_{\pi_{\theta}} \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) + \alpha H(\pi(\cdot | s_t)) \right]
\]</div>
<p><strong><span class="arithmatex">\(\alpha\)</span></strong> is the temperature parameter that balances between exploration and exploitation.</p>
<h5 id="overcoming-exploration-bias-in-multimodal-q-functions">Overcoming Exploration Bias in Multimodal Q-Functions</h5>
<p>Here we want to explain the concept of a <strong>multimodal Q-function</strong> in reinforcement learning (RL), where the <strong>Q-value function</strong>, <span class="arithmatex">\(Q(s, a)\)</span>, represents the expected cumulative reward for taking action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span>. </p>
<p>In this context, the robot is in an initial state and has two possible passages to follow, which result in a <strong>bimodal Q-function</strong> (a function with two peaks). These peaks correspond to two different action choices, each leading to different potential outcomes.</p>
<h6 id="standard-rl-approach"><strong>Standard RL Approach</strong></h6>
<center> 
<img src="\assets\images\course_notes\advanced\SAC1.png"
    alt="sac1"
    style="float: center; margin-right: 10px;" 
    /> 
    </center>

<p>The <strong>grey curve</strong> represents the Q-function, which has two peaks, indicating two promising actions.A conventional RL approach typically assumes a unimodal (single-peaked) policy distribution, represented by the <strong>red curve</strong>.</p>
<p>This policy distribution is modeled as a Gaussian <span class="arithmatex">\(\mathcal{N}(\mu(s_t), \Sigma)\)</span> centered around the highest Q-value peak.</p>
<p>This setup results in exploration bias where the agent primarily explores around the highest peak and ignores the lower peak entirely.</p>
<h6 id="improved-exploration-strategy"><strong>Improved Exploration Strategy</strong></h6>
<p>Instead of using a unimodal Gaussian policy, a Boltzmann-weighted policy is used.
The <strong>policy distribution (green shaded area)</strong> is proportional to <span class="arithmatex">\(\exp(Q(s_t, a_t))\)</span>, meaning actions are sampled based on their Q-values. </p>
<p>This approach allows the agent to explore multiple high-reward actions and avoids the bias of ignoring one passage.
As a result, the agent recognizes both options, increasing the chance of finding the optimal path.</p>
<center> 
<img src="\assets\images\course_notes\advanced\SAC2.png"
    alt="SAC2"
    style="float: center; margin-right: 10px;" 
    /> 
    </center>

<p>This concept is relevant for <strong>actor-critic RL methods</strong> like <strong>Soft Actor-Critic (SAC)</strong>, which uses entropy to encourage diverse exploration.</p>
<h5 id="soft-policy">Soft Policy</h5>
<ul>
<li>Soft policy </li>
</ul>
<div class="arithmatex">\[
J(\pi) = \sum_{t=0}^{T} \mathbb{E}_{(s_t, a_t)\sim\rho_{\pi}} \left[ r(s_t, a_t) + \alpha\mathcal{H}(\pi(\cdot | s_t)) \right]
\]</div>
<p>With new objective function we need to define Value funciton and Q-value funciton again. </p>
<ul>
<li>Soft Q-value funciton</li>
</ul>
<div class="arithmatex">\[
Q(s_t, a_t) = r(s_t, a_t) + \gamma\mathbb{E}_{s_{t+1}\sim p}\left[ V(s_{t+1}) \right]
\]</div>
<ul>
<li>Soft Value function</li>
</ul>
<div class="arithmatex">\[
V(s_t) = \mathbb{E}_{a_t\sim \pi} \left[Q(s_t, a_t) - \text{log}\space\pi(a_t|s_t)\right]
\]</div>
<h5 id="soft-policy-iteration">Soft Policy Iteration</h5>
<p>Soft Policy Iteration is an entropy-regularized version of classical policy iteration, which consists of:</p>
<ol>
<li><em>Soft Policy Evaluation</em>: Estimating the soft Q-value function under the current policy.</li>
<li><em>Soft Policy Improvement</em>:  Updating the policy to maximize the soft Q-value function,incorporating entropy regularization.</li>
</ol>
<p>This process iteratively improves the policy while balancing exploration and exploitation.</p>
<h6 id="soft-policy-evaluation-critic-update">Soft Policy Evaluation (Critic Update)</h6>
<p>The goal of soft policy evaluation is to compute the expected return of a given policy <span class="arithmatex">\(\pi\)</span> under the maximum entropy objective, which modifies the standard Bellman equation by adding an entropy term. (SAC explicitly learns the Q-function for the current policy)</p>
<p>The soft Q-value function for a policy <span class="arithmatex">\(\pi\)</span> is updated using a modified Bellman operator <span class="arithmatex">\(T^{\pi}\)</span>:</p>
<div class="arithmatex">\[
T^\pi Q(s_t, a_t) = r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim p} [V(s_{t+1})]
\]</div>
<p>with substitution of <span class="arithmatex">\(V\)</span> we have :</p>
<div class="arithmatex">\[
T^\pi Q(s_t, a_t) = r(s_t, a_t) + \gamma \mathbb{E}_{\substack{s_{t+1} \sim p \\ a_{t+1} \sim \pi}} [Q(s_{t+1,}, a_{t+1}) - \text{log}\space\pi(a_{t+1}|s_{t+1})]
\]</div>
<p><strong>Key Result: Soft Bellman Backup Convergence</strong>  </p>
<p><strong>Theorem:</strong> By repeatedly applying the operator <span class="arithmatex">\(T^\pi\)</span>, the Q-value function converges to the true soft Q-value function for policy <span class="arithmatex">\(\pi\)</span>:  </p>
<div class="arithmatex">\[
Q_k \to Q^\pi \text{ as } k \to \infty
\]</div>
<p>Thus, we can estimate <span class="arithmatex">\(Q^\pi\)</span> iteratively.</p>
<h6 id="soft-policy-improvement-actor-update">Soft Policy Improvement (Actor Update)</h6>
<p>Once the Q-function is learned, we need to improve the policy using a gradient-based update. This means:</p>
<ul>
<li>Instead of directly maximizing Q-values, the policy is updated to optimize a modified objective that balances reward maximization and exploration.</li>
<li>The update is off-policy, meaning the policy can be trained using past experiences stored in a replay buffer, rather than requiring fresh samples like on-policy methods (e.g., PPO).</li>
</ul>
<p>The update is based on an <strong><em>exponential function of the Q-values</em></strong>:</p>
<div class="arithmatex">\[
\pi^*(a | s) \propto \exp (Q^\pi(s, a))
\]</div>
<p>which means the optimal policy is obtained by normalizing <span class="arithmatex">\(\exp (Q^\pi(s, a))\)</span> over all actions:</p>
<div class="arithmatex">\[
\pi^*(a | s) = \frac{\exp (Q^\pi(s, a))}{Z^\pi(s)}
\]</div>
<p>where <span class="arithmatex">\(Z^\pi(s)\)</span> is the partition function that ensures the distribution sums to 1.</p>
<p>For the policy improvement step, we update the policy distribution towards the softmax distribution for the current Q function.</p>
<div class="arithmatex">\[
\pi_{\text{new}} = \arg \min_{\pi' \in \Pi} D_{\text{KL}} \left( \pi'(\cdot | s) \, \bigg|\bigg| \, \frac{\exp (Q^\pi(s, \cdot))}{Z^\pi(s)} \right)
\]</div>
<p><strong>Key Result: Soft Policy Improvement Theorem</strong><br />
The new policy <span class="arithmatex">\(\pi_{\text{new}}\)</span> obtained via this update improves the expected soft return:</p>
<div class="arithmatex">\[
Q^{\pi_{\text{new}}}(s, a) \geq Q^{\pi_{\text{old}}}(s, a) \quad \forall (s, a)
\]</div>
<p>Thus, iterating this process leads to a better policy.</p>
<h6 id="convergence-of-soft-policy-iteration">Convergence of Soft Policy Iteration</h6>
<p>By alternating between soft policy evaluation and soft policy improvement, soft policy iteration converges to an optimal maximum entropy policy within the policy class <span class="arithmatex">\(\Pi\)</span>:</p>
<div class="arithmatex">\[
\pi^* = \arg \max_{\pi \in \Pi} \sum_t \mathbb{E}[r_t + \alpha H(\pi(\cdot | s_t))]
\]</div>
<p>However, this exact method is only feasible in the <strong><em>tabular setting</em></strong>. For <strong><em>continuous control</em></strong>, we approximate it using function approximators.</p>
<h5 id="soft-actor-critic">Soft Actor-Critic</h5>
<p>For complex learning domains with high-dimensional and/or continuous state-action spaces, it is mostly impossible to find exact solutions for the MDP. Thus, we must leverage function approximation (i.e. neural networks) to find a practical approximation to soft policy iteration. then we use stochastic gradient descent (SGD) to update parameters of these networks.</p>
<p>we model the value functions as expressive neural networks, and the policy as a Gaussian distribution over the action space with the mean and covariance given as neural network outputs with the current state as input.</p>
<h6 id="soft-value-function-v_psis"><strong>Soft Value function (<span class="arithmatex">\(V_{\psi}(s)\)</span>)</strong></h6>
<p>A separate soft value function which helps in stabilising the training process. The soft value function approximator minimizes the squared residual error as follows:</p>
<div class="arithmatex">\[
J_V(\psi) = \mathbb{E}_{s_{t} \sim \mathcal{D}} \left[ \frac{1}{2} \left( V_{\psi}(s_t) - \mathbb{E}_{a \sim \pi_{\phi}} [Q_{\theta}(s_t, a_t) - \log \pi_{\phi}(a_t | s_t)] \right)^2 \right]
\]</div>
<p>It means the learning of the state-value function <span class="arithmatex">\(V\)</span>, is done by minimizing the squared difference between the prediction of the value network and expected prediction of Q-function with the entropy of the policy, <span class="arithmatex">\(\pi\)</span>.</p>
<ul>
<li><span class="arithmatex">\(D\)</span> is the distribution of previously sampled states and actions, or a replay buffer.</li>
</ul>
<p><strong>Gradient Update for <span class="arithmatex">\(V_{\psi}(s)\)</span></strong></p>
<div class="arithmatex">\[
\hat{\nabla}_{\psi} J_V(\psi) = \nabla_{\psi} V_{\psi}(s_t) \left( V_{\psi}(s_t) - Q_{\theta}(s_t, a_t) + \log \pi_{\phi}(a_t | s_t) \right)
\]</div>
<p>where the actions are sampled according to the current policy, instead of the replay buffer.</p>
<h6 id="soft-q-funciton-q_thetas-a"><strong>Soft Q-funciton (<span class="arithmatex">\(Q_{\theta}(s, a)\)</span>)</strong></h6>
<p>We minimize the soft Q-function parameters by using the soft Bellman residual provided here:</p>
<div class="arithmatex">\[
J_Q(\theta) = \mathbb{E}_{(s_{t}, a_t) \sim \mathcal{D}} \left[ \frac{1}{2} \left( Q_{\theta}(s_t, a_t) - \hat{Q}(s_t, a_t)\right)^2 \right]
\]</div>
<p>with : </p>
<div class="arithmatex">\[
\hat{Q}(s_t, a_t) = r(s_t, a_t) + \gamma \space \mathbb{E}_{s_{t+1} \sim p} [V_{\bar{\psi}}(s_{t+1})]
\]</div>
<p><strong>Gradient Update for <span class="arithmatex">\(Q_{\theta}\)</span></strong>:</p>
<div class="arithmatex">\[
\hat{\nabla}_\theta J_Q(\theta) = \nabla_{\theta} Q_{\theta}(s_t, a_t) \left( Q_{\theta}(s_t, a_t) - r(s_t, a_t) - \gamma V_{\bar{\psi}}(s_{t+1}) \right)
\]</div>
<p>A <strong>target value function</strong> <span class="arithmatex">\(V_{\bar{\psi}}\)</span> (exponentially moving average of <span class="arithmatex">\(V_{\psi}\)</span>) is used to stabilize training.</p>
<hr />
<details class="note" open="open">
<summary>More Explanation About Target Networks</summary>
<p>The use of target networks is motivated by a problem in training <span class="arithmatex">\(V\)</span> network. If you go back to the objective functions in the Theory section, you will find that the target for the <span class="arithmatex">\(Q\)</span> network training depends on the <span class="arithmatex">\(V\)</span> Network and the target for the <span class="arithmatex">\(V\)</span> Network depends on the <span class="arithmatex">\(Q\)</span> network (this makes sense because we are trying to enforce Bellman Consistency between the two functions). Because of this, the <span class="arithmatex">\(V\)</span> network has a target thatâ€™s indirectly dependent on itself which means that the <span class="arithmatex">\(V\)</span> networkâ€™s target depends on the same parameters we are trying to train. This makes training very unstable.</p>
<p>The solution is to use a set of parameters which comes close to the parameters of the main <span class="arithmatex">\(V\)</span> network, but with a time delay. Thus we create a second network which lags the main network called the target network. There are two ways to go about this.</p>
<ol>
<li>
<p><strong>Periodic Hard Update</strong></p>
<p>This method involves completely overwriting the target networkâ€™s parameters (<strong><span class="arithmatex">\(\theta^{-}\)</span></strong>) with the main networkâ€™s parameters (<strong><span class="arithmatex">\(\theta\)</span></strong>) at regular intervals (after a fixed number of steps).</p>
<ul>
<li>
<p><strong>Purpose</strong>:<br />
The periodic hard update ensures the target network aligns closely with the main network, preventing significant divergence between them.</p>
</li>
<li>
<p><strong>Key Characteristics</strong>:  </p>
<ul>
<li>Sudden updates: The target network parameters are replaced entirely at specific intervals.</li>
<li>Simplicity: Easy to implement without requiring complex calculations.</li>
<li>Stability: Reduces computational overhead during training.</li>
</ul>
</li>
<li>
<p><strong>Equation</strong>: </p>
</li>
</ul>
<div class="arithmatex">\[
\theta^{-} \leftarrow \theta
\]</div>
<ul>
<li><strong>Drawback</strong>:<br />
The abrupt change can lead to instability in learning if the main network's parameters shift significantly during training. It may result in fluctuations in performance for some environments.</li>
</ul>
</li>
<li>
<p><strong>Soft Update</strong>  </p>
<p>Soft updates use <strong>Polyak averaging</strong> (a kind of moving averaging), a method where the target networkâ€™s parameters (<strong><span class="arithmatex">\(\theta^{-}\)</span></strong>) are updated gradually based on a weighted combination of the current main networkâ€™s parameters (<strong><span class="arithmatex">\(\theta\)</span></strong>) and the existing target networkâ€™s parameters.</p>
<ul>
<li>
<p><strong>Purpose</strong>:<br />
This smooth transition avoids abrupt shifts and allows the target network to slowly converge towards the main networkâ€™s parameters, promoting stability in learning.</p>
</li>
<li>
<p><strong>Key Characteristics</strong>:  </p>
<ul>
<li>Incremental updates: Parameters are updated gradually at each training step.</li>
<li>Flexibility: The weighting factor <strong><span class="arithmatex">\(\tau\)</span></strong> (a small constant, e.g., 0.001) controls the speed of convergence.(<span class="arithmatex">\(\tau \ll 1\)</span>)</li>
<li>Stability: Ensures smooth transitions and minimizes sudden changes.</li>
</ul>
</li>
<li>
<p><strong>Equation</strong>: </p>
</li>
</ul>
<div class="arithmatex">\[
\theta^{-} \leftarrow \tau \theta + (1-\tau) \theta^{-}
\]</div>
<ul>
<li>
<p><strong>Advantages</strong>:  </p>
<ul>
<li>Gradual updates reduce instability in learning.</li>
<li>Ideal for environments requiring smooth and stable convergence.</li>
</ul>
</li>
<li>
<p><strong>Trade-off</strong>:<br />
Slower adaptation to the main networkâ€™s parameters compared to hard updates, but the added stability usually outweighs this drawback.</p>
</li>
</ul>
</li>
</ol>
<p><center> 
<img src="\assets\images\course_notes\advanced\target_network1.png"
    alt="Q function approximation without target network"
    style="float: center; margin-right: 10px;" 
    /> 
    </center>
<center> 
<img src="\assets\images\course_notes\advanced\target_network_2.png"
    alt="Q function approximation with target network"
    style="float: center; margin-right: 10px;" 
    /> 
    </center></p>
<p><em>source: <a href="https://livebook.manning.com/concept/reinforcement-learning/target-network#:~:text=By%20using%20target%20networks%2C%20we,a%20new%20one%20is%20set.">concept target network in category reinforcement learning</a></em></p>
<p><strong>other links:</strong></p>
<p><a href="https://abhishm.github.io/DQN/">Deep Q-Network -- Tips, Tricks, and Implementation</a></p>
<p><a href="https://ai.stackexchange.com/questions/21485/how-and-when-should-we-update-the-q-target-in-deep-q-learning">How and when should we update the Q-target in deep Q-learning?</a></p>
</details>
<h6 id="policy-network-pi_phias"><strong>Policy network (<span class="arithmatex">\(\pi_{\phi}(a|s)\)</span>)</strong></h6>
<p>The policy <span class="arithmatex">\(\pi_{\phi}(a | s)\)</span> is updated using the soft policy improvement step, minimizing the KL-divergence:</p>
<div class="arithmatex">\[
J_{\pi}(\phi) = \mathbb{E}_{s_t \sim \mathcal{D}} \left[ D_{\text{KL}} \left( \pi_{\phi}(\cdot | s_t) \bigg\| \frac{\exp(Q_{\theta}(s_t, \cdot))}{Z_{\theta}(s_t)} \right) \right]
\]</div>
<p>Instead of solving this directly, SAC reparameterizes the policy using:</p>
<div class="arithmatex">\[
a_t = f_{\phi}(\epsilon_t; s_t)
\]</div>
<p>This trick is used to make sure that sampling from the policy is a differentiable process so that there are no problems in backpropagating the errors.  <span class="arithmatex">\(\epsilon_t\)</span> is random noise vector sampled from fixed distribution (e.g., Spherical Gaussian).</p>
<details class="note" open="open">
<summary>Why Reparameterization is Needed?</summary>
<p>In reinforcement learning, the policy <span class="arithmatex">\(\pi(a | s)\)</span> often outputs a probability distribution over actions rather than deterministic actions. The standard way to sample an action is:</p>
<div class="arithmatex">\[
a_t \sim \pi_{\phi}(a_t | s_t)
\]</div>
<p>However, this sampling operation blocks gradient flow during backpropagation, preventing efficient training using stochastic gradient descent (SGD).</p>
<p>Instead of directly sampling <span class="arithmatex">\(a_t\)</span> from <span class="arithmatex">\(\pi_{\phi}(a_t | s_t)\)</span>, we transform a simple noise variable into an action:</p>
<div class="arithmatex">\[
a_t = f_{\phi}(\epsilon_t, s_t)
\]</div>
<ul>
<li><span class="arithmatex">\(\epsilon_t \sim \mathcal{N}(0, I)\)</span> is sampled from a fixed noise distribution (e.g., a Gaussian).</li>
<li><span class="arithmatex">\(f_{\phi}(\epsilon_t, s_t)\)</span> is a differentiable function (often a neural network) that maps noise to an action.</li>
</ul>
<p>For a Gaussian policy in SAC, the action is computed as:</p>
<div class="arithmatex">\[
a_t = \mu_{\phi}(s_t) + \sigma_{\phi}(s_t) \cdot \epsilon_t
\]</div>
<p>So instead of sampling from <span class="arithmatex">\(\mathcal{N}(\mu, \sigma^2)\)</span> directly, we sample from a fixed standard normal and transform it using a differentiable function.This makes the policy differentiable, allowing gradients to flow through <span class="arithmatex">\(\mu_{\phi}\)</span> and <span class="arithmatex">\(\sigma_{\phi}\)</span>.</p>
<p><center> 
<img src="\assets\images\course_notes\advanced\trick.png"
    alt="trick"
    style="float: center; margin-right: 10px;" 
    /> 
    </center>
<center> 
<img src="\assets\images\course_notes\advanced\trick2.png"
    alt="trick2"
    style="float: center; margin-right: 10px;" 
    /> 
    </center></p>
<ul>
<li><strong>Continuous Action Generation</strong></li>
</ul>
<p>In a continuous action space soft actor-critic agent, the neural network in the actor takes the current observation and generates two outputs, one for the mean and the other for the standard deviation. To select an action, the actor randomly selects an unbounded action from this Gaussian distribution. If the soft actor-critic agent needs to generate bounded actions, the actor applies tanh and scaling operations to the action sampled from the Gaussian distribution.</p>
<p>During training, the agent uses the unbounded Gaussian distribution to calculate the entropy of the policy for the given observation.
<center> 
<img src="\assets\images\course_notes\advanced\bounded.png"
    alt="bounded"
    style="float: center; margin-right: 10px;" 
    /> 
    </center></p>
<ul>
<li><strong>Discrete Action Generation</strong></li>
</ul>
<p>In a discrete action space soft actor-critic agent, the actor takes the current observation and generates a categorical distribution, in which each possible action is associated with a probability. Since each action that belongs to the finite set is already assumed feasible, no bounding is needed.</p>
<p>During training, the agent uses the categorical distribution to calculate the entropy of the policy for the given observation.</p>
</details>
<p>if we rewrite the equation we have:</p>
<div class="arithmatex">\[
J_{\pi}(\phi) = \mathbb{E}_{s_t \sim \mathcal{D}, \epsilon_t \sim \mathcal{N}} \left[ \text{log}\space\pi_{\phi} \left(f_{\phi}(\epsilon_t; s_t) | s_t \right) - Q_{\theta}(s_t,f_{\phi}(\epsilon_t; s_t) \right]
\]</div>
<p>where <span class="arithmatex">\(\pi_{\phi}\)</span> is defined implicitly in terms of <span class="arithmatex">\(f_{\phi}\)</span>, and we have
noted that the partition function is independent of <span class="arithmatex">\(\phi\)</span> and can
thus be omitted.</p>
<p><strong>Policy Gradient Update</strong></p>
<div class="arithmatex">\[
\hat{\nabla}_{\phi} J_{\pi}(\phi) = \nabla_{\phi} \log \pi_{\phi}(a_t | s_t) + \left( \nabla_{a_t} \log \pi_{\phi}(a_t | s_t)- \nabla_{a_t} Q_{\theta}(s_t, a_t) \right) \nabla_{\phi} f_{\phi}(\epsilon_t; s_t)
\]</div>
<div class="admonition note">
<p class="admonition-title">SAC main steps</p>
<ol>
<li>
<p><strong>Q-function Update</strong>:</p>
<div class="arithmatex">\[
Q(s, a) \leftarrow r(s, a) + \mathbb{E}_{s' \sim p, a' \sim \pi} \left[ Q(s', a') - \log \pi(a' | s') \right].
\]</div>
<p>This update converges to <span class="arithmatex">\(Q^\pi\)</span>, the Q-function under the current policy <span class="arithmatex">\(\pi\)</span>.</p>
</li>
<li>
<p><strong>Policy Update</strong>:</p>
<div class="arithmatex">\[
\pi_{\text{new}} = \arg\min_{\pi'} D_{KL} \left( \pi'(\cdot | s) \Bigg\| \frac{1}{Z} \exp Q^{\pi_{\text{old}}}(s, \cdot) \right).
\]</div>
<p>In practice, only one gradient step is taken on this objective to ensure stability.</p>
</li>
<li>
<p><strong>Interaction with the Environment</strong>:<br />
    Collect more data by interacting with the environment using the updated policy.</p>
</li>
</ol>
</div>
<p><strong>Algorithm: Soft Actor-Critic</strong></p>
<p>Initialize parameter vectors <span class="arithmatex">\(\psi, \bar{\psi}, \theta, \phi\)</span></p>
<p><strong>for</strong> each iteration <strong>do</strong><br />
&nbsp;&nbsp;&nbsp;&nbsp;<strong>for</strong> each environment step <strong>do</strong><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="arithmatex">\(a_t \sim \pi_{\phi}(a_t | s_t)\)</span><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="arithmatex">\(s_{t+1} \sim p(s_{t+1} | s_t, a_t)\)</span><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="arithmatex">\(\mathcal{D} \gets \mathcal{D} \cup \{(s_t, a_t, r(s_t, a_t), s_{t+1})\}\)</span><br />
&nbsp;&nbsp;&nbsp;&nbsp;<strong>end for</strong>  </p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>for</strong> each gradient step <strong>do</strong><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="arithmatex">\(\psi \gets \psi - \lambda \hat{\nabla}_{\psi} J_V(\psi)\)</span><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="arithmatex">\(\theta_i \gets \theta_i - \lambda_Q \hat{\nabla}_{\theta_i} J_Q(\theta_i) \quad \text{for } i \in \{1,2\}\)</span><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="arithmatex">\(\phi \gets \phi - \lambda_{\pi} \hat{\nabla}_{\phi} J_{\pi}(\phi)\)</span><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="arithmatex">\(\bar{\psi} \gets \tau \psi + (1 - \tau) \bar{\psi}\)</span><br />
&nbsp;&nbsp;&nbsp;&nbsp;<strong>end for</strong><br />
<strong>end for</strong></p>
<hr />
<h4 id="conclusion">Conclusion</h4>
<h5 id="reward-to-go_1"><strong>Reward-to-Go</strong></h5>
<p>Reward-to-Go computes the sum of future rewards starting from a specific timestep, ensuring that the agent optimizes actions based on expected future returns rather than full trajectory rewards.</p>
<div class="arithmatex">\[
R_t = \sum_{k=t}^{T} \gamma^{k-t} r_k
\]</div>
<p>where <span class="arithmatex">\(\gamma\)</span> is the discount factor.</p>
<ul>
<li>
<p><strong>Advantages:</strong>  </p>
<ul>
<li>More efficient than using complete trajectory returns.  </li>
<li>Enhances learning by assigning appropriate importance to past actions based on their future impact.</li>
</ul>
</li>
<li>
<p><strong>Disadvantages:</strong>  </p>
<ul>
<li>Still introduces variance in training.  </li>
<li>Can be unstable if the reward structure is sparse or highly delayed.  </li>
</ul>
</li>
</ul>
<h5 id="advantage-estimation"><strong>Advantage Estimation</strong></h5>
<p>The advantage function quantifies how much better a specific action is compared to the expected return under the current policy. It is defined as:  </p>
<div class="arithmatex">\[
A(s_t, a_t) = Q(s_t, a_t) - V(s_t)
\]</div>
<ul>
<li>
<p><strong>Advantages:</strong>  </p>
<ul>
<li>Reduces variance in policy gradient updates compared to directly using return estimates.  </li>
<li>Helps improve stability in training by distinguishing between good and bad actions.  </li>
</ul>
</li>
<li>
<p><strong>Disadvantages:</strong>  </p>
<ul>
<li>Requires an accurate value function estimation.  </li>
<li>Can still introduce bias if the value function is not well-trained.  </li>
</ul>
</li>
</ul>
<h5 id="generalized-advantage-estimation-gae_1"><strong>Generalized Advantage Estimation (GAE)</strong></h5>
<p>GAE improves advantage estimation by introducing a <strong>trade-off between bias and variance</strong>, using an exponentially-weighted sum of temporal-difference (TD) residuals:</p>
<div class="arithmatex">\[
A_t^{GAE(\lambda)} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}
\]</div>
<p>where <span class="arithmatex">\(\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)\)</span>. The parameter <span class="arithmatex">\(\lambda\)</span> determines the trade-off:  </p>
<ul>
<li><strong>Low <span class="arithmatex">\(\lambda\)</span> (close to 0)</strong> â†’ More bias, less variance (TD-learning).  </li>
<li>
<p><strong>High <span class="arithmatex">\(\lambda\)</span> (close to 1)</strong> â†’ Less bias, more variance (Monte Carlo estimation).  </p>
</li>
<li>
<p><strong>Advantages:</strong>  </p>
<ul>
<li>Provides a tunable balance between bias and variance.  </li>
<li>Improves sample efficiency by reducing variance in advantage estimates.  </li>
</ul>
</li>
<li>
<p><strong>Disadvantages:</strong>  </p>
<ul>
<li>Requires careful tuning of <span class="arithmatex">\(\lambda\)</span> for optimal performance.  </li>
<li>Slightly increases computational overhead due to extra calculations.  </li>
</ul>
</li>
</ul>
<h4 id="comparison-of-reward-to-go-advantage-estimation-and-gae"><strong>Comparison of Reward-to-Go, Advantage Estimation, and GAE</strong></h4>
<table>
<thead>
<tr>
<th>Method</th>
<th>Variance Reduction</th>
<th>Bias</th>
<th>Stability</th>
<th>Computational Cost</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Reward-to-Go</strong></td>
<td>Moderate</td>
<td>No bias</td>
<td>Moderate</td>
<td>Low</td>
</tr>
<tr>
<td><strong>Advantage Estimation</strong></td>
<td>High</td>
<td>Some bias</td>
<td>High</td>
<td>Moderate</td>
</tr>
<tr>
<td><strong>GAE</strong></td>
<td>Adjustable (via <span class="arithmatex">\(\lambda\)</span>)</td>
<td>Adjustable</td>
<td>High</td>
<td>Higher</td>
</tr>
</tbody>
</table>
<h5 id="proximal-policy-optimization-ppo_1"><strong>Proximal Policy Optimization (PPO)</strong></h5>
<p>PPO is an <strong>on-policy</strong> actor-critic algorithm that improves stability by constraining policy updates with a clipped objective function. It builds upon Trust Region Policy Optimization (TRPO) while being simpler to implement.  </p>
<ul>
<li>
<p><strong>Advantages:</strong>  </p>
<ul>
<li>Ensures stable updates by limiting drastic policy changes.  </li>
<li>Works well in large-scale reinforcement learning problems.  </li>
<li>Simple to implement compared to TRPO.  </li>
<li>Effective for environments with discrete and continuous action spaces.  </li>
</ul>
</li>
<li>
<p><strong>Disadvantages:</strong>  </p>
<ul>
<li>As an on-policy method, it requires more samples for training.  </li>
<li>Less sample-efficient than off-policy methods like DDPG and SAC.  </li>
<li>Clipping can sometimes slow down convergence if not tuned properly.  </li>
</ul>
</li>
</ul>
<h5 id="deep-deterministic-policy-gradient-ddpg"><strong>Deep Deterministic Policy Gradient (DDPG)</strong></h5>
<p>DDPG is an <strong>off-policy, model-free</strong> algorithm designed for <strong>continuous action spaces</strong>. It extends <strong>Deterministic Policy Gradient (DPG)</strong> by incorporating <strong>experience replay</strong> and <strong>target networks</strong>, inspired by Deep Q-Networks (DQN).  </p>
<ul>
<li>
<p><strong>Advantages:</strong>  </p>
<ul>
<li>More sample-efficient than on-policy methods like PPO.  </li>
<li>Can handle high-dimensional continuous action spaces effectively.  </li>
<li>Uses replay buffers to break correlation in training data.  </li>
</ul>
</li>
<li>
<p><strong>Disadvantages:</strong>  </p>
<ul>
<li>Highly sensitive to hyperparameters like learning rates and noise scaling.  </li>
<li>Poor exploration due to its deterministic policy, requiring techniques like Ornstein-Uhlenbeck (OU) noise.  </li>
<li>Prone to instability due to function approximation errors in Q-learning.  </li>
</ul>
</li>
</ul>
<h5 id="soft-actor-critic-sac_1"><strong>Soft Actor-Critic (SAC)</strong></h5>
<p>SAC is an <strong>off-policy</strong> algorithm that improves upon DDPG by introducing <strong>entropy regularization</strong>, which encourages exploration and robustness. It uses <strong>two Q-networks (double Q-learning)</strong> to mitigate overestimation bias and a <strong>stochastic policy</strong> for improved exploration.  </p>
<ul>
<li>
<p><strong>Advantages:</strong>  </p>
<ul>
<li>Better exploration due to entropy regularization.  </li>
<li>More stable than DDPG because of double Q-learning.  </li>
<li>Handles high-dimensional continuous control tasks efficiently.  </li>
<li>More sample-efficient than PPO.  </li>
</ul>
</li>
<li>
<p><strong>Disadvantages:</strong>  </p>
<ul>
<li>Higher computational cost due to multiple Q-function updates.  </li>
<li>Requires careful tuning of entropy coefficient <span class="arithmatex">\(\alpha\)</span>.  </li>
<li>Slower than DDPG in deterministic settings where exploration is not an issue.  </li>
</ul>
</li>
</ul>
<h4 id="comparison-table-ppo-vs-ddpg-vs-sac"><strong>Comparison Table: PPO vs. DDPG vs. SAC</strong></h4>
<table>
<thead>
<tr>
<th>Algorithm</th>
<th>Type</th>
<th>Sample Efficiency</th>
<th>Stability</th>
<th>Exploration</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>PPO</strong></td>
<td>On-policy</td>
<td>Low</td>
<td>High</td>
<td>Moderate</td>
</tr>
<tr>
<td><strong>DDPG</strong></td>
<td>Off-policy</td>
<td>High</td>
<td>Moderate</td>
<td>Weak (deterministic)</td>
</tr>
<tr>
<td><strong>SAC</strong></td>
<td>Off-policy</td>
<td>High</td>
<td>High</td>
<td>Strong (entropy regularization)</td>
</tr>
</tbody>
</table>
<h4 id="final-thoughts"><strong>Final Thoughts</strong></h4>
<p>Actor-critic methods such as <strong>PPO, DDPG, and SAC</strong> have significantly improved reinforcement learning, making it more scalable and sample-efficient.  </p>
<ul>
<li><strong>PPO</strong> is widely used for its stability and ease of implementation but is less sample-efficient.  </li>
<li><strong>DDPG</strong> works well in continuous control but suffers from poor exploration and instability.  </li>
<li><strong>SAC</strong> improves upon DDPG by adding entropy regularization, leading to better exploration and stability.  </li>
</ul>
<h2 id="authors">Author(s)</h2>
<div class="grid cards">
<ul>
<li><img align="left" alt="Instructor Avatar" src="/assets/images/staff/Ahmad-Karami.jpg" width="150" />
    <span class="description">
        <p><strong>Ahmad Karami</strong></p>
        <p>Teaching Assistant</p>
        <p><a href="mailto:ahmad.karami77@yahoo.com">ahmad.karami77@yahoo.com</a></p>
        <p>
        <a href="https://www.linkedin.com/in/ahmad-karami-8a6a14255" target="_blank"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M100.3 448H7.4V148.9h92.9zM53.8 108.1C24.1 108.1 0 83.5 0 53.8c0-14.3 5.7-27.9 15.8-38S39.6 0 53.8 0s27.9 5.7 38 15.8 15.8 23.8 15.8 38c0 29.7-24.1 54.3-53.8 54.3M447.9 448h-92.7V302.4c0-34.7-.7-79.2-48.3-79.2-48.3 0-55.7 37.7-55.7 76.7V448h-92.8V148.9h89.1v40.8h1.3c12.4-23.5 42.7-48.3 87.9-48.3 94 0 111.3 61.9 111.3 142.3V448z"/></svg></span></a>
        </p>
    </span></li>
<li><img align="left" alt="Instructor Avatar" src="/assets/images/staff/Hamidreza-Ebrahimpour.jpg" width="150" />
    <span class="description">
        <p><strong>Hamidreza Ebrahimpour</strong></p>
        <p>Teaching Assistant</p>
        <p><a href="mailto:ebrahimpour.7879@gmail.com">ebrahimpour.7879@gmail.com</a></p>
        <p>
        <a href="https://github.com/hamidRezA7878" target="_blank"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg></span></a>
        <a href="https://linkedin.com/in/hamidreza-ebrahimpour78" target="_blank"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M100.3 448H7.4V148.9h92.9zM53.8 108.1C24.1 108.1 0 83.5 0 53.8c0-14.3 5.7-27.9 15.8-38S39.6 0 53.8 0s27.9 5.7 38 15.8 15.8 23.8 15.8 38c0 29.7-24.1 54.3-53.8 54.3M447.9 448h-92.7V302.4c0-34.7-.7-79.2-48.3-79.2-48.3 0-55.7 37.7-55.7 76.7V448h-92.8V148.9h89.1v40.8h1.3c12.4-23.5 42.7-48.3 87.9-48.3 94 0 111.3 61.9 111.3 142.3V448z"/></svg></span></a>
        </p>
    </span></li>
<li><img align="left" alt="Instructor Avatar" src="/assets/images/staff/Hesam-Hosseini.jpg" width="150" />
    <span class="description">
        <p><strong>Hesam Hosseini</strong></p>
        <p>Teaching Assistant</p>
        <p><a href="mailto:hesam138122@gmail.com">hesam138122@gmail.com</a></p>
        <p>
        <a href="https://scholar.google.com/citations?user=ODTtV1gAAAAJ&amp;hl=en" target="_blank"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M390.9 298.5s0 .1.1.1c9.2 19.4 14.4 41.1 14.4 64C405.3 445.1 338.5 512 256 512s-149.3-66.9-149.3-149.3c0-22.9 5.2-44.6 14.4-64 1.7-3.6 3.6-7.2 5.6-10.7q6.6-11.4 15-21.3c27.4-32.6 68.5-53.3 114.4-53.3 33.6 0 64.6 11.1 89.6 29.9 9.1 6.9 17.4 14.7 24.8 23.5 5.6 6.6 10.6 13.8 15 21.3 2 3.4 3.8 7 5.5 10.5zm26.4-18.8c-30.1-58.4-91-98.4-161.3-98.4s-131.2 40-161.3 98.4L0 202.7 256 0l256 202.7-94.7 77.1z"/></svg></span></a>
        <a href="https://github.com/Sam-the-first" target="_blank"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg></span></a>
        <a href="https://www.linkedin.com/in/hesam-hosseini-b57092259" target="_blank"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M100.3 448H7.4V148.9h92.9zM53.8 108.1C24.1 108.1 0 83.5 0 53.8c0-14.3 5.7-27.9 15.8-38S39.6 0 53.8 0s27.9 5.7 38 15.8 15.8 23.8 15.8 38c0 29.7-24.1 54.3-53.8 54.3M447.9 448h-92.7V302.4c0-34.7-.7-79.2-48.3-79.2-48.3 0-55.7 37.7-55.7 76.7V448h-92.8V148.9h89.1v40.8h1.3c12.4-23.5 42.7-48.3 87.9-48.3 94 0 111.3 61.9 111.3 142.3V448z"/></svg></span></a>
        </p>
    </span></li>
</ul>
</div>
<h1 id="references">References</h1>
<ol>
<li>
<p><a href="http://172.27.48.15/Resources/Books/Nikolic%20L.%20Reinforcement%20Learning%20Explained.%20A%20Step-by-Step%20Guide...2023.pdf">Reinforcement Learning Explained</a></p>
</li>
<li>
<p><a href="http://172.27.48.15/Resources/Books/Textbooks/An%20Introduction%20to%20Deep%20Reinforcement%20Learning.pdf">An Introduction to Deep Reinforcement Learning</a></p>
</li>
<li>
<p><a href="http://172.27.48.15/Resources/Books/Juhyoung%20Lee%2C%20Hoi-Jun%20Yoo%20-%20Deep%20Reinforcement%20Learning%20Processor%20Design%20for%20Mobile%20Applications-Springer%20%282023%29.pdf">Deep Reinforcement Learning Processor Design for Mobile Applications</a></p>
</li>
<li>
<p><a href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/">Policy Gradient Algorithms</a></p>
</li>
<li>
<p><a href="http://172.27.48.15/Resources/Books/Textbooks/Aske%20Plaat%20-%20Deep%20Reinforcement%20Learning-arXiv%20%282023%29.pdf">Deep Reinforcement Learning</a></p>
</li>
<li>
<p><a href="https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf">Reinforcement Learning (BartoSutton)</a></p>
</li>
<li>
<p><a href="https://spinningup.openai.com/en/latest/index.html">Spinning Up in Deep RL!</a></p>
</li>
<li>
<p><a href="https://docs.cleanrl.dev/rl-algorithms/overview/">CleanRL Algorithms</a></p>
</li>
</ol>







  
  



  



  <form class="md-feedback" name="feedback" hidden>
    <fieldset>
      <legend class="md-feedback__title">
        Was this page helpful?
      </legend>
      <div class="md-feedback__inner">
        <div class="md-feedback__list">
          
            <button class="md-feedback__icon md-icon" type="submit" title="This page was helpful" data-md-value="1">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m7 0c0 .8-.7 1.5-1.5 1.5S14 10.3 14 9.5 14.7 8 15.5 8s1.5.7 1.5 1.5m-5 7.73c-1.75 0-3.29-.73-4.19-1.81L9.23 14c.45.72 1.52 1.23 2.77 1.23s2.32-.51 2.77-1.23l1.42 1.42c-.9 1.08-2.44 1.81-4.19 1.81"/></svg>
            </button>
          
            <button class="md-feedback__icon md-icon" type="submit" title="This page could be improved" data-md-value="0">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10m-6.5-4c.8 0 1.5.7 1.5 1.5s-.7 1.5-1.5 1.5-1.5-.7-1.5-1.5.7-1.5 1.5-1.5M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m2 4.5c1.75 0 3.29.72 4.19 1.81l-1.42 1.42C14.32 16.5 13.25 16 12 16s-2.32.5-2.77 1.23l-1.42-1.42C8.71 14.72 10.25 14 12 14"/></svg>
            </button>
          
        </div>
        <div class="md-feedback__note">
          
            <div data-md-value="1" hidden>
              
              
                
              
              
              
                
                
              
              Thanks for your feedback!
            </div>
          
            <div data-md-value="0" hidden>
              
              
                
              
              
              
                
                
              
              Thanks for your feedback! Help us improve this page by using our <a href="..." target="_blank" rel="noopener">feedback form</a>.
            </div>
          
        </div>
      </div>
    </fieldset>
  </form>


<script src="https://giscus.app/client.js"
        data-repo="DeepRLCourse/DeepRLCourse.github.io"
        data-repo-id="R_kgDOMPOFbQ"
        data-category="Announcements"
        data-category-id="DIC_kwDOMPOFbc4CmGY6"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="1"
        data-input-position="top"
        data-theme="preferred_color_scheme"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>

                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Made with â¤ï¸ in Robust and Interpretable Machine Learning Lab
    </div>
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.top", "toc.integrate"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>