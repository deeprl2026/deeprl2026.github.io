
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="This page provides an in-depth exploration of policy-based methods in reinforcement learning, focusing on their theoretical foundations, practical implementations, and advantages over value-based methods. Topics include policy gradient theorem, variance reduction techniques, REINFORCE algorithm, actor-critic methods, and their applications in continuous action spaces. The content is enriched with mathematical proofs, examples, and visual aids to enhance understanding.">
      
      
      
        <link rel="canonical" href="https://DeepRL2026.github.io/course_notes/policy-based/">
      
      
      
      
        
      
      
      <link rel="icon" href="../../assets/favicon/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.3">
    
    
      
        <title>Week 3: Policy-Based Methods - Deep RL Course</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300,300i,400,400i,700,700i%7CRed+Hat+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Ubuntu";--md-code-font:"Red Hat Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
      <link rel="stylesheet" href="../../stylesheets/vazirmatn.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-Q1YT8K39WE"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-Q1YT8K39WE",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-Q1YT8K39WE",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
  
<meta property="og:type" content="website" />
<meta property="og:title" content="Week 3: Policy-Based Methods - Deep RL Course" />
<meta property="og:description" content="This page provides an in-depth exploration of policy-based methods in reinforcement learning, focusing on their theoretical foundations, practical implementations, and advantages over value-based methods. Topics include policy gradient theorem, variance reduction techniques, REINFORCE algorithm, actor-critic methods, and their applications in continuous action spaces. The content is enriched with mathematical proofs, examples, and visual aids to enhance understanding." />
<meta property="og:image" content="https://DeepRL2026.github.io/assets/images/social/course_notes/policy-based.png" />
<meta property="og:image:type" content="image/png" />
<meta property="og:image:width" content="1200" />
<meta property="og:image:height" content="630" />
<meta property="og:url" content="https://DeepRL2026.github.io/course_notes/policy-based/" />
<meta property="twitter:card" content="summary_large_image" />
<meta property="twitter:title" content="Week 3: Policy-Based Methods - Deep RL Course" />
<meta property="twitter:description" content="This page provides an in-depth exploration of policy-based methods in reinforcement learning, focusing on their theoretical foundations, practical implementations, and advantages over value-based methods. Topics include policy gradient theorem, variance reduction techniques, REINFORCE algorithm, actor-critic methods, and their applications in continuous action spaces. The content is enriched with mathematical proofs, examples, and visual aids to enhance understanding." />
<meta property="twitter:image" content="https://DeepRL2026.github.io/assets/images/social/course_notes/policy-based.png" />
</head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#week-3-policy-based-methods" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Deep RL Course" class="md-header__button md-logo" aria-label="Deep RL Course" data-md-component="logo">
      
  <img src="../../assets/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Deep RL Course
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Week 3: Policy-Based Methods
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="red"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="red"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  Home

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  Course Notes

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../lectures/lecture1/" class="md-tabs__link">
          
  
  
  Lectures

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  TA Sessions

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  Homeworks

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  Projects

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../exams/" class="md-tabs__link">
          
  
  
  Exams

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../resources/" class="md-tabs__link">
          
  
  
  Resources

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
    
      

    

    

        

            
                  
            

        

            

        
    

    


    <!-- Navigation -->
    
        
        <div
            class="md-sidebar md-sidebar--primary"
            data-md-component="sidebar"
            data-md-type="navigation"
            
        >
            <div class="md-sidebar__scrollwrap">
            <div class="md-sidebar__inner">
                <!--
  Copyright (c) 2016-2025 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->



<!-- Determine classes -->


  


  


<!-- Navigation -->
<nav
  class="md-nav md-nav--primary md-nav--lifted"
  aria-label="Navigation"
  data-md-level="0"
>

  <!-- Site title -->
  <label class="md-nav__title" for="__drawer">
    <a
      href="../.."
      title="Deep RL Course"
      class="md-nav__button md-logo"
      aria-label="Deep RL Course"
      data-md-component="logo"
    >
      
  <img src="../../assets/logo.png" alt="logo">

    </a>
    Deep RL Course
  </label>

  <!-- Repository information -->
  

  <!-- Navigation list -->
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Home
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Course Notes
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Course Notes
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Lectures
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Lectures
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../lectures/lecture1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Lecture1
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../lectures/lecture2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Lecture2
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    TA Sessions
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    TA Sessions
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Homeworks
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    Homeworks
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Projects
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    Projects
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Exams
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            
  
    Exams
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../exams/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Previous Semesters
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Resources
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            
  
    Resources
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../resources/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Resources
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
            </div>
            </div>
        </div>
    

    <!-- Table of contents -->
    
        
        <div
            class="md-sidebar md-sidebar--secondary"
            data-md-component="sidebar"
            data-md-type="toc"
            
        >
            <div class="md-sidebar__scrollwrap">
            <div class="md-sidebar__inner">
                <!--
  Copyright (c) 2016-2025 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->

<!-- Determine title -->




<!-- Table of contents -->
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  

  <!--
    Check whether the content starts with a level 1 headline. If it does, the
    top-level anchor must be skipped, since it would be redundant to the link
    to the current page that is located just above the anchor. Therefore we
    directly continue with the children of the anchor.
  -->
  
  
    
  

  <!-- Table of contents title and list -->
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#why-policy-gradient-methods" class="md-nav__link">
    <span class="md-ellipsis">
      
        Why Policy Gradient Methods?
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#policy-gradient" class="md-nav__link">
    <span class="md-ellipsis">
      
        Policy Gradient
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Policy Gradient">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#policy-gradient-theorem" class="md-nav__link">
    <span class="md-ellipsis">
      
        Policy Gradient Theorem
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#proof-of-policy-gradient-theorem" class="md-nav__link">
    <span class="md-ellipsis">
      
        Proof of Policy Gradient Theorem
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#policy-gradient-in-continuous-action-space" class="md-nav__link">
    <span class="md-ellipsis">
      
        Policy Gradient in Continuous Action Space
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforce" class="md-nav__link">
    <span class="md-ellipsis">
      
        REINFORCE
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="REINFORCE">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      
        Algorithm
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bias-and-variance" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bias and Variance
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Bias and Variance">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#monte-carlo-estimators-in-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Monte Carlo Estimators in Reinforcement Learning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bias-in-policy-gradient-methods" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bias in Policy Gradient Methods
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Bias in Policy Gradient Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sources-of-bias" class="md-nav__link">
    <span class="md-ellipsis">
      
        Sources of Bias
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#biased-vs-unbiased-estimation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Biased vs. Unbiased Estimation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#variance-in-policy-gradient-methods" class="md-nav__link">
    <span class="md-ellipsis">
      
        Variance in Policy Gradient Methods
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Variance in Policy Gradient Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sources-of-variance" class="md-nav__link">
    <span class="md-ellipsis">
      
        Sources of Variance
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#techniques-to-reduce-variance-in-policy-gradient-methods" class="md-nav__link">
    <span class="md-ellipsis">
      
        Techniques to Reduce Variance in Policy Gradient Methods
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Techniques to Reduce Variance in Policy Gradient Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#baseline-subtraction" class="md-nav__link">
    <span class="md-ellipsis">
      
        Baseline Subtraction
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#causality-trick-and-reward-to-go-estimation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Causality Trick and Reward-to-Go Estimation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#discount-factor-adjustment" class="md-nav__link">
    <span class="md-ellipsis">
      
        Discount Factor Adjustment
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#advantage-estimation-and-actor-critic-methods" class="md-nav__link">
    <span class="md-ellipsis">
      
        Advantage Estimation and Actor-Critic Methods
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#actor-critic" class="md-nav__link">
    <span class="md-ellipsis">
      
        Actor-Critic
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary-of-variance-reduction-methods" class="md-nav__link">
    <span class="md-ellipsis">
      
        Summary of Variance Reduction Methods
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#concluding-remarks" class="md-nav__link">
    <span class="md-ellipsis">
      
        Concluding Remarks
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#authors" class="md-nav__link">
    <span class="md-ellipsis">
      
        Author(s)
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
            </div>
            </div>
        </div>
    

          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="week-3-policy-based-methods">Week 3: Policy-Based Methods</h1>
<p>Reinforcement Learning (RL) focuses on training an agent to interact
with an environment by learning a policy <span class="arithmatex">\(\pi_{\theta}(a | s)\)</span> that
maximizes the cumulative reward. Policy gradient methods are a class of
algorithms that directly optimize the policy by adjusting the parameters
<span class="arithmatex">\(\theta\)</span> via gradient ascent.</p>
<h2 id="why-policy-gradient-methods">Why Policy Gradient Methods?</h2>
<p>Unlike value-based methods (e.g., Q-learning), which rely on estimating
value functions, policy gradient methods:
- Can naturally handle stochastic policies, which are crucial in
environments requiring exploration.</p>
<ul>
<li>
<p>Work well in continuous action spaces, where discrete action methods
become infeasible.</p>
</li>
<li>
<p>Can directly optimize differentiable policy representations, such as
neural networks.</p>
</li>
<li>
<p>Avoid the need for an explicit action-value function approximation,
making them more robust in high-dimensional problems.</p>
</li>
<li>
<p>Are capable of optimizing <strong>parameterized policies</strong> without
relying on action selection heuristics.</p>
</li>
<li>
<p>Can incorporate entropy regularization to improve exploration and
prevent premature convergence to suboptimal policies.</p>
</li>
<li>
<p>Allow for more <strong>stable convergence</strong> in some cases compared to
value-based methods, which may suffer from instability due to
bootstrapping.</p>
</li>
<li>
<p>Can leverage <strong>variance reduction techniques</strong> (e.g., advantage
estimation, baseline subtraction) to improve learning efficiency.</p>
</li>
</ul>
<h2 id="policy-gradient">Policy Gradient</h2>
<p>The goal of reinforcement learning is to find an optimal behavior
strategy for the agent to obtain optimal rewards. The <strong>policy
gradient</strong> methods target at modeling and optimizing the policy
directly. The policy is usually modeled with a parameterized function
respect to <span class="arithmatex">\(\theta\)</span>, <span class="arithmatex">\(\pi_{\theta}(a|s)\)</span>. The value of the reward
(objective) function depends on this policy and then various algorithms
can be applied to optimize <span class="arithmatex">\(\theta\)</span> for the best reward.</p>
<p>The reward function is defined as:</p>
<div class="arithmatex">\[J(\theta) = \sum_{s \in  \mathcal{S}} d^{\pi}(s) V^{\pi}(s) = \sum_{s \in  \mathcal{S}} d^{\pi}(s) \sum_{a \in  \mathcal{A}} \pi_{\theta}(a|s) Q^{\pi}(s,a)\]</div>
<p>where <span class="arithmatex">\(d^{\pi}(s)\)</span> is the stationary distribution of Markov chain for
<span class="arithmatex">\(\pi_{\theta}\)</span> (on-policy state distribution under <span class="arithmatex">\(\pi\)</span>). For
simplicity, the parameter <span class="arithmatex">\(\theta\)</span> would be omitted for the policy
<span class="arithmatex">\(\pi_{\theta}\)</span> when the policy is present in the subscript of other
functions; for example, <span class="arithmatex">\(d^{\pi}\)</span> and <span class="arithmatex">\(Q^{\pi}\)</span> should be
<span class="arithmatex">\(d^{\pi_{\theta}}\)</span> and <span class="arithmatex">\(Q^{\pi_{\theta}}\)</span> if written in full.</p>
<p>Imagine that you can travel along the Markov chain's states forever, and
eventually, as the time progresses, the probability of you ending up
with one state becomes unchanged --- this is the stationary probability
for <span class="arithmatex">\(\pi_{\theta}\)</span>.
<span class="arithmatex">\(d^{\pi}(s) = \lim_{t \to  \infty} P(s_t = s | s_0, \pi_{\theta})\)</span> is the
probability that <span class="arithmatex">\(s_t = s\)</span> when starting from <span class="arithmatex">\(s_0\)</span> and following policy
<span class="arithmatex">\(\pi_{\theta}\)</span> for <span class="arithmatex">\(t\)</span> steps. Actually, the existence of the stationary
distribution of Markov chain is one main reason for why PageRank
algorithm works.</p>
<p>It is natural to expect policy-based methods are more useful in the
continuous space. Because there is an infinite number of actions and
(or) states to estimate the values for and hence value-based approaches
are way too expensive computationally in the continuous space. For
example, in <em>generalized policy iteration</em>, the policy improvement step
<span class="arithmatex">\(\arg  \max_{a \in  \mathcal{A}} Q^{\pi}(s,a)\)</span> requires a full scan of the
action space, suffering from the <em>curse of dimensionality</em>.</p>
<p>Using <em>gradient ascent</em>, we can move <span class="arithmatex">\(\theta\)</span> toward the direction
suggested by the gradient <span class="arithmatex">\(\nabla_{\theta} J(\theta)\)</span> to find the best
<span class="arithmatex">\(\theta\)</span> for <span class="arithmatex">\(\pi_{\theta}\)</span> that produces the highest return.</p>
<h3 id="policy-gradient-theorem">Policy Gradient Theorem</h3>
<p>Computing the gradient <span class="arithmatex">\(\nabla_{\theta}J(\theta)\)</span> is tricky because it
depends on both the action selection (directly determined by
<span class="arithmatex">\(\pi_{\theta}\)</span>) and the stationary distribution of states following the
target selection behavior (indirectly determined by <span class="arithmatex">\(\pi_{\theta}\)</span>).
Given that the environment is generally unknown, it is difficult to
estimate the effect on the state distribution by a policy update.</p>
<p>Luckily, the <strong>policy gradient theorem</strong> comes to save the world!
 It provides a nice reformation of the derivative of the
objective function to not involve the derivative of the state
distribution <span class="arithmatex">\(d^{\pi}(\cdot)\)</span> and simplify the gradient computation
<span class="arithmatex">\(\nabla_{\theta}J(\theta)\)</span> a lot.</p>
<div class="arithmatex">\[\nabla_{\theta}J(\theta) = \nabla_{\theta} \sum_{s \in  \mathcal{S}} d^{\pi}(s) \sum_{a \in  \mathcal{A}} Q^{\pi}(s,a) \pi_{\theta}(a|s)\]</div>
<div class="arithmatex">\[\propto  \sum_{s \in  \mathcal{S}} d^{\pi}(s) \sum_{a \in  \mathcal{A}} Q^{\pi}(s,a) \nabla_{\theta} \pi_{\theta}(a|s)\]</div>
<h3 id="proof-of-policy-gradient-theorem">Proof of Policy Gradient Theorem</h3>
<p>This session is pretty dense, as it is the time for us to go through the
proof and figure out why the policy gradient theorem is correct.</p>
<details class="warning" open="open">
<summary>Warning</summary>
<p>This proof may be unnecessary for the first phase of the course. </p>
</details>
<details class="note">
<summary>proof</summary>
<p>We first start with the derivative of the state value function:</p>
<div class="arithmatex">\[
\begin{aligned}
\nabla_{\theta} V^{\pi}(s) &amp;= \nabla_{\theta} \left( \sum_{a \in \mathcal{A}} \pi_{\theta}(a|s) Q^{\pi}(s,a) \right) \\
&amp;= \sum_{a \in \mathcal{A}} \left( \nabla_{\theta} \pi_{\theta}(a|s) Q^{\pi}(s,a) + \pi_{\theta}(a|s) \nabla_{\theta} Q^{\pi}(s,a) \right) \quad \text{; Derivative product rule.} \\
&amp;= \sum_{a \in \mathcal{A}} \left( \nabla_{\theta} \pi_{\theta}(a|s) Q^{\pi}(s,a) + \pi_{\theta}(a|s) \nabla_{\theta} \sum_{s', r} P(s',r|s,a) (r + V^{\pi}(s')) \right) \quad \text{; Extend } Q^{\pi} \text{ with future state value.} \\
&amp;= \sum_{a \in \mathcal{A}} \left( \nabla_{\theta} \pi_{\theta}(a|s) Q^{\pi}(s,a) + \pi_{\theta}(a|s) \sum_{s',r} P(s',r|s,a) \nabla_{\theta} V^{\pi}(s') \right) \\
&amp;= \sum_{a \in \mathcal{A}} \left( \nabla_{\theta} \pi_{\theta}(a|s) Q^{\pi}(s,a) + \pi_{\theta}(a|s) \sum_{s'} P(s'|s,a) \nabla_{\theta} V^{\pi}(s') \right) \quad \text{; Because } P(s'|s,a) = \sum_{r} P(s',r|s,a)
\end{aligned}
\]</div>
<p>Now we have:</p>
<div class="arithmatex">\[
\begin{aligned}
\nabla_{\theta} V^{\pi}(s) &amp;= \sum_{a \in \mathcal{A}} \left( \nabla_{\theta} \pi_{\theta}(a|s) Q^{\pi}(s,a) + \pi_{\theta}(a|s) \sum_{s'} P(s'|s,a) \nabla_{\theta} V^{\pi}(s') \right)
\end{aligned}
\]</div>
<p>This equation has a nice recursive form, and the future state value function <span class="arithmatex">\(V^{\pi}(s')\)</span> can be repeatedly unrolled by following the same equation.</p>
<p>Let's consider the following visitation sequence and label the probability of transitioning from state <span class="arithmatex">\(s\)</span> to state <span class="arithmatex">\(x\)</span> with policy <span class="arithmatex">\(\pi_{\theta}\)</span> after <span class="arithmatex">\(k\)</span> steps as <span class="arithmatex">\(\rho^{\pi}(s \to x, k)\)</span>.</p>
<div class="arithmatex">\[
s \xrightarrow{a \sim \pi_{\theta}(\cdot | s)} s' \xrightarrow{a' \sim \pi_{\theta}(\cdot | s')} s'' \xrightarrow{a'' \sim \pi_{\theta}(\cdot | s'')} \dots
\]</div>
<ul>
<li>
<p>When <span class="arithmatex">\(k = 0\)</span>: <span class="arithmatex">\(\rho^{\pi}(s \to s, k = 0) = 1\)</span>.</p>
</li>
<li>
<p>When <span class="arithmatex">\(k = 1\)</span>, we scan through all possible actions and sum up the transition probabilities to the target state:</p>
</li>
</ul>
<div class="arithmatex">\[
\rho^{\pi}(s \to s', k = 1) = \sum_{a} \pi_{\theta}(a|s) P(s'|s,a).
\]</div>
<ul>
<li>Imagine that the goal is to go from state <span class="arithmatex">\(s\)</span> to <span class="arithmatex">\(x\)</span> after <span class="arithmatex">\(k+1\)</span> steps while following policy <span class="arithmatex">\(\pi_{\theta}\)</span>. We can first travel from <span class="arithmatex">\(s\)</span> to a middle point <span class="arithmatex">\(s'\)</span> (any state can be a middle point, <span class="arithmatex">\(s' \in S\)</span>) after <span class="arithmatex">\(k\)</span> steps and then go to the final state <span class="arithmatex">\(x\)</span> during the last step. In this way, we are able to update the visitation probability recursively:</li>
</ul>
<div class="arithmatex">\[
\rho^{\pi}(s \to x, k + 1) = \sum_{s'} \rho^{\pi}(s \to s', k) \rho^{\pi}(s' \to x, 1).
\]</div>
<p>Then we go back to unroll the recursive representation of <span class="arithmatex">\(\nabla_{\theta}V^{\pi}(s)\)</span>. Let</p>
<div class="arithmatex">\[
\phi(s) = \sum_{a \in \mathcal{A}} \nabla_{\theta} \pi_{\theta}(a|s) Q^{\pi}(s,a)
\]</div>
<p>to simplify the equations. If we keep on extending <span class="arithmatex">\(\nabla_{\theta}V^{\pi}(\cdot)\)</span> infinitely, it is easy to find out that we can transition from the starting state <span class="arithmatex">\(s\)</span> to any state after any number of steps in this unrolling process and by summing up all the visitation probabilities, we get <span class="arithmatex">\(\nabla_{\theta}V^{\pi}(s)\)</span>!</p>
<div class="arithmatex">\[
\begin{aligned}
\nabla_{\theta}V^{\pi}(s) &amp;= \phi(s) + \sum_{a} \pi_{\theta}(a|s) \sum_{s'} P(s'|s,a) \nabla_{\theta}V^{\pi}(s') \\
&amp;= \phi(s) + \sum_{s'} \sum_{a} \pi_{\theta}(a|s) P(s'|s,a) \nabla_{\theta}V^{\pi}(s') \\
&amp;= \phi(s) + \sum_{s'} \rho^{\pi}(s \to s', 1) \nabla_{\theta}V^{\pi}(s') \\
&amp;= \phi(s) + \sum_{s'} \rho^{\pi}(s \to s', 1) \sum_{a \in \mathcal{A}} \left( \nabla_{\theta} \pi_{\theta}(a|s') Q^{\pi}(s',a) + \pi_{\theta}(a|s') \sum_{s''} P(s''|s',a) \nabla_{\theta}V^{\pi}(s'') \right) \\
&amp;= \phi(s) + \sum_{s'} \rho^{\pi}(s \to s', 1) \left[ \phi(s') + \sum_{s''} \rho^{\pi}(s' \to s'', 1) \nabla_{\theta}V^{\pi}(s'') \right] \\
&amp;= \phi(s) + \sum_{s'} \rho^{\pi}(s \to s', 1) \phi(s') + \sum_{s'} \rho^{\pi}(s \to s', 1) \sum_{s''} \rho^{\pi}(s' \to s'', 1) \nabla_{\theta}V^{\pi}(s'') \\
&amp;= \phi(s) + \sum_{s'} \rho^{\pi}(s \to s', 1) \phi(s') + \sum_{s''} \rho^{\pi}(s \to s'', 2) \nabla_{\theta}V^{\pi}(s'') \quad \text{; Consider } s' \text{ as the middle point for } s \to s''. \\
&amp;= \phi(s) + \sum_{s'} \rho^{\pi}(s \to s', 1) \phi(s') + \sum_{s''} \rho^{\pi}(s \to s'', 2) \phi(s'') + \sum_{s'''} \rho^{\pi}(s \to s''', 3) \nabla_{\theta}V^{\pi}(s''') \\
&amp;= \dots \quad \text{; Repeatedly unrolling the part of } \nabla_{\theta}V^{\pi}(\cdot) \\
&amp;= \sum_{x \in \mathcal{S}} \sum_{k=0}^{\infty} \rho^{\pi}(s \to x, k) \phi(x)
\end{aligned}
\]</div>
<p>The nice rewriting above allows us to exclude the derivative of Q-value function, <span class="arithmatex">\(\nabla_{\theta} Q^{\pi}(s,a)\)</span>. By plugging it into the objective function <span class="arithmatex">\(J(\theta)\)</span>, we are getting the following:</p>
<div class="arithmatex">\[
\begin{aligned}
\nabla_{\theta}J(\theta) &amp;= \nabla_{\theta}V^{\pi}(s_0) \\
&amp;= \sum_{s} \sum_{k=0}^{\infty} \rho^{\pi}(s_0 \to s, k) \phi(s) \quad \text{; Starting from a random state } s_0 \\
&amp;= \sum_{s} \eta(s) \phi(s) \quad \text{; Let } \eta(s) = \sum_{k=0}^{\infty} \rho^{\pi}(s_0 \to s, k) \\
&amp;= \left( \sum_{s} \eta(s) \right) \sum_{s} \frac{\eta(s)}{\sum_{s} \eta(s)} \phi(s) \quad \text{; Normalize } \eta(s), s \in \mathcal{S} \text{ to be a probability distribution.} \\
&amp;\propto \sum_{s} \frac{\eta(s)}{\sum_{s} \eta(s)} \phi(s) \quad \text{; } \sum_{s} \eta(s) \text{ is a constant} \\
&amp;= \sum_{s} d^{\pi}(s) \sum_{a} \nabla_{\theta} \pi_{\theta}(a|s) Q^{\pi}(s,a) \quad d^{\pi}(s) = \frac{\eta(s)}{\sum_{s} \eta(s)} \text{ is stationary distribution.}
\end{aligned}
\]</div>
<p>In the episodic case, the constant of proportionality (<span class="arithmatex">\(\sum_{s} \eta(s)\)</span>) is the average length of an episode; in the continuing case, it is 1. The gradient can be further written as:</p>
<div class="arithmatex">\[
\begin{aligned}
\nabla_{\theta}J(\theta) &amp;\propto \sum_{s \in \mathcal{S}} d^{\pi}(s) \sum_{a \in \mathcal{A}} Q^{\pi}(s,a) \nabla_{\theta} \pi_{\theta}(a|s) \\
&amp;= \sum_{s \in \mathcal{S}} d^{\pi}(s) \sum_{a \in \mathcal{A}} \pi_{\theta}(a|s) Q^{\pi}(s,a) \frac{\nabla_{\theta} \pi_{\theta}(a|s)}{\pi_{\theta}(a|s)} \quad \text{; Because } \ln(x)'=1/x \\
&amp;= \mathbb{E}_{\pi} [Q^{\pi}(s,a) \nabla_{\theta} \ln \pi_{\theta}(a|s)]
\end{aligned}
\]</div>
<p>Where <span class="arithmatex">\(\mathbb{E}_{\pi}\)</span> refers to <span class="arithmatex">\(\mathbb{E}_{s \sim d^{\pi}, a \sim \pi_{\theta}}\)</span> when both state and action distributions follow the policy <span class="arithmatex">\(\pi_{\theta}\)</span> (on policy).</p>
<p>The policy gradient theorem lays the theoretical foundation for various policy gradient algorithms. This vanilla policy gradient update has no bias but high variance. Many following algorithms were proposed to reduce the variance while keeping the bias unchanged.</p>
<div class="arithmatex">\[
\nabla_{\theta}J(\theta) = \mathbb{E}_{\pi} [Q^{\pi}(s,a) \nabla_{\theta} \ln \pi_{\theta}(a|s)]
\]</div>
</details>
<h3 id="policy-gradient-in-continuous-action-space">Policy Gradient in Continuous Action Space</h3>
<p>In a continuous action space, the policy gradient theorem is given by:</p>
<div class="arithmatex">\[\nabla_{\theta}J(\theta) = \mathbb{E}_{s \sim d^{\pi}, a \sim  \pi_{\theta}} \left[ Q^{\pi}(s,a) \nabla_{\theta} \ln  \pi_{\theta}(a|s) \right]\]</div>
<p>Since the action space is continuous, the summation over actions in the
discrete case is replaced by an integral:</p>
<div class="arithmatex">\[\nabla_{\theta} J(\theta) = \int_{\mathcal{S}} d^{\pi}(s) \int_{\mathcal{A}} Q^{\pi}(s,a) \nabla_{\theta} \ln  \pi_{\theta}(a|s) \pi_{\theta}(a|s) \, da \, ds\]</div>
<p>where:</p>
<ul>
<li>
<p><span class="arithmatex">\(d^{\pi}(s)\)</span> is the stationary state distribution under policy
<span class="arithmatex">\(\pi_{\theta}\)</span>,</p>
</li>
<li>
<p><span class="arithmatex">\(\pi_{\theta}(a|s)\)</span> is the probability density function for the
continuous action <span class="arithmatex">\(a\)</span> given state <span class="arithmatex">\(s\)</span>,</p>
</li>
<li>
<p><span class="arithmatex">\(Q^{\pi}(s,a)\)</span> is the state-action value function,</p>
</li>
<li>
<p><span class="arithmatex">\(\nabla_{\theta} \ln  \pi_{\theta}(a|s)\)</span> is the score function
(policy gradient term),</p>
</li>
<li>
<p>The integral is taken over all possible states <span class="arithmatex">\(s\)</span> and actions <span class="arithmatex">\(a\)</span>.</p>
</li>
</ul>
<details class="example" open="open">
<summary>Gaussian Policy Example</summary>
<p>A common choice for a continuous policy is a Gaussian distribution:</p>
<div class="arithmatex">\[a \sim  \pi_{\theta}(a|s) = \mathcal{N}(\mu_{\theta}(s), \Sigma_{\theta}(s))\]</div>
<p>where:</p>
<ul>
<li>
<p><span class="arithmatex">\(\mu_{\theta}(s)\)</span> is the mean of the action distribution,
parameterized by <span class="arithmatex">\(\theta\)</span>,</p>
</li>
<li>
<p><span class="arithmatex">\(\Sigma_{\theta}(s)\)</span> is the covariance matrix (often assumed
diagonal or fixed).</p>
</li>
</ul>
<p>For a Gaussian policy, the logarithm of the probability density is:</p>
<div class="arithmatex">\[\ln  \pi_{\theta}(a|s) = -\frac{1}{2} (a - \mu_{\theta}(s))^T \Sigma_{\theta}^{-1} (a - \mu_{\theta}(s)) - \frac{1}{2} \ln |\Sigma_{\theta}|\]</div>
<p>Taking the gradient:</p>
<div class="arithmatex">\[\nabla_{\theta} \ln  \pi_{\theta}(a|s) = \Sigma_{\theta}^{-1} (a - \mu_{\theta}(s)) \nabla_{\theta} \mu_{\theta}(s)\]</div>
<p>Thus, the policy gradient update becomes:</p>
<div class="arithmatex">\[\nabla_{\theta} J(\theta) = \mathbb{E}_{s \sim d^{\pi}, a \sim  \pi_{\theta}} \left[ Q^{\pi}(s,a) \Sigma_{\theta}^{-1} (a - \mu_{\theta}(s)) \nabla_{\theta} \mu_{\theta}(s) \right]\]</div>
</details>
<h3 id="reinforce">REINFORCE</h3>
<p>REINFORCE (Monte-Carlo policy gradient) relies on an estimated return by
<strong>Monte-Carlo</strong> methods using episode samples to update the policy
parameter <span class="arithmatex">\(\theta\)</span>. REINFORCE works because the expectation of the
sample gradient is equal to the actual gradient:</p>
<div class="arithmatex">\[
\begin{aligned}
\nabla_{\theta}J(\theta) &amp;= \mathbb{E}_{\pi} \left[ Q^{\pi}(s,a) \nabla_{\theta} \ln \pi_{\theta}(a|s) \right] \\
&amp;= \mathbb{E}_{\pi} \left[ G_t \nabla_{\theta} \ln \pi_{\theta}(A_t|S_t) \right] \quad \text{; Because } Q^{\pi}(S_t, A_t) = \mathbb{E}_{\pi} \left[ G_t \mid S_t, A_t \right]
\end{aligned}
\]</div>
<p>where <span class="arithmatex">\(G_t = \sum_{k=0}^{\infty} \gamma^kR_{t+k+1}\)</span> is the discounted future reward starting from time setp <span class="arithmatex">\(t\)</span>.</p>
<p>Therefore we are able to measure <span class="arithmatex">\(G_t\)</span> from real sample trajectories and
use that to update our policy gradient. It relies on a full trajectory
and that's why it is a Monte-Carlo method.</p>
<h4 id="algorithm">Algorithm</h4>
<p>The process is pretty straightforward:</p>
<ol>
<li>
<p>Initialize the policy parameter <span class="arithmatex">\(\theta\)</span> at random.</p>
</li>
<li>
<p>Generate one trajectory on policy <span class="arithmatex">\(\pi_{\theta}\)</span>: <span class="arithmatex">\(S_1, A_1, R_2, S_2, A_2, \dots, S_T\)</span>.</p>
</li>
<li>
<p>For <span class="arithmatex">\(t = 1, 2, \dots, T\)</span>:</p>
<ol>
<li>
<p>Estimate the return <span class="arithmatex">\(G_t\)</span>;</p>
</li>
<li>
<p>Update policy parameters: <span class="arithmatex">\(\theta  \leftarrow  \theta + \alpha  \gamma^t G_t \nabla_{\theta} \ln  \pi_{\theta}(A_t|S_t)\)</span></p>
</li>
</ol>
</li>
</ol>
<p>A widely used variation of REINFORCE is to subtract a baseline value
from the return <span class="arithmatex">\(G_t\)</span> to <strong>reduce the variance of gradient estimation
while keeping the bias unchanged</strong> (Remember we always want to do this
when possible).</p>
<p>For example, a common baseline is to subtract state-value from
action-value, and if applied, we would use <strong>advantage</strong> <span class="arithmatex">\(A(s,a) = Q(s,a) - V(s)\)</span> in the gradient ascent update. This <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/">post</a> nicely explained why a baseline works for reducing the variance, in addition to a set of fundamentals of policy gradient.</p>
<!--   

####  $G(s)$ in Continuous Action Space 



In the continuous setting, we define the **return** $G(s)$ as:

$$G(s) = \sum_{k=0}^{\infty} \gamma^k R(s_k, a_k), \quad s_0 = s, \quad a_k \sim  \pi_{\theta}(\cdot | s_k)$$


where:


- $R(s_k, a_k)$ is the reward function for state-action pair
$(s_k, a_k)$.



- $\gamma$ is the **discount factor**.



- $s_k$ evolves according to the environment dynamics.



- $a_k \sim  \pi_{\theta}(\cdot | s_k)$ means actions are sampled from
the policy.



#### Monte Carlo Approximation of $Q^{\pi}(s,a)$



In expectation, $G(s)$ serves as an **unbiased estimator** of the
state-action value function:



$$Q^{\pi}(s,a) = \mathbb{E} \left[ G(s) \middle| s_0 = s, a_0 = a \right]$$



Using this, we rewrite the policy gradient update as:



$$\nabla_{\theta} J(\theta) = \mathbb{E}_{s \sim d^{\pi}, a \sim  \pi_{\theta}} \left[ G(s) \nabla_{\theta} \ln  \pi_{\theta}(a | s) \right]$$



#### Variance Reduction: Advantage Function



A **baseline** is often subtracted to reduce variance while keeping
the expectation unchanged:



$$A(s, a) = G(s) - V^{\pi}(s)$$



$$\nabla_{\theta} J(\theta) = \mathbb{E}_{s \sim d^{\pi}, a \sim  \pi_{\theta}} \left[ A(s, a) \nabla_{\theta} \ln  \pi_{\theta}(a | s) \right]$$



where:



- $V^{\pi}(s) = \mathbb{E}_{a \sim  \pi_{\theta}(\cdot | s)} [Q^{\pi}(s,a)]$

is the **state value function**.



- $A(s,a)$ measures the **advantage** of taking action $a$ over
the expected policy action.

   -->

<h2 id="bias-and-variance">Bias and Variance</h2>
<p>As introduced in previous sections, REINFORCE employs Monte Carlo estimation of returns. Recall that Monte Carlo methods estimate expected values by sampling trajectories from the environment. While these estimators are unbiased (they converge to the true expected value given enough samples), they often suffer from high variance, making policy gradient methods challenging to stabilize.</p>
<p>In this section, we delve deeper into the bias-variance tradeoff in reinforcement learning, with a focus on policy gradient methods. While these concepts were mentioned earlier, we now analyze them as the central topic:</p>
<p><strong>Bias</strong> occurs when our gradient estimates systematically deviate from the true expected gradient, leading to suboptimal policy updates.</p>
<p><strong>Variance</strong> measures how much our gradient estimates fluctuate across different batches of samples, affecting training stability.</p>
<p>Policy gradient methods face unique challenges:</p>
<ul>
<li>
<p>High variance from Monte Carlo sampling of full trajectories.</p>
</li>
<li>
<p>Potential bias from function approximation (e.g., neural networks) or improper baselines.</p>
</li>
</ul>
<p>Below, we formalize these concepts and explore techniques to mitigate their effects.</p>
<h3 id="monte-carlo-estimators-in-reinforcement-learning">Monte Carlo Estimators in Reinforcement Learning</h3>
<p>A Monte Carlo estimator is a method used to approximate the expected
value of a function <span class="arithmatex">\(f(X)\)</span> over a random variable <span class="arithmatex">\(X\)</span> with a given
probability distribution <span class="arithmatex">\(p(X)\)</span>. The true expectation is:</p>
<div class="arithmatex">\[E[f(X)] = \int f(x) p(x) \, dx\]</div>
<p>However, directly computing this integral may be complex. Instead, we
use Monte Carlo estimation by drawing <span class="arithmatex">\(N\)</span> independent samples
<span class="arithmatex">\(X_1, X_2, \dots, X_N\)</span> from <span class="arithmatex">\(p(X)\)</span> and computing:</p>
<div class="arithmatex">\[\hat{\mu}_{MC} = \frac{1}{N} \sum_{i=1}^{N} f(X_i)\]</div>
<p>This estimator provides an approximation to the true expectation
<span class="arithmatex">\(E[f(X)]\)</span>.</p>
<p>By the law of large numbers (LLN), as <span class="arithmatex">\(N \to  \infty\)</span>, we have:</p>
<div class="arithmatex">\[\hat{X}_N \to  \mathbb{E}[X] \quad  \text{(almost surely)}\]</div>
<p>Monte Carlo methods are commonly used in RL for estimating expected
rewards, state-value functions, and action-value functions.</p>
<h3 id="bias-in-policy-gradient-methods">Bias in Policy Gradient Methods</h3>
<p>Bias in reinforcement learning arises when an estimator systematically
deviates from the true value. In policy gradient methods, bias is
introduced due to function approximation, reward estimation, or gradient
computation errors.</p>
<h4 id="sources-of-bias">Sources of Bias</h4>
<ul>
<li>
<p><strong>Function Approximation Bias:</strong> Policy gradient methods often rely
on neural networks or other function approximators for policy
representation. Imperfect approximations introduce systematic
errors, leading to biased policy updates.</p>
</li>
<li>
<p><strong>Reward Clipping or Discounting:</strong> Algorithms using reward clipping
or high discount factors (<span class="arithmatex">\(\gamma\)</span>) can distort return estimates,
causing the learned policy to be biased toward short-term rewards.</p>
</li>
<li>
<p><strong>Baseline Approximation:</strong> Variance reduction techniques like
baseline subtraction use estimates of expected returns. If the
baseline is inaccurately estimated, it introduces bias in the policy
gradient computation.</p>
</li>
</ul>
<details class="example" open="open">
<summary>Example of Bias</summary>
<p>Consider a self-driving car optimizing for fuel efficiency. If the
reward function prioritizes immediate fuel consumption over long-term
efficiency, the learned policy may favor suboptimal strategies that
minimize fuel use in the short term while missing globally optimal
driving behaviors.</p>
</details>
<h4 id="biased-vs-unbiased-estimation">Biased vs. Unbiased Estimation</h4>
<p>For example: The biased formula for the sample variance <span class="arithmatex">\(S^2\)</span> is given
by:</p>
<div class="arithmatex">\[S^2_{\text{biased}} = \frac{1}{n} \sum_{i=1}^{n} (X_i - \overline{X})^2\]</div>
<p>This is an underestimation of the true population variance <span class="arithmatex">\(\sigma^2\)</span>
because it does not account for the degrees of freedom in estimation.</p>
<p>Instead, the unbiased estimator is:</p>
<div class="arithmatex">\[S^2_{\text{unbiased}} = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \overline{X})^2.\]</div>
<p>This unbiased estimator correctly accounts for variance in small sample
sizes, ensuring <span class="arithmatex">\(\mathbb{E}[S^2_{\text{unbiased}}] = \sigma^2\)</span>.</p>
<h3 id="variance-in-policy-gradient-methods">Variance in Policy Gradient Methods</h3>
<p>Variance in policy gradient estimates refers to fluctuations in gradient
estimates across different training episodes. High variance leads to
instability and slow convergence.</p>
<h4 id="sources-of-variance">Sources of Variance</h4>
<ul>
<li>
<p><strong>Monte Carlo Estimation:</strong> REINFORCE estimates gradients using
complete episodes, leading to high variance due to trajectory
randomness.</p>
</li>
<li>
<p><strong>Stochastic Policy Outputs:</strong> Policies represented as probability
distributions (e.g., Gaussian policies) introduce additional
randomness in gradient updates.</p>
</li>
<li>
<p><strong>Exploration Strategies:</strong> Methods like softmax or epsilon-greedy
increase variance by adding stochasticity to action selection.</p>
</li>
</ul>
<details class="example" open="open">
<summary>Example of Variance</summary>
<p>Consider a robotic arm learning to grasp objects. Due to high variance,
in some episodes, it succeeds, while in others, minor variations cause
failure. These inconsistencies slow down convergence.</p>
</details>
<h3 id="techniques-to-reduce-variance-in-policy-gradient-methods">Techniques to Reduce Variance in Policy Gradient Methods</h3>
<p>Several strategies help mitigate variance in policy gradient methods
while preserving unbiased gradient estimates.</p>
<h4 id="baseline-subtraction">Baseline Subtraction</h4>
<p>A baseline function <span class="arithmatex">\(b\)</span> reduces variance without introducing bias:</p>
<div class="arithmatex">\[\nabla_{\theta} J(\theta) = \mathbb{E}_{\pi_{\theta}} \left[ \nabla_{\theta} \log  \pi_{\theta}(a_t | s_t) (G_t - b) \right].\]</div>
<p>A common choice for <span class="arithmatex">\(b\)</span> is the average return over trajectories:</p>
<div class="arithmatex">\[b = \frac{1}{N} \sum_{i=1}^{N} G_i.\]</div>
<p>Since <span class="arithmatex">\(b\)</span> is independent of actions, it does not introduce bias in the
gradient estimate while reducing variance. A simple proof for this is illustrated bleow.</p>
<details class="note">
<summary>proof</summary>
<div class="arithmatex">\[\begin{aligned}
E\left[\nabla_\theta  \log p_\theta(\tau) b\right] &amp;= \int p_\theta(\tau) \nabla_\theta  \log p_\theta(\tau) b \, d\tau \\
&amp;= \int  \nabla_\theta p_\theta(\tau) b \, d\tau \\
&amp;= b \nabla_\theta  \int p_\theta(\tau) \, d\tau \\
&amp;= b \nabla_\theta  1 \\
&amp;= 0
\end{aligned}\]</div>
</details>
<!-- ![image](\assets\images\course_notes\policy-based\a4.png){width="0.8\\linewidth"} -->

<center> 
<img src="\assets\images\course_notes\policy-based\a4.png"
    style="float: center; margin-right: 10px;" 
    /> 
    </center>

<h4 id="causality-trick-and-reward-to-go-estimation">Causality Trick and Reward-to-Go Estimation</h4>
<p>To ensure that policy updates at time <span class="arithmatex">\(t\)</span> are only influenced by rewards
from that time step onward, we use the causality trick:</p>
<div class="arithmatex">\[\nabla_{\theta} J(\theta) \approx  \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} \log  \pi_{\theta}(a_{i,t} | s_{i,t}) \left( \sum_{t'=t}^{T} r(a_{i,t'}, s_{i,t'}) \right).\]</div>
<p>Instead of summing over all rewards, the reward-to-go estimate restricts
the sum to future rewards only:</p>
<div class="arithmatex">\[Q(s_t, a_t) = \sum_{t'=t}^{T} \mathbb{E}_{\pi_{\theta}} [r(s_{t'}, a_{t'}) | s_t, a_t].\]</div>
<div class="arithmatex">\[\nabla_{\theta} J(\theta) \approx  \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} \log  \pi_{\theta}(a_{i,t} | s_{i,t}) Q(s_{i,t}, a_{i,t}).\]</div>
<p>This prevents rewards from future time steps from affecting past
actions, reducing variance. This approach results in much lower variance
compared to the traditional Monte Carlo methods.</p>
<!-- ![image](\assets\images\course_notes\policy-based\a1.png){width="0.4\\linewidth"} ![image](\assets\images\course_notes\policy-based\a2.png){width="0.4\\linewidth"} -->

<center> 
<img src="\assets\images\course_notes\policy-based\a1.png"
    style="float: center; margin-right: 10px;" 
    /> 
    </center>

<center> 
<img src="\assets\images\course_notes\policy-based\a2.png"
    style="float: center; margin-right: 10px;" 
    /> 
    </center>

<details class="note">
<summary>proof</summary>
<div class="arithmatex">\[
\begin{aligned}
A_{t_0-1} &amp;= s_{t_0-1}, a_{t_0-1}, \dots, a_0, s_0 \\
\mathbb{E}_{A_{t_0-1}} &amp;\left[ \mathbb{E}_{s_{t_0}, a_{t_0} | A_{t_0-1}} \left[ \nabla_{\theta} \log \pi_{\theta} (a_{t_0} | s_{t_0}) \sum_{t=0}^{t_0 - 1} r(s_t, a_t) \right] \right] \\
U_{t_0-1} &amp;= \sum_{t=0}^{t_0 - 1} r(s_t, a_t) \\
&amp;= \mathbb{E}_{A_{t_0-1}} \left[ U_{t_0-1} \mathbb{E}_{s_{t_0}, a_{t_0} | s_{t_0-1}, a_{t_0-1}} \nabla_{\theta} \log \pi_{\theta} (a_{t_0} | s_{t_0}) \right] \\
&amp;= \mathbb{E}_{A_{t_0-1}} \left[ U_{t_0-1} \mathbb{E}_{s_{t_0} | s_{t_0-1}, a_{t_0-1}} \mathbb{E}_{a_{t_0} | s_{t_0-1}, a_{t_0-1}, s_{t_0}} \nabla_{\theta} \log \pi_{\theta} (a_{t_0} | s_{t_0}) \right] \\
&amp;= \mathbb{E}_{A_{t_0-1}} \left[ U_{t_0-1} \mathbb{E}_{s_{t_0} | s_{t_0-1}, a_{t_0-1}} \mathbb{E}_{a_{t_0} | s_{t_0}} \nabla_{\theta} \log \pi_{\theta} (a_{t_0} | s_{t_0}) \right] \\
&amp;= \mathbb{E}_{A_{t_0-1}} \left[ U_{t_0-1} \mathbb{E}_{s_{t_0} | s_{t_0-1}, a_{t_0-1}} \mathbb{E}_{\pi_{\theta} (a_{t_0} | s_{t_0})} \nabla_{\theta} \log \pi_{\theta} (a_{t_0} | s_{t_0}) \right] \\
\mathbb{E}_{\pi_{\theta} (a_{t_0} | s_{t_0})} &amp;\nabla_{\theta} \log \pi_{\theta} (a_{t_0} | s_{t_0}) = 0 \\
\mathbb{E}_{A_{t_0-1}}&amp; \left[ \mathbb{E}_{s_{t_0}, a_{t_0} | A_{t_0-1}} \left[ \nabla_{\theta} \log \pi_{\theta} (a_{t_0} | s_{t_0}) \sum_{t=0}^{t_0 - 1} r(s_t, a_t) \right] \right] = 0
\end{aligned}
\]</div>
</details>
<h4 id="discount-factor-adjustment">Discount Factor Adjustment</h4>
<p>The discount factor <span class="arithmatex">\(\gamma\)</span> helps reduce variance by weighting rewards
closer to the present more heavily:</p>
<div class="arithmatex">\[G_t = \sum_{t' = t}^{T} \gamma^{t'-t} r(s_{t'}, a_{t'}).\]</div>
<details class="note">
<summary>proof</summary>
<div class="arithmatex">\[
\begin{aligned}
\nabla_{\theta} J(\theta) &amp;\approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta} (a_{i,t} | s_{i,t}) \left( \sum_{t' = t}^{T} \gamma^{t' - t} r(s_{i,t'}, a_{i,t'}) \right) \\
\nabla_{\theta} J(\theta) &amp;\approx \frac{1}{N} \sum_{i=1}^{N} \left( \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta} (a_{i,t} | s_{i,t}) \right) \left( \sum_{t=1}^{T} \gamma^{t-1} r(s_{i,t}, a_{i,t}) \right) \\
\nabla_{\theta} J(\theta) &amp;\approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta} (a_{i,t} | s_{i,t}) \left( \sum_{t' = t}^{T} \gamma^{t' - t} r(s_{i,t'}, a_{i,t'}) \right) \\
\nabla_{\theta} J(\theta) &amp;\approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \gamma^{t-1} \nabla_{\theta} \log \pi_{\theta} (a_{i,t} | s_{i,t}) \left( \sum_{t' = t}^{T} \gamma^{t' - t} r(s_{i,t'}, a_{i,t'}) \right)
\end{aligned}
\]</div>
</details>
<p>A lower <span class="arithmatex">\(\gamma\)</span> (e.g., 0.9) reduces variance but increases bias, while
a higher <span class="arithmatex">\(\gamma\)</span> (e.g., 0.99) improves long-term estimation but
increases variance. A balance is needed.</p>
<h4 id="advantage-estimation-and-actor-critic-methods">Advantage Estimation and Actor-Critic Methods</h4>
<p>Actor-critic methods combine policy optimization (actor) with value
function estimation (critic). The advantage function is defined as:</p>
<div class="arithmatex">\[A^{\pi}(s_t, a_t) = Q^{\pi}(s_t, a_t) - V^{\pi}(s_t),\]</div>
<p>where the action-value function is:</p>
<div class="arithmatex">\[Q^{\pi}(s_t, a_t) = \sum_{t' = t}^{T} \mathbb{E}_{\pi} [r(s_{t'}, a_{t'}) | s_t, a_t],\]</div>
<p>and the state-value function is:</p>
<div class="arithmatex">\[V^{\pi}(s_t) = \mathbb{E}_{a_t \sim  \pi_{\theta}(a_t | s_t)} [Q^{\pi}(s_t, a_t)].\]</div>
<p>The policy gradient update using the advantage function becomes:</p>
<div class="arithmatex">\[\nabla_{\theta} J(\theta) \approx  \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} \log  \pi_{\theta}(a_{i,t} | s_{i,t}) A^{\pi}(s_{i,t}, a_{i,t}).\]</div>
<p>This formulation allows for lower variance in policy updates while
leveraging learned state-value estimates. Actor-critic methods are
widely used in modern reinforcement learning due to their stability and
efficiency.</p>
<center> 
<img src="\assets\images\course_notes\policy-based\a3.png"
    style="float: center; margin-right: 10px;" 
    /> 
    </center>

<h4 id="actor-critic">Actor-Critic</h4>
<p>Two main components in policy gradient methods are the policy model and
the value function. It makes a lot of sense to learn the value function
in addition to the policy since knowing the value function can assist
the policy update, such as by reducing gradient variance in vanilla
policy gradients. That is exactly what the Actor-Critic method does.</p>
<p>Actor-Critic methods consist of two models, which may optionally share
parameters:</p>
<ul>
<li>
<p><strong>Critic</strong>: Updates the value function parameters <span class="arithmatex">\(w\)</span>. Depending on
the algorithm, it could be an action-value function <span class="arithmatex">\(Q(s, a)\)</span> or a
state-value function <span class="arithmatex">\(V(s)\)</span>.</p>
</li>
<li>
<p><strong>Actor</strong>: Updates the policy parameters <span class="arithmatex">\(\theta\)</span> for <span class="arithmatex">\(\pi_{\theta}(a | s)\)</span>, in the direction suggested by the critic.</p>
</li>
</ul>
<p>Let's see how it works in a simple action-value Actor-Critic algorithm:</p>
<ol>
<li>
<p>Initialize policy parameters <span class="arithmatex">\(\theta\)</span> and value function parameters
<span class="arithmatex">\(w\)</span> at random.</p>
</li>
<li>
<p>Sample initial state <span class="arithmatex">\(s_0\)</span>.</p>
</li>
<li>
<p>For each time step <span class="arithmatex">\(t\)</span>:</p>
<ol>
<li>
<p>Sample reward <span class="arithmatex">\(r_t\)</span> and next state <span class="arithmatex">\(s_{t+1}\)</span>.</p>
</li>
<li>
<p>Then sample the next action <span class="arithmatex">\(a_{t+1}\)</span> from policy: <span class="arithmatex">\(\pi_{\theta}(s_{t+1})\)</span></p>
</li>
<li>
<p>Update the policy parameters: <span class="arithmatex">\(\theta  \leftarrow  \theta + \alpha  \nabla_{\theta} \log  \pi_{\theta}(a_t | s_t) Q(s_t, a_t)\)</span></p>
</li>
</ol>
</li>
<li>
<p>Compute the correction (TD error) for action-value at time <span class="arithmatex">\(t\)</span>: <span class="arithmatex">\(\delta_t = r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q_w(s_t, a_t)\)</span></p>
</li>
<li>
<p>Compute MSE loss : <span class="arithmatex">\(\mathcal{L}(w) = \frac{1}{2} \mathbb{E}_{(s_t, a_t, r_t, s_{t+1}) \sim \pi_\theta} \left[ \delta_t^2 \right]\)</span></p>
</li>
<li>
<p>Use it to update the parameters of the action-value function : <span class="arithmatex">\(w \leftarrow w + \beta  \delta_t  \nabla_w Q_w(s_t, a_t)\)</span></p>
</li>
<li>
<p>Update <span class="arithmatex">\(\theta\)</span> and <span class="arithmatex">\(w\)</span>.</p>
</li>
</ol>
<p>Two learning rates, <span class="arithmatex">\(\alpha\)</span> and <span class="arithmatex">\(\beta\)</span>, are predefined for policy and
value function parameter updates, respectively. Also note that the <span class="arithmatex">\(Q(s_{t+1}, a_{t+1})\)</span> in the TD error uses the freezed values of <span class="arithmatex">\(w\)</span> for better stablity.</p>
<center> 
<img src="\assets\images\course_notes\policy-based\a6.png"
    style="float: center; margin-right: 10px;" 
    /> 
    </center>

<details class="example" open="open">
<summary>Actor-Critic Architecture: Cartpole Example</summary>
<p>Let's illustrate the Actor-Critic architecture with an example of a
classic reinforcement learning problem: the <em>Cartpole</em> environment.</p>
<p><center> 
<img src="\assets\images\course_notes\policy-based\cartpole.png"
    alt="pi estimation with monte carlo"
    style="float: center; margin-right: 10px;" 
    /> 
    </center></p>
<hr />
<p>In the <em>Cartpole</em> environment, the agent controls a cart that can move
horizontally on a track. A pole is attached to the cart, and the agent's
task is to balance the pole upright for as long as possible.</p>
<ol>
<li>
<p><strong>Actor (Policy-Based)</strong>: The actor is responsible for learning the
policy, which is the agent's strategy for selecting actions (left or
right) based on the observed state (cart position, cart velocity,
pole angle, and pole angular velocity).</p>
</li>
<li>
<p><strong>Critic (Value-Based)</strong>: The critic is responsible for learning the
value function, which estimates the expected total reward (return)
from each state. The value function helps evaluate how good or bad a
specific state is, which guides the actor's updates.</p>
</li>
<li>
<p><strong>Policy Representation</strong>: For simplicity, let's use a neural
network as the actor. The neural network takes the current state of
the cart and pole as input and outputs the probabilities of
selecting actions (left or right).</p>
</li>
<li>
<p><strong>Value Function Representation</strong>: For the critic, we also use a
neural network. The neural network takes the current state as input
and outputs an estimate of the expected total reward (value) for
that state.</p>
</li>
<li>
<p><strong>Collecting Experiences</strong>: The agent interacts with the
environment, using the current policy to select actions (left or
right). As it moves through the environment, it collects
experiences, including states, actions, rewards, and next states.</p>
</li>
<li>
<p><strong>Updating the Critic (Value Function)</strong>: The critic learns to
estimate the value function using the collected experiences. It
optimizes its neural network parameters to minimize the difference
between the predicted values and the actual rewards experienced by
the agent.</p>
</li>
<li>
<p><strong>Calculating the Advantage</strong>: The advantage represents how much
better or worse an action is compared to the average expected value.
It is calculated as the difference between the total return (reward)
and the value function estimate for each state-action pair.</p>
</li>
<li>
<p><strong>Updating the Actor (Policy)</strong>: The actor updates its policy to
increase the probabilities of actions with higher advantages and
decrease the probabilities of actions with lower advantages. This
process helps the actor learn from the critic's feedback and improve
its policy to maximize the expected rewards.</p>
</li>
<li>
<p><strong>Iteration and Learning</strong>: The learning process is repeated over
multiple episodes and iterations. As the agent explores and
interacts with the environment, the actor and critic networks
gradually improve their performance and converge to better policies
and value function estimates.</p>
</li>
</ol>
<p>Through these steps, the Actor-Critic architecture teaches the agent how
to balance the pole effectively in the <em>Cartpole</em> environment. The actor
learns the best actions to take in different states, while the critic
provides feedback on the quality of the actor's decisions. As a result,
the agent converges to a more optimal policy, achieving longer balancing
times and better performance in the task.</p>
</details>
<h3 id="summary-of-variance-reduction-methods">Summary of Variance Reduction Methods</h3>
<p>To summarize, the key methods for reducing variance in policy gradient methods include:</p>
<ul>
<li>
<p><strong>Baseline Subtraction:</strong> Subtracting an average return baseline to
reduce variance while keeping gradients unbiased.</p>
</li>
<li>
<p><strong>Causality Trick and Reward-to-Go:</strong> Using future rewards from time
step <span class="arithmatex">\(t\)</span> onward to prevent variance from irrelevant past rewards.</p>
</li>
<li>
<p><strong>Discount Factor Adjustment:</strong> Adjusting <span class="arithmatex">\(\gamma\)</span> to balance
variance reduction and long-term reward optimization.</p>
</li>
<li>
<p><strong>Advantage Estimation:</strong> Using the advantage function <span class="arithmatex">\(A(s_t, a_t)\)</span>
instead of raw returns to stabilize learning.</p>
</li>
<li>
<p><strong>Actor-Critic Methods:</strong> Combining policy gradient updates with
value function estimation to create more stable and efficient
training.</p>
</li>
</ul>
<p>By employing these techniques, policy gradient methods can achieve more
stable and efficient learning with reduced variance.</p>
<h2 id="concluding-remarks">Concluding Remarks</h2>
<p>Now that we have seen the principles behind a policy-based algorithm,
let us see how policy-based algorithms work in practice, and compare
advantages and disadvantages of the policy-based approach.</p>
<p>Let us start with the advantages. First of all, parameterization is at
the core of policy-based methods, making them a good match for deep
learning. For value- based methods, deep learning had to be retrofitted,
giving rise to complications. Second, policy-based methods can easily
find stochastic policies, whereas value- based methods find
deterministic policies. Due to their stochastic nature, policy- based
methods naturally explore, without the need for methods such as
<span class="arithmatex">\(\epsilon\)</span>-greedy, or more involved methods that may require tuning to
work well. Third, policy-based methods are effective in large or
continuous action spaces. Small changes in <span class="arithmatex">\(\theta\)</span> lead to small
changes in <span class="arithmatex">\(\pi\)</span>, and to small changes in state distributions (they are
smooth). Policy-based algorithms do not suffer (as much) from
convergence and stability issues that are seen in <span class="arithmatex">\(\arg\max\)</span>-based
algorithms in large or continuous action spaces.</p>
<p>On the other hand, there are disadvantages to the episodic Monte Carlo
version of the REINFORCE algorithm. Remember that REINFORCE generates a
full random episode in each iteration before it assesses the quality.
(Value-based methods use a reward to select the next action in each time
step of the episode.) Because of this, policy-based methods exhibit low
bias since full random trajectories are generated. However, they are
also high variance, since the full trajectory is generated randomly,
whereas value-based methods use the value for guidance at each selection
step.</p>
<p>What are the consequences? First, policy evaluation of full trajectories
has low sample efficiency and high variance. As a consequence, policy
improvement happens infrequently, leading to slow convergence compared
to value-based methods. Second, this approach often finds a local
optimum, since convergence to the global optimum takes too long.</p>
<p>Much research has been performed to address the high variance of the
episode- based vanilla policy gradient. The enhancements that have been
found have greatly improved performance, so much so that policy-based
approaches---such as A3C, PPO, SAC, and DDPG---have become favorite
model-free reinforcement learning algorithms for many applications.</p>
<h2 id="authors">Author(s)</h2>
<div class="grid cards">
<ul>
<li><img align="left" alt="Instructor Avatar" src="/assets/images/staff/Nima-Shirzady.jpg" width="150" />
    <span class="description">
        <p><strong>Nima Shirzady</strong></p>
        <p>Teaching Assistant</p>
        <p><a href="mailto:shirzady.1934@gmail.com">shirzady.1934@gmail.com</a></p>
        <p>
        <a href="https://github.com/shirzady1934" target="_blank"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg></span></a>
        <a href="https://www.linkedin.com/in/shirzady" target="_blank"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M100.3 448H7.4V148.9h92.9zM53.8 108.1C24.1 108.1 0 83.5 0 53.8c0-14.3 5.7-27.9 15.8-38S39.6 0 53.8 0s27.9 5.7 38 15.8 15.8 23.8 15.8 38c0 29.7-24.1 54.3-53.8 54.3M447.9 448h-92.7V302.4c0-34.7-.7-79.2-48.3-79.2-48.3 0-55.7 37.7-55.7 76.7V448h-92.8V148.9h89.1v40.8h1.3c12.4-23.5 42.7-48.3 87.9-48.3 94 0 111.3 61.9 111.3 142.3V448z"/></svg></span></a>
        </p>
    </span></li>
<li><img align="left" alt="Instructor Avatar" src="/assets/images/staff/Ali-MirGhasemi.jpg" width="150" />
    <span class="description">
        <p><strong>SeyyedAli MirGhasemi</strong></p>
        <p>Teaching Assistant</p>
        <p><a href="mailto:sam717269@gmail.com">sam717269@gmail.com</a></p>
        <p>
        <a href="https://www.linkedin.com/in/sayyed-ali-mirghasemi-6033661b9" target="_blank"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M100.3 448H7.4V148.9h92.9zM53.8 108.1C24.1 108.1 0 83.5 0 53.8c0-14.3 5.7-27.9 15.8-38S39.6 0 53.8 0s27.9 5.7 38 15.8 15.8 23.8 15.8 38c0 29.7-24.1 54.3-53.8 54.3M447.9 448h-92.7V302.4c0-34.7-.7-79.2-48.3-79.2-48.3 0-55.7 37.7-55.7 76.7V448h-92.8V148.9h89.1v40.8h1.3c12.4-23.5 42.7-48.3 87.9-48.3 94 0 111.3 61.9 111.3 142.3V448z"/></svg></span></a>
        </p>
    </span></li>
<li><img align="left" alt="Instructor Avatar" src="/assets/images/staff/Hesam-Hosseini.jpg" width="150" />
    <span class="description">
        <p><strong>Hesam Hosseini</strong></p>
        <p>Teaching Assistant</p>
        <p><a href="mailto:hesam138122@gmail.com">hesam138122@gmail.com</a></p>
        <p>
        <a href="https://scholar.google.com/citations?user=ODTtV1gAAAAJ&amp;hl=en" target="_blank"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M390.9 298.5s0 .1.1.1c9.2 19.4 14.4 41.1 14.4 64C405.3 445.1 338.5 512 256 512s-149.3-66.9-149.3-149.3c0-22.9 5.2-44.6 14.4-64 1.7-3.6 3.6-7.2 5.6-10.7q6.6-11.4 15-21.3c27.4-32.6 68.5-53.3 114.4-53.3 33.6 0 64.6 11.1 89.6 29.9 9.1 6.9 17.4 14.7 24.8 23.5 5.6 6.6 10.6 13.8 15 21.3 2 3.4 3.8 7 5.5 10.5zm26.4-18.8c-30.1-58.4-91-98.4-161.3-98.4s-131.2 40-161.3 98.4L0 202.7 256 0l256 202.7-94.7 77.1z"/></svg></span></a>
        <a href="https://github.com/Sam-the-first" target="_blank"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg></span></a>
        <a href="https://www.linkedin.com/in/hesam-hosseini-b57092259" target="_blank"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M100.3 448H7.4V148.9h92.9zM53.8 108.1C24.1 108.1 0 83.5 0 53.8c0-14.3 5.7-27.9 15.8-38S39.6 0 53.8 0s27.9 5.7 38 15.8 15.8 23.8 15.8 38c0 29.7-24.1 54.3-53.8 54.3M447.9 448h-92.7V302.4c0-34.7-.7-79.2-48.3-79.2-48.3 0-55.7 37.7-55.7 76.7V448h-92.8V148.9h89.1v40.8h1.3c12.4-23.5 42.7-48.3 87.9-48.3 94 0 111.3 61.9 111.3 142.3V448z"/></svg></span></a>
        </p>
    </span></li>
</ul>
</div>
<h1 id="references">References</h1>
<ol>
<li>
<p><a href="http://172.27.48.15/Resources/Books/Nikolic%20L.%20Reinforcement%20Learning%20Explained.%20A%20Step-by-Step%20Guide...2023.pdf">Reinforcement Learning Explained</a></p>
</li>
<li>
<p><a href="http://172.27.48.15/Resources/Books/Textbooks/An%20Introduction%20to%20Deep%20Reinforcement%20Learning.pdf">An Introduction to Deep Reinforcement Learning</a></p>
</li>
<li>
<p><a href="http://172.27.48.15/Resources/Books/Juhyoung%20Lee%2C%20Hoi-Jun%20Yoo%20-%20Deep%20Reinforcement%20Learning%20Processor%20Design%20for%20Mobile%20Applications-Springer%20%282023%29.pdf">Deep Reinforcement Learning Processor Design for Mobile Applications</a></p>
</li>
<li>
<p><a href="https://medium.com/intro-to-artificial-intelligence/reinforce-a-policy-gradient-based-reinforcement-learning-algorithm-84bde440c816">REINFORCE  a policy-gradient based reinforcement Learning algorithm</a></p>
</li>
<li>
<p><a href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/">Policy Gradient Algorithms</a></p>
</li>
<li>
<p><a href="http://172.27.48.15/Resources/Books/Textbooks/Aske%20Plaat%20-%20Deep%20Reinforcement%20Learning-arXiv%20%282023%29.pdf">Deep Reinforcement Learning</a></p>
</li>
<li>
<p><a href="https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf">Reinforcement Learning (BartoSutton)</a></p>
</li>
</ol>







  
  



  



  <form class="md-feedback" name="feedback" hidden>
    <fieldset>
      <legend class="md-feedback__title">
        Was this page helpful?
      </legend>
      <div class="md-feedback__inner">
        <div class="md-feedback__list">
          
            <button class="md-feedback__icon md-icon" type="submit" title="This page was helpful" data-md-value="1">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m7 0c0 .8-.7 1.5-1.5 1.5S14 10.3 14 9.5 14.7 8 15.5 8s1.5.7 1.5 1.5m-5 7.73c-1.75 0-3.29-.73-4.19-1.81L9.23 14c.45.72 1.52 1.23 2.77 1.23s2.32-.51 2.77-1.23l1.42 1.42c-.9 1.08-2.44 1.81-4.19 1.81"/></svg>
            </button>
          
            <button class="md-feedback__icon md-icon" type="submit" title="This page could be improved" data-md-value="0">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10m-6.5-4c.8 0 1.5.7 1.5 1.5s-.7 1.5-1.5 1.5-1.5-.7-1.5-1.5.7-1.5 1.5-1.5M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m2 4.5c1.75 0 3.29.72 4.19 1.81l-1.42 1.42C14.32 16.5 13.25 16 12 16s-2.32.5-2.77 1.23l-1.42-1.42C8.71 14.72 10.25 14 12 14"/></svg>
            </button>
          
        </div>
        <div class="md-feedback__note">
          
            <div data-md-value="1" hidden>
              
              
                
              
              
              
                
                
              
              Thanks for your feedback!
            </div>
          
            <div data-md-value="0" hidden>
              
              
                
              
              
              
                
                
              
              Thanks for your feedback! Help us improve this page by using our <a href="..." target="_blank" rel="noopener">feedback form</a>.
            </div>
          
        </div>
      </div>
    </fieldset>
  </form>


<script src="https://giscus.app/client.js"
        data-repo="DeepRLCourse/DeepRLCourse.github.io"
        data-repo-id="R_kgDOMPOFbQ"
        data-category="Announcements"
        data-category-id="DIC_kwDOMPOFbc4CmGY6"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="1"
        data-input-position="top"
        data-theme="preferred_color_scheme"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>

                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Made with  in Robust and Interpretable Machine Learning Lab
    </div>
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.top", "toc.integrate"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>