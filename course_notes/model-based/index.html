
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="This page provides a comprehensive overview of Model-Based Reinforcement Learning (MBRL), covering foundational concepts, key methodologies, and modern algorithms. It explores the integration of planning and learning, dynamics model learning, and advanced techniques for handling stochasticity, uncertainty, and partial observability. The document also highlights state-of-the-art MBRL algorithms such as PETS, MBPO, Dreamer, and MuZero, emphasizing their benefits, challenges, and applications in achieving sample-efficient and robust decision-making.">
      
      
      
        <link rel="canonical" href="https://DeepRL2026.github.io/course_notes/model-based/">
      
      
      
      
        
      
      
      <link rel="icon" href="../../assets/favicon/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.3">
    
    
      
        <title>Week 5: Model-Based Methods - Deep RL Course</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300,300i,400,400i,700,700i%7CRed+Hat+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Ubuntu";--md-code-font:"Red Hat Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
      <link rel="stylesheet" href="../../stylesheets/vazirmatn.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-Q1YT8K39WE"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-Q1YT8K39WE",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-Q1YT8K39WE",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
  
<meta property="og:type" content="website" />
<meta property="og:title" content="Week 5: Model-Based Methods - Deep RL Course" />
<meta property="og:description" content="This page provides a comprehensive overview of Model-Based Reinforcement Learning (MBRL), covering foundational concepts, key methodologies, and modern algorithms. It explores the integration of planning and learning, dynamics model learning, and advanced techniques for handling stochasticity, uncertainty, and partial observability. The document also highlights state-of-the-art MBRL algorithms such as PETS, MBPO, Dreamer, and MuZero, emphasizing their benefits, challenges, and applications in achieving sample-efficient and robust decision-making." />
<meta property="og:image" content="https://DeepRL2026.github.io/assets/images/social/course_notes/model-based.png" />
<meta property="og:image:type" content="image/png" />
<meta property="og:image:width" content="1200" />
<meta property="og:image:height" content="630" />
<meta property="og:url" content="https://DeepRL2026.github.io/course_notes/model-based/" />
<meta property="twitter:card" content="summary_large_image" />
<meta property="twitter:title" content="Week 5: Model-Based Methods - Deep RL Course" />
<meta property="twitter:description" content="This page provides a comprehensive overview of Model-Based Reinforcement Learning (MBRL), covering foundational concepts, key methodologies, and modern algorithms. It explores the integration of planning and learning, dynamics model learning, and advanced techniques for handling stochasticity, uncertainty, and partial observability. The document also highlights state-of-the-art MBRL algorithms such as PETS, MBPO, Dreamer, and MuZero, emphasizing their benefits, challenges, and applications in achieving sample-efficient and robust decision-making." />
<meta property="twitter:image" content="https://DeepRL2026.github.io/assets/images/social/course_notes/model-based.png" />
</head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#week-5-model-based-methods" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Deep RL Course" class="md-header__button md-logo" aria-label="Deep RL Course" data-md-component="logo">
      
  <img src="../../assets/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Deep RL Course
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Week 5: Model-Based Methods
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="red"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="red"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  Home

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  Course Notes

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../lectures/lecture1/" class="md-tabs__link">
          
  
  
  Lectures

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  TA Sessions

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  Homeworks

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  Projects

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../exams/" class="md-tabs__link">
          
  
  
  Exams

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../resources/" class="md-tabs__link">
          
  
  
  Resources

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
    
      

    

    

        

            
                  
            

        

            

        
    

    


    <!-- Navigation -->
    
        
        <div
            class="md-sidebar md-sidebar--primary"
            data-md-component="sidebar"
            data-md-type="navigation"
            
        >
            <div class="md-sidebar__scrollwrap">
            <div class="md-sidebar__inner">
                <!--
  Copyright (c) 2016-2025 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->



<!-- Determine classes -->


  


  


<!-- Navigation -->
<nav
  class="md-nav md-nav--primary md-nav--lifted"
  aria-label="Navigation"
  data-md-level="0"
>

  <!-- Site title -->
  <label class="md-nav__title" for="__drawer">
    <a
      href="../.."
      title="Deep RL Course"
      class="md-nav__button md-logo"
      aria-label="Deep RL Course"
      data-md-component="logo"
    >
      
  <img src="../../assets/logo.png" alt="logo">

    </a>
    Deep RL Course
  </label>

  <!-- Repository information -->
  

  <!-- Navigation list -->
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Home
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Course Notes
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Course Notes
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Lectures
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Lectures
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../lectures/lecture1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Lecture1
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../lectures/lecture2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Lecture2
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    TA Sessions
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    TA Sessions
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Homeworks
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    Homeworks
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Projects
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    Projects
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Exams
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            
  
    Exams
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../exams/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Previous Semesters
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Resources
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            
  
    Resources
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../resources/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Resources
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
            </div>
            </div>
        </div>
    

    <!-- Table of contents -->
    
        
        <div
            class="md-sidebar md-sidebar--secondary"
            data-md-component="sidebar"
            data-md-type="toc"
            
        >
            <div class="md-sidebar__scrollwrap">
            <div class="md-sidebar__inner">
                <!--
  Copyright (c) 2016-2025 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->

<!-- Determine title -->




<!-- Table of contents -->
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  

  <!--
    Check whether the content starts with a level 1 headline. If it does, the
    top-level anchor must be skipped, since it would be redundant to the link
    to the current page that is located just above the anchor. Therefore we
    directly continue with the children of the anchor.
  -->
  
  
    
  

  <!-- Table of contents title and list -->
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#table-of-contents" class="md-nav__link">
    <span class="md-ellipsis">
      
        Table of Contents
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-introduction-scope" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. Introduction &amp; Scope
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-markov-decision-processes" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Markov Decision Processes
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-categories-of-model-based-rl" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Categories of Model-Based RL
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Categories of Model-Based RL">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-planning" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.1 Planning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-model-free-rl" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.2 Model-Free RL
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-model-based-rl-model-global-policyvalue" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.3 Model-Based RL (Model + Global Policy/Value)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3.3 Model-Based RL (Model + Global Policy/Value)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#331-model-based-rl-with-a-known-model" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.3.1 Model-Based RL with a Known Model
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#332-model-based-rl-with-a-learned-model" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.3.2 Model-Based RL with a Learned Model
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34-planning-over-a-learned-model-without-a-global-policy" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.4 Planning Over a Learned Model Without a Global Policy
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-basic-schemes" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Basic Schemes
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. Basic Schemes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#version-05-single-shot-model-planning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Version 0.5: Single-Shot Model + Planning
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Version 0.5: Single-Shot Model + Planning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#core-steps" class="md-nav__link">
    <span class="md-ellipsis">
      
        Core Steps
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shortcomings" class="md-nav__link">
    <span class="md-ellipsis">
      
        Shortcomings
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-move-on" class="md-nav__link">
    <span class="md-ellipsis">
      
        Why Move On?
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#version-10-iterative-re-fitting-planning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Version 1.0: Iterative Re-Fitting + Planning
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Version 1.0: Iterative Re-Fitting + Planning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#core-steps_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Core Steps
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shortcomings_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Shortcomings
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-move-on_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Why Move On?
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#version-15-model-predictive-control-mpc" class="md-nav__link">
    <span class="md-ellipsis">
      
        Version 1.5: Model Predictive Control (MPC)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Version 1.5: Model Predictive Control (MPC)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#core-steps_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Core Steps
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shortcomings_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Shortcomings
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-move-on_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Why Move On?
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#version-20-backprop-through-the-learned-model" class="md-nav__link">
    <span class="md-ellipsis">
      
        Version 2.0: Backprop Through the Learned Model
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Version 2.0: Backprop Through the Learned Model">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#core-steps_3" class="md-nav__link">
    <span class="md-ellipsis">
      
        Core Steps
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shortcomings_3" class="md-nav__link">
    <span class="md-ellipsis">
      
        Shortcomings
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-move-on_3" class="md-nav__link">
    <span class="md-ellipsis">
      
        Why Move On?
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reward-overshooting-overoptimistic-planning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Reward Overshooting (Overoptimistic Planning)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Reward Overshooting (Overoptimistic Planning)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-it-is" class="md-nav__link">
    <span class="md-ellipsis">
      
        What It Is
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-it-happens" class="md-nav__link">
    <span class="md-ellipsis">
      
        Why It Happens
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#consequences" class="md-nav__link">
    <span class="md-ellipsis">
      
        Consequences
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mitigation-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      
        Mitigation Strategies
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#other-challenges-and-notes" class="md-nav__link">
    <span class="md-ellipsis">
      
        Other Challenges and Notes
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-dynamics-model-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. Dynamics Model Learning
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Dynamics Model Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-basic-considerations" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.1 Basic Considerations
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52-stochasticity" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.2 Stochasticity
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.2 Stochasticity">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#521-multi-modal-transitions-and-the-conditional-mean-problem" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.2.1 Multi-Modal Transitions and the Conditional Mean Problem
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#522-descriptive-distribution-models" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.2.2 Descriptive (Distribution) Models
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#523-generative-approaches" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.2.3 Generative Approaches
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#524-training-objectives" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.2.4 Training Objectives
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#525-practical-considerations-and-challenges" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.2.5 Practical Considerations and Challenges
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#526-example-gaussian-transitions-via-maximum-likelihood" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.2.6 Example: Gaussian Transitions via Maximum Likelihood
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#527-concluding-remarks-on-stochastic-transitions" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.2.7 Concluding Remarks on Stochastic Transitions
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#53-uncertainty" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.3 Uncertainty
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.3 Uncertainty">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bayesian-neural-networks" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bayesian Neural Networks
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ensembles-and-bootstrapping" class="md-nav__link">
    <span class="md-ellipsis">
      
        Ensembles and Bootstrapping
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#54-partial-observability" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.4 Partial Observability
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#55-non-stationarity" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.5 Non-Stationarity
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#56-multi-step-prediction" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.6 Multi-Step Prediction
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#57-state-abstraction" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.7 State Abstraction
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.7 State Abstraction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#571-common-approaches-to-representation-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.7.1 Common Approaches to Representation Learning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#572-planning-in-latent-space" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.7.2 Planning in Latent Space
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#58-temporal-abstraction" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.8 Temporal Abstraction
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5.8 Temporal Abstraction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#581-options-framework" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.8.1 Options Framework
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#582-goal-conditioned-policies" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.8.2 Goal-Conditioned Policies
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#583-subgoal-discovery" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.8.3 Subgoal Discovery
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#584-benefits-of-temporal-abstraction" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.8.4 Benefits of Temporal Abstraction
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-integration-of-planning-and-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        6. Integration of Planning and Learning
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. Integration of Planning and Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#61-which-state-to-start-planning-from" class="md-nav__link">
    <span class="md-ellipsis">
      
        6.1 Which State to Start Planning From?
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#62-planning-budget-vs-real-data-collection" class="md-nav__link">
    <span class="md-ellipsis">
      
        6.2 Planning Budget vs. Real Data Collection
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#63-how-to-plan-planning-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      
        6.3 How to Plan? (Planning Algorithms)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6.3 How to Plan? (Planning Algorithms)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#631-monte-carlo-tree-search-mcts" class="md-nav__link">
    <span class="md-ellipsis">
      
        6.3.1 Monte Carlo Tree Search (MCTS)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#64-integration-in-the-learning-and-acting-loop" class="md-nav__link">
    <span class="md-ellipsis">
      
        6.4 Integration in the Learning and Acting Loop
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#65-dyna-and-dyna-style-methods" class="md-nav__link">
    <span class="md-ellipsis">
      
        6.5 Dyna and Dyna-Style Methods
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-modern-model-based-rl-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      
        7. Modern Model-Based RL Algorithms
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7. Modern Model-Based RL Algorithms">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#71-world-models-ha-schmidhuber-2018" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.1 World Models (Ha &amp; Schmidhuber, 2018)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#72-pets-chua-et-al-2018" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.2 PETS (Chua et al., 2018)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#73-mbpo-janner-et-al-2019" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.3 MBPO (Janner et al., 2019)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#74-dreamer-hafner-et-al-20202023" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.4 Dreamer (Hafner et al., 2020–2023)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#75-muzero-deepmind-2020" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.5 MuZero (DeepMind, 2020)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-key-benefits-and-drawbacks-of-mbrl" class="md-nav__link">
    <span class="md-ellipsis">
      
        8. Key Benefits (and Drawbacks) of MBRL
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8. Key Benefits (and Drawbacks) of MBRL">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#81-data-efficiency" class="md-nav__link">
    <span class="md-ellipsis">
      
        8.1 Data Efficiency
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#82-exploration" class="md-nav__link">
    <span class="md-ellipsis">
      
        8.2 Exploration
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#83-optimality" class="md-nav__link">
    <span class="md-ellipsis">
      
        8.3 Optimality
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#84-transfer" class="md-nav__link">
    <span class="md-ellipsis">
      
        8.4 Transfer
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#85-safety" class="md-nav__link">
    <span class="md-ellipsis">
      
        8.5 Safety
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#86-explainability" class="md-nav__link">
    <span class="md-ellipsis">
      
        8.6 Explainability
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#87-disbenefits" class="md-nav__link">
    <span class="md-ellipsis">
      
        8.7 Disbenefits
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9-conclusion" class="md-nav__link">
    <span class="md-ellipsis">
      
        9. Conclusion
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#10-references" class="md-nav__link">
    <span class="md-ellipsis">
      
        10. References
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
            </div>
            </div>
        </div>
    

          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="week-5-model-based-methods">Week 5: Model-Based Methods</h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This document merges <strong>Lectures 9 and 10</strong> from Prof. Mohammad Hossein Rohban's DRL course, <strong>Lecture 9: Model-Based RL</strong> slides from Prof. Sergey Levine’s CS 294-112 (Deep RL) with a <strong>more rigorous, survey-based structure</strong> drawing on Moerland et al. (2022). We provide intuitions, mathematical details, and references to relevant works.</p>
</div>
<h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#week-5-model-based-methods">Week 5: Model-Based Methods</a></li>
<li><a href="#table-of-contents">Table of Contents</a></li>
<li><a href="#1-introduction--scope">1. Introduction \&amp; Scope</a></li>
<li><a href="#2-markov-decision-processes">2. Markov Decision Processes</a></li>
<li><a href="#3-categories-of-model-based-rl">3. Categories of Model-Based RL</a><ul>
<li><a href="#31-planning">3.1 Planning</a></li>
<li><a href="#32-model-free-rl">3.2 Model-Free RL</a></li>
<li><a href="#33-model-based-rl-model--global-policyvalue">3.3 Model-Based RL (Model + Global Policy/Value)</a></li>
<li><a href="#331-model-based-rl-with-a-known-model">3.3.1 Model-Based RL with a <em>Known</em> Model</a></li>
<li><a href="#332-model-based-rl-with-a-learned-model">3.3.2 Model-Based RL with a <em>Learned</em> Model</a></li>
<li><a href="#34-planning-over-a-learned-model-without-a-global-policy">3.4 Planning Over a Learned Model Without a Global Policy</a></li>
</ul>
</li>
<li><a href="#4-basic-schemes">4. Basic Schemes</a><ul>
<li><a href="#version-05-single-shot-model--planning">Version 0.5: Single-Shot Model + Planning</a></li>
<li><a href="#core-steps">Core Steps</a></li>
<li><a href="#shortcomings">Shortcomings</a></li>
<li><a href="#why-move-on">Why Move On?</a></li>
<li><a href="#version-10-iterative-re-fitting--planning">Version 1.0: Iterative Re-Fitting + Planning</a></li>
<li><a href="#core-steps-1">Core Steps</a></li>
<li><a href="#shortcomings-1">Shortcomings</a></li>
<li><a href="#why-move-on-1">Why Move On?</a></li>
<li><a href="#version-15-model-predictive-control-mpc">Version 1.5: Model Predictive Control (MPC)</a></li>
<li><a href="#core-steps-2">Core Steps</a></li>
<li><a href="#shortcomings-2">Shortcomings</a></li>
<li><a href="#why-move-on-2">Why Move On?</a></li>
<li><a href="#version-20-backprop-through-the-learned-model">Version 2.0: Backprop Through the Learned Model</a></li>
<li><a href="#core-steps-3">Core Steps</a></li>
<li><a href="#shortcomings-3">Shortcomings</a></li>
<li><a href="#why-move-on-3">Why Move On?</a></li>
<li><a href="#reward-overshooting-overoptimistic-planning">Reward Overshooting (Overoptimistic Planning)</a></li>
<li><a href="#what-it-is">What It Is</a></li>
<li><a href="#why-it-happens">Why It Happens</a></li>
<li><a href="#consequences">Consequences</a></li>
<li><a href="#mitigation-strategies">Mitigation Strategies</a></li>
<li><a href="#other-challenges-and-notes">Other Challenges and Notes</a></li>
</ul>
</li>
<li><a href="#5-dynamics-model-learning">5. Dynamics Model Learning</a><ul>
<li><a href="#51-basic-considerations">5.1 Basic Considerations</a></li>
<li><a href="#52-stochasticity">5.2 Stochasticity</a></li>
<li><a href="#521-multi-modal-transitions-and-the-conditional-mean-problem">5.2.1 Multi-Modal Transitions and the Conditional Mean Problem</a></li>
<li><a href="#522-descriptive-distribution-models">5.2.2 Descriptive (Distribution) Models</a></li>
<li><a href="#523-generative-approaches">5.2.3 Generative Approaches</a></li>
<li><a href="#524-training-objectives">5.2.4 Training Objectives</a></li>
<li><a href="#525-practical-considerations-and-challenges">5.2.5 Practical Considerations and Challenges</a></li>
<li><a href="#526-example-gaussian-transitions-via-maximum-likelihood">5.2.6 Example: Gaussian Transitions via Maximum Likelihood</a></li>
<li><a href="#527-concluding-remarks-on-stochastic-transitions">5.2.7 Concluding Remarks on Stochastic Transitions</a></li>
<li><a href="#53-uncertainty">5.3 Uncertainty</a></li>
<li><a href="#bayesian-neural-networks">Bayesian Neural Networks</a></li>
<li><a href="#ensembles-and-bootstrapping">Ensembles and Bootstrapping</a></li>
<li><a href="#54-partial-observability">5.4 Partial Observability</a></li>
<li><a href="#55-non-stationarity">5.5 Non-Stationarity</a></li>
<li><a href="#56-multi-step-prediction">5.6 Multi-Step Prediction</a></li>
<li><a href="#57-state-abstraction">5.7 State Abstraction</a></li>
<li><a href="#571-common-approaches-to-representation-learning">5.7.1 Common Approaches to Representation Learning</a></li>
<li><a href="#572-planning-in-latent-space">5.7.2 Planning in Latent Space</a></li>
<li><a href="#58-temporal-abstraction">5.8 Temporal Abstraction</a></li>
<li><a href="#581-options-framework">5.8.1 Options Framework</a></li>
<li><a href="#582-goal-conditioned-policies">5.8.2 Goal-Conditioned Policies</a></li>
<li><a href="#583-subgoal-discovery">5.8.3 Subgoal Discovery</a></li>
<li><a href="#584-benefits-of-temporal-abstraction">5.8.4 Benefits of Temporal Abstraction</a></li>
</ul>
</li>
<li><a href="#6-integration-of-planning-and-learning">6. Integration of Planning and Learning</a><ul>
<li><a href="#61-which-state-to-start-planning-from">6.1 Which State to Start Planning From?</a></li>
<li><a href="#62-planning-budget-vs-real-data-collection">6.2 Planning Budget vs. Real Data Collection</a></li>
<li><a href="#63-how-to-plan-planning-algorithms">6.3 How to Plan? (Planning Algorithms)</a></li>
<li><a href="#631-monte-carlo-tree-search-mcts">6.3.1 Monte Carlo Tree Search (MCTS)</a></li>
<li><a href="#64-integration-in-the-learning-and-acting-loop">6.4 Integration in the Learning and Acting Loop</a></li>
<li><a href="#65-dyna-and-dyna-style-methods">6.5 Dyna and Dyna-Style Methods</a></li>
</ul>
</li>
<li><a href="#7-modern-model-based-rl-algorithms">7. Modern Model-Based RL Algorithms</a><ul>
<li><a href="#71-world-models-ha--schmidhuber-2018">7.1 World Models (Ha \&amp; Schmidhuber, 2018)</a></li>
<li><a href="#72-pets-chua-et-al-2018">7.2 PETS (Chua et al., 2018)</a></li>
<li><a href="#73-mbpo-janner-et-al-2019">7.3 MBPO (Janner et al., 2019)</a></li>
<li><a href="#74-dreamer-hafner-et-al-20202023">7.4 Dreamer (Hafner et al., 2020–2023)</a></li>
<li><a href="#75-muzero-deepmind-2020">7.5 MuZero (DeepMind, 2020)</a></li>
</ul>
</li>
<li><a href="#8-key-benefits-and-drawbacks-of-mbrl">8. Key Benefits (and Drawbacks) of MBRL</a><ul>
<li><a href="#81-data-efficiency">8.1 Data Efficiency</a></li>
<li><a href="#82-exploration">8.2 Exploration</a></li>
<li><a href="#83-optimality">8.3 Optimality</a></li>
<li><a href="#84-transfer">8.4 Transfer</a></li>
<li><a href="#85-safety">8.5 Safety</a></li>
<li><a href="#86-explainability">8.6 Explainability</a></li>
<li><a href="#87-disbenefits">8.7 Disbenefits</a></li>
</ul>
</li>
<li><a href="#9-conclusion">9. Conclusion</a></li>
<li><a href="#10-references">10. References</a></li>
</ul>
<hr />
<h2 id="1-introduction-scope">1. Introduction &amp; Scope</h2>
<p>Model-Based Reinforcement Learning (MBRL) combines <strong>planning</strong> (using a model of environment dynamics) and <strong>learning</strong> (to approximate value functions or policies globally). MBRL benefits from being able to reason about environment dynamics “in imagination,” thus often achieving higher sample efficiency than purely model-free RL. However, ensuring accurate models and mitigating compounding errors pose key challenges.</p>
<p>We address:</p>
<ol>
<li>The <strong>MDP</strong> framework and definitions.  </li>
<li><strong>Model learning</strong>: from basic supervised regression to advanced methods handling stochasticity, uncertainty, partial observability, etc.  </li>
<li><strong>Integrating planning</strong>: how to incorporate planning loops, short vs. long horizons, and real-world data interplay.  </li>
<li><strong>Modern MBRL</strong> algorithms (World Models, PETS, MBPO, Dreamer, MuZero).  </li>
<li><strong>Benefits and drawbacks</strong> of MBRL.</li>
</ol>
<hr />
<h2 id="2-markov-decision-processes">2. Markov Decision Processes</h2>
<p>We adopt the standard <strong>Markov Decision Process (MDP)</strong> formulation [Puterman, 2014]:</p>
<div class="arithmatex">\[
\mathcal{M} = \bigl(\mathcal{S}, \mathcal{A}, P, R, p(s_0), \gamma\bigr),
\]</div>
<p>where:
- <span class="arithmatex">\(\mathcal{S}\)</span> is the (possibly high-dimensional) state space.<br />
- <span class="arithmatex">\(\mathcal{A}\)</span> is the action space (can be discrete or continuous).<br />
- <span class="arithmatex">\(P(s_{t+1}\mid s_t,a_t)\)</span> is the transition distribution.<br />
- <span class="arithmatex">\(R(s_t,a_t,s_{t+1})\)</span> is the reward function.<br />
- <span class="arithmatex">\(p(s_0)\)</span> is the initial-state distribution.<br />
- <span class="arithmatex">\(\gamma \in [0,1]\)</span> is the discount factor.</p>
<p>A <strong>policy</strong> <span class="arithmatex">\(\pi(a \mid s)\)</span> dictates which action to choose at each state. The <strong>value function</strong> and <strong>action-value function</strong> are:</p>
<div class="arithmatex">\[
V^\pi(s) \;=\; \mathbb{E}\Bigl[\sum_{k=0}^\infty \gamma^k r_{t+k}\;\big|\;s_t = s,\;\pi\Bigr],
\]</div>
<div class="arithmatex">\[
Q^\pi(s,a) \;=\; \mathbb{E}\Bigl[\sum_{k=0}^\infty \gamma^k r_{t+k}\;\big|\;s_t = s,\,a_t = a,\;\pi\Bigr].
\]</div>
<p>We want to find <span class="arithmatex">\(\pi^\star\)</span> that maximizes expected return. <strong>Model-Based RL</strong> obtains a <strong>model</strong> of the environment’s dynamics <span class="arithmatex">\( \hat{P}, \hat{R}\)</span>, then uses <strong>planning</strong> with that model (e.g., rollouts, search) to aid in learning or acting.</p>
<hr />
<h2 id="3-categories-of-model-based-rl">3. Categories of Model-Based RL</h2>
<p>Following Moerland et al., we distinguish:</p>
<ol>
<li><strong>Planning</strong> (known model, local solutions).  </li>
<li><strong>Model-Free RL</strong> (no explicit model, but learns a global policy or value).  </li>
<li><strong>Model-Based RL</strong> (learned or known model <strong>and</strong> a global policy/value solution).</li>
</ol>
<p><strong>Model-Based RL</strong> itself splits into two key variants:</p>
<ul>
<li><strong>Model-based RL with a <em>known</em> model</strong>: E.g., AlphaZero uses perfect board-game rules.  </li>
<li><strong>Model-based RL with a <em>learned</em> model</strong>: E.g., Dyna, MBPO, Dreamer, where the agent must learn <span class="arithmatex">\(\hat{P}(s_{t+1}\mid s_t,a_t)\)</span>.</li>
</ul>
<p>In addition, one could do <strong>planning over a learned model</strong> but never store a global policy or value (just do local search each time)—that’s still “planning + learning,” but not strictly “model-based RL” if no global policy is learned in the end .</p>
<hr />
<h3 id="31-planning">3.1 Planning</h3>
<p>Planning methods assume access to a <em>perfect</em> model of the environment’s dynamics <span class="arithmatex">\(\mathcal{P}(s_{t+1} \mid s_t, a_t)\)</span> and reward function <span class="arithmatex">\(r(s_t,a_t)\)</span>. In other words, the transition probabilities and/or the state transitions are <em>fully known</em>. Given this perfect model, the agent can perform a search procedure (e.g., lookahead search, tree search) to find the best action from the <em>current</em> state.</p>
<ul>
<li><strong>Local Search</strong>: Typically, planning algorithms only compute a solution <em>locally</em>, from the agent’s current state or a small set of states. They do not necessarily store or learn a <em>global</em> policy (i.e., a mapping from any possible state to an action).</li>
<li><strong>Classical Example</strong>: In board games (like chess or Go), an algorithm such as minimax with alpha–beta pruning uses the known, <em>perfect</em> rules of the game to explore future states and pick an optimal move from the <em>current</em> position.</li>
</ul>
<p>Because these approaches do not usually store or learn a parametric global policy or value function, they fall under “Planning” rather than “Model-Based RL.”</p>
<hr />
<h3 id="32-model-free-rl">3.2 Model-Free RL</h3>
<p>Model-Free RL methods <em>do not</em> explicitly use or learn the environment’s transition model. Instead, they optimize a policy <span class="arithmatex">\(\pi_\theta(a_t \mid s_t)\)</span> or a value function <span class="arithmatex">\(V_\theta(s_t)\)</span> (or both) solely based on interactions with the environment.</p>
<ul>
<li><strong>No Transition Model</strong>: The policy or value function is learned directly from sampled trajectories <span class="arithmatex">\((s_t, a_t, r_t, s_{t+1}, \dots)\)</span>. There is no component that learns <span class="arithmatex">\(\hat{P}(s_{t+1} \mid s_t,a_t)\)</span>.</li>
<li><strong>Global Solutions</strong>: Model-free methods generally learn <em>global</em> solutions: policies or value functions valid across all states encountered during training.</li>
<li><strong>Examples</strong>: Deep Q-Networks (DQN), Policy Gradient methods (REINFORCE, PPO), and actor–critic approaches.</li>
</ul>
<p>Despite being effective, model-free methods may require large amounts of environment interaction, since they cannot leverage planning over a learned or known model.</p>
<hr />
<h3 id="33-model-based-rl-model-global-policyvalue">3.3 Model-Based RL (Model + Global Policy/Value)</h3>
<p>In Model-Based RL, the agent has (or learns) a model of the environment and uses it to learn a <em>global</em> policy or value function. The policy or value function can then be used to make decisions <em>for all states</em>, not just the current one. This class of methods can further be subdivided into two main variants:</p>
<h4 id="331-model-based-rl-with-a-known-model">3.3.1 Model-Based RL with a <em>Known</em> Model</h4>
<p>In some tasks, the transition model <span class="arithmatex">\(\mathcal{P}(s_{t+1} \mid s_t, a_t)\)</span> is known in advance (e.g., it is given by the rules of the environment). The agent can then use this <em>perfect</em> model to plan and to learn a global policy or value function. </p>
<ul>
<li><strong>AlphaZero</strong>: A canonical example in board games (chess, Go, shogi). The rules of the game form a perfect simulator. AlphaZero does extensive lookahead (tree search), but it also uses that data to update a global policy and value network. Thus, it integrates planning and policy learning.</li>
<li><strong>Advantages</strong>: Since the model is perfect, there is no model-learning error. The primary challenge is how to efficiently search with that model and how to integrate the search results into a global solution.</li>
</ul>
<h4 id="332-model-based-rl-with-a-learned-model">3.3.2 Model-Based RL with a <em>Learned</em> Model</h4>
<p>In many real-world tasks, the transition model is <em>not</em> known in advance. The agent must learn an <em>approximate</em> model <span class="arithmatex">\(\hat{P}(s_{t+1} \mid s_t,a_t)\)</span> from environment interactions.</p>
<ul>
<li><strong>Learning the Model</strong>: The agent collects transitions <span class="arithmatex">\((s_t, a_t, r_t, s_{t+1})\)</span> and trains a parametric model to predict the next state(s) and reward given the current state–action pair.</li>
<li><strong>Planning with the Learned Model</strong>: The agent can then plan ahead (e.g., via simulated rollouts or lookahead) in this learned model. Although approximate, it allows the agent to generate additional training data or refine its strategy without costly real-world interactions.</li>
<li><strong>Examples</strong>:</li>
<li><strong>Dyna</strong> (Sutton, 1990): Interleaves real experience with “imagined” experience from the learned model to update the value function or policy.</li>
<li><strong>MBPO (Model-Based Policy Optimization)</strong>: Uses a learned model to generate short rollouts for policy optimization.</li>
<li><strong>Dreamer</strong>: Trains a world model and then uses latent imagination (rollouts in latent space) to learn a global policy.</li>
</ul>
<hr />
<h3 id="34-planning-over-a-learned-model-without-a-global-policy">3.4 Planning Over a Learned Model Without a Global Policy</h3>
<p>An additional possibility is to <em>only</em> do planning with a learned model—without ever storing or committing to a parametric global policy or value function. In this scenario, the agent:</p>
<ul>
<li>Learns or refines a model of the environment.</li>
<li>Uses local search (e.g., tree search or some other planning method) each time to select actions.</li>
<li><strong>Does not</strong> maintain a single policy or value function that applies across all states.</li>
</ul>
<p>Although this constitutes “planning + learning” (the learning is in the model, and the planning is local search), it does <em>not</em> fully qualify as “Model-Based RL” in the strict sense—because there is no <em>global</em> policy or value function being learned. Instead, the agent repeatedly plans from scratch (or near-scratch) in the learned model.</p>
<hr />
<h2 id="4-basic-schemes">4. Basic Schemes</h2>
<p>References present high-level approaches, sometimes referred to as:</p>
<ul>
<li><strong>Version 0.5</strong>: Collect random samples once, fit a model, do single-shot planning. Risks severe distribution mismatch.  </li>
<li><strong>Version 1.0</strong>: Iterative approach (collect data, re-fit model, plan). Improves coverage, but naive open-loop plans can fail.  </li>
<li><strong>Version 1.5</strong>: <strong>Model Predictive Control (MPC)</strong>: replan at each step or on short horizons – more robust but computationally heavier.  </li>
<li><strong>Version 2.0</strong>: <strong>Backprop through the learned model</strong> directly into the policy – can be efficient at runtime, but numerically unstable for complex or stochastic tasks.</li>
</ul>
<h3 id="version-05-single-shot-model-planning">Version 0.5: Single-Shot Model + Planning</h3>
<p><img alt="Model-Based v0.5" src="../../assets/images/figures/model-based/v05.png" /></p>
<h4 id="core-steps">Core Steps</h4>
<ol>
<li>
<p><strong>One-Time Data Collection</strong>  </p>
<ul>
<li>Collect a static dataset of (state, action, next-state, reward) tuples, typically via random or fixed exploration.</li>
<li>No further data is gathered afterward.</li>
</ul>
</li>
<li>
<p><strong>One-Time Model Learning</strong>  </p>
<ul>
<li>Fit a dynamics model <span class="arithmatex">\( p_\theta(s' \mid s, a) \)</span> using the static dataset.</li>
<li>The model may be inaccurate in regions not well-represented in the dataset.</li>
</ul>
</li>
<li>
<p><strong>One-Time Planning</strong>  </p>
<ul>
<li>Use the learned model to plan or optimize a policy (e.g., via trajectory optimization or tree search).</li>
<li>The plan is executed in the real environment without re-planning.</li>
</ul>
</li>
</ol>
<h4 id="shortcomings">Shortcomings</h4>
<ul>
<li><strong>Distribution Mismatch</strong>: The policy can enter states the model has never “seen,” leading to large prediction errors (extrapolation).  </li>
<li><strong>Compounding Errors</strong>: Small modeling inaccuracies early on can push the system into unmodeled states, magnifying the errors.  </li>
<li><strong>No Iterative Refinement</strong>: With no new data collection, there’s no way to correct model inaccuracies discovered during execution.</li>
</ul>
<h4 id="why-move-on">Why Move On?</h4>
<ul>
<li><strong>Severe Mismatch &amp; Error Growth</strong>: Because you never adapt to real outcomes beyond the initial dataset, errors can escalate and result in catastrophic failures.  </li>
<li><strong>Limited Practicality</strong>: Version 0.5 can work in highly controlled or small problems, but most real tasks require iterative data gathering and model improvements.</li>
</ul>
<hr />
<h3 id="version-10-iterative-re-fitting-planning">Version 1.0: Iterative Re-Fitting + Planning</h3>
<p><img alt="Model-Based v1.0" src="../../assets/images/figures/model-based/v10.png" /></p>
<h4 id="core-steps_1">Core Steps</h4>
<ol>
<li>
<p><strong>Iterative Data Collection</strong></p>
<ul>
<li>Use a current policy (or plan) to interact with the environment.</li>
<li>Gather new transitions (state, action, next-state, reward) and add them to the dataset.</li>
</ul>
</li>
<li>
<p><strong>Re-Fit the Model</strong></p>
<ul>
<li>Update the dynamics model <span class="arithmatex">\( p_\theta(s' \mid s, a) \)</span> using the expanded dataset.</li>
<li>The model gradually learns the dynamics in regions the policy visits.</li>
</ul>
</li>
<li>
<p><strong>Re-Plan or Update the Policy</strong></p>
<ul>
<li>After each model update, re-run planning or policy optimization to refine the policy.</li>
<li>Deploy the updated policy in the real environment, collect more data, and repeat.</li>
</ul>
</li>
</ol>
<h4 id="shortcomings_1">Shortcomings</h4>
<ul>
<li><strong>Open-Loop Execution</strong>: Even though the model is updated iteratively, each plan can be executed “open loop,” so stochastic events or modest model errors can derail a plan until the <strong>next</strong> re-planning cycle.  </li>
<li><strong>Long-Horizon Vulnerability</strong>: If the planning horizon is substantial, inaccuracies can still compound within a single rollout.</li>
</ul>
<h4 id="why-move-on_1">Why Move On?</h4>
<ul>
<li><strong>Stochastic / Complex Tasks</strong>: Open-loop plans are brittle. You need a way to <strong>correct</strong> for real-time deviations instead of waiting until the next iteration.  </li>
<li><strong>Distribution Still Grows</strong>: While iterating helps gather more relevant data, you may still face large drifts in states if the environment is noisy or high-dimensional.</li>
</ul>
<hr />
<h3 id="version-15-model-predictive-control-mpc">Version 1.5: Model Predictive Control (MPC)</h3>
<p><img alt="Model-Based v1.5" src="../../assets/images/figures/model-based/v15.png" /></p>
<h4 id="core-steps_2">Core Steps</h4>
<ol>
<li><strong>Short-Horizon Re-Planning at Each Step</strong></li>
<li>
<p>At each time <span class="arithmatex">\(t\)</span>:</p>
<ol>
<li>Observe the real current state <span class="arithmatex">\( s_t \)</span>.</li>
<li>Plan a short sequence of actions <span class="arithmatex">\(\{a_t, \dots, a_{t+H-1}\}\)</span> with the learned model.</li>
<li>Execute only the <strong>first</strong> action <span class="arithmatex">\( a_t \)</span>.</li>
<li>Observe the next real state <span class="arithmatex">\( s_{t+1} \)</span>.</li>
<li>Re-plan from <span class="arithmatex">\( s_{t+1} \)</span>.</li>
</ol>
</li>
<li>
<p><strong>Closed-Loop Control</strong></p>
</li>
<li>
<p>Frequent re-planning reduces the impact of model errors because the system constantly “checks in” with reality.</p>
</li>
<li>
<p><strong>Iterative Model Updates (Optional)</strong></p>
</li>
<li>As in Version 1.0, you can continue to collect data and update the model periodically.</li>
</ol>
<h4 id="shortcomings_2">Shortcomings</h4>
<ul>
<li><strong>High Computational Cost</strong>: Planning at every time step can be expensive, especially in high-dimensional or time-critical domains.  </li>
<li><strong>Still Requires a Good Model</strong>: Significant model inaccuracies can still cause erroneous plans, though re-planning mitigates compounding errors.</li>
</ul>
<h4 id="why-move-on_2">Why Move On?</h4>
<ul>
<li><strong>Continuous Planning Overhead</strong>: MPC may be infeasible when real-time constraints or massive action spaces make on-the-fly planning too slow.  </li>
<li><strong>Desire for a Learned Policy</strong>: A direct policy could yield near-instant action selection at runtime, motivating Version 2.0.</li>
</ul>
<hr />
<h3 id="version-20-backprop-through-the-learned-model">Version 2.0: Backprop Through the Learned Model</h3>
<p><img alt="Model-Based v2.0" src="../../assets/images/figures/model-based/v20.png" /></p>
<h4 id="core-steps_3">Core Steps</h4>
<ol>
<li><strong>Differentiable Dynamics Model</strong></li>
<li>
<p>Train a neural network or another differentiable function <span class="arithmatex">\( p_\theta(s' \mid s, a) \)</span>.</p>
</li>
<li>
<p><strong>End-to-End Policy Optimization</strong></p>
</li>
<li>Unroll the model over multiple timesteps, applying the policy <span class="arithmatex">\(\pi_\phi\)</span> to get actions, and accumulate predicted rewards.</li>
<li>Backpropagate through the learned model to update policy parameters <span class="arithmatex">\(\phi\)</span>.</li>
<li>
<p>Once trained, the resulting policy can be deployed directly—no online planning is needed.</p>
</li>
<li>
<p><strong>High Efficiency at Deployment</strong></p>
</li>
<li>Action selection is a simple forward pass of the policy network, suitable for time-critical or large-scale applications.</li>
</ol>
<h4 id="shortcomings_3">Shortcomings</h4>
<ul>
<li><strong>Numerical Instability</strong>: Backpropagating through many timesteps can lead to exploding or vanishing gradients.  </li>
<li><strong>Model Exploitation</strong>: The policy can “exploit” any inaccuracies in the model, especially over long horizons or in stochastic environments.  </li>
<li><strong>Careful Regularization</strong>: Shorter horizon unrolls, ensembles, or uncertainty estimation are often used to keep policy learning stable and robust.</li>
</ul>
<h4 id="why-move-on_3">Why Move On?</h4>
<p>Well, <strong>Version 2.0</strong> is often seen as an “end goal” rather than a stepping stone—because once you have a learned policy that requires no online planning, you enjoy high-speed inference. However:
- <strong>Complexity and Instability</strong>: Real-world tasks may need a mix of methods (e.g., partial MPC, ensembles) to handle uncertainty and prevent exploitation of model errors.</p>
<hr />
<h3 id="reward-overshooting-overoptimistic-planning">Reward Overshooting (Overoptimistic Planning)</h3>
<h4 id="what-it-is">What It Is</h4>
<ul>
<li><strong>Definition</strong>: When a planner or policy exploits incorrect or extrapolated high reward predictions from the learned model, leading to unrealistic or unsafe behavior in the real environment.</li>
</ul>
<h4 id="why-it-happens">Why It Happens</h4>
<ul>
<li><strong>Sparse Coverage</strong>: The model has little data in certain regions, so it overestimates rewards there.  </li>
<li><strong>Open-Loop Plans</strong>: Versions 0.5 and 1.0 may chase these “fantasy” states for an entire rollout before correcting in the next iteration.  </li>
<li><strong>Uncertainty Blindness</strong>: If the approach doesn’t penalize states with high model uncertainty, the planner may favor them solely because the model “thinks” they are high-reward.</li>
</ul>
<h4 id="consequences">Consequences</h4>
<ul>
<li><strong>Poor Real-World Transfer</strong>: The agent’s performance can appear great under the learned model but fail in actual interaction.  </li>
<li><strong>Safety Violations</strong>: In real-world robotics or critical applications, overshooting can lead to dangerous actions.</li>
</ul>
<h4 id="mitigation-strategies">Mitigation Strategies</h4>
<ul>
<li><strong>Frequent Re-Planning (MPC)</strong>: Correct for erroneous predictions step by step.  </li>
<li><strong>Iterative Data Collection</strong>: Gradually gather real data in uncertain regions.  </li>
<li><strong>Uncertainty-Aware Models</strong>: Ensembles or Bayesian approaches that identify high-uncertainty states and penalize them.  </li>
<li><strong>Regularization</strong>: Shorter horizon rollouts, trust regions, or other constraints limit destructive exploitation of model errors.</li>
</ul>
<hr />
<h4 id="other-challenges-and-notes">Other Challenges and Notes</h4>
<ol>
<li>
<p><strong>Reward Function Misspecification.</strong> If the reward function itself is imperfect or learned, it can exacerbate overshooting or produce unintended behaviors.</p>
</li>
<li>
<p><strong>Stochastic Environments.</strong> Open-loop methods (Version 0.5, 1.0) can fail if they don’t adapt in real time. MPC (Version 1.5) or robust policy optimization (Version 2.0) are better at handling randomness.</p>
</li>
<li>
<p><strong>Exploding/Vanishing Gradients.</strong> A big challenge for Version 2.0 when unrolling many timesteps through a neural model.</p>
</li>
<li>
<p><strong>Safety Concerns.</strong> In physical or high-stakes domains, any form of model inaccuracy can be dangerous. MPC is often the pragmatic choice in safety-critical tasks.</p>
</li>
<li>
<p><strong>Computational Trade-Offs.</strong> MPC (Version 1.5) can be expensive online. End-to-end policy learning (Version 2.0) moves the heavy lifting offline, but training is more delicate.</p>
</li>
</ol>
<hr />
<h2 id="5-dynamics-model-learning">5. Dynamics Model Learning</h2>
<p>Learning a model <span class="arithmatex">\(\hat{P}(s_{t+1}\mid s_t,a_t)\)</span> + <span class="arithmatex">\(\hat{R}(s_t,a_t)\)</span> is often done via <strong>supervised learning</strong> on transitions <span class="arithmatex">\((s_t,a_t,s_{t+1},r_t)\)</span>. This section surveys advanced considerations from Moerland et al. .</p>
<h3 id="51-basic-considerations">5.1 Basic Considerations</h3>
<ol>
<li>
<p><strong>Type of Model</strong>  </p>
<ul>
<li><strong>Forward</strong> (most common): <span class="arithmatex">\((s_t,a_t)\mapsto s_{t+1}\)</span>.  </li>
<li><strong>Backward</strong> (reverse model): <span class="arithmatex">\(s_{t+1}\mapsto (s_t,a_t)\)</span>. Used in prioritized sweeping.  </li>
<li><strong>Inverse</strong>: <span class="arithmatex">\((s_t, s_{t+1}) \mapsto a_t\)</span>. Sometimes used in representation learning or feedback control.</li>
</ul>
</li>
<li>
<p><strong>Estimation Method</strong>  </p>
<ul>
<li><strong>Parametric</strong> (e.g., neural networks, linear regressors, GPs).  </li>
<li><strong>Non-parametric</strong> (e.g., nearest neighbors, kernel methods).  </li>
<li><strong>Exact</strong> (tabular) vs. <strong>approximate</strong> (function approximation).</li>
</ul>
</li>
<li>
<p><strong>Region of Validity</strong>  </p>
<ul>
<li><strong>Global</strong> model: Attempt to capture all states. Common in large-scale MBRL.  </li>
<li><strong>Local</strong> model: Fit only around the current trajectory or region of interest (common in robotics, e.g., local linearization).</li>
</ul>
</li>
</ol>
<p><img alt="Overview of different types of mappings in model learning" src="../../assets/images/figures/model-based/dyn-base.png" />
<em>Overview of different types of mappings in model learning. </em><em>1)</em><em> Standard Markovian transition model <span class="arithmatex">\( s_t, a_t \rightarrow s_{t+1} \)</span>. </em><em>2)</em><em> Partial observability. We model <span class="arithmatex">\( s_0 \ldots s_t, a_t \rightarrow s_{t+1} \)</span>, leveraging the state history to make an accurate prediction. </em><em>3)</em><em> Multi-step prediction (Section 4.6), where we model <span class="arithmatex">\( s_t, a_t \ldots a_{t+n-1} \rightarrow s_{t+n} \)</span>, to predict the <span class="arithmatex">\( n \)</span> step effect of a sequence of actions. </em><em>4)</em><em> State abstraction, where we compress the state into a compact representation <span class="arithmatex">\( z_t \)</span> and model the transition in this latent space. </em><em>5)</em><em> Temporal/action abstraction, better known as hierarchical reinforcement learning, where we learn an abstract action <span class="arithmatex">\( u_t \)</span> that brings us to <span class="arithmatex">\( s_{t+n} \)</span>. Temporal abstraction directly implies multi-step prediction, as otherwise the abstract action <span class="arithmatex">\( u_t \)</span> is equal to the low level action <span class="arithmatex">\( a_t \)</span>. All the above ideas (</em><em>2–5</em><em>) are orthogonal and can be combined.</em></p>
<hr />
<h3 id="52-stochasticity">5.2 Stochasticity</h3>
<p>Real MDPs can be <strong>stochastic</strong>, meaning that the environment transition from <span class="arithmatex">\((s_t, a_t)\)</span> to <span class="arithmatex">\(s_{t+1}\)</span> is governed by a <em>distribution</em>:</p>
<div class="arithmatex">\[
P\bigl(s_{t+1} \mid s_t, a_t \bigr).
\]</div>
<p>Unlike a deterministic setting (where we might write <span class="arithmatex">\(s_{t+1} = f(s_t, a_t)\)</span>), this transition function yields a probability distribution over all possible next states rather than a single outcome.</p>
<hr />
<h4 id="521-multi-modal-transitions-and-the-conditional-mean-problem">5.2.1 Multi-Modal Transitions and the Conditional Mean Problem</h4>
<p>When training a purely deterministic network (e.g., a standard neural network with mean-squared error, MSE) to predict <span class="arithmatex">\(s_{t+1}\)</span> from <span class="arithmatex">\((s_t, a_t)\)</span>, the model typically learns the <strong>conditional mean</strong> of the next-state distribution. This can be problematic if the true transition distribution is multi-modal, since the mean might not align with any <em>actual</em> or <em>likely</em> realization of the environment.</p>
<p><img alt="Simple diagram showing multi-modal distribution and how MSE yields the mean." src="../../assets/images/figures/model-based/multimodal-mse.png" /></p>
<p><em>Illustration of stochastic transition dynamics. </em><em>Left</em><em>: 500 samples from an example transition function <span class="arithmatex">\(P(s_{t+1} \mid s, a)\)</span>. The vertical dashed line indicates the cross-section distribution on the right. </em><em>Right</em><em>: distribution of <span class="arithmatex">\(s_{t+1}\)</span> for a particular <span class="arithmatex">\((s, a)\)</span>. We observe a multimodal distribution. The conditional mean of this distribution, which would be predicted by MSE training, is shown as a vertical line.</em></p>
<p>Formally, if the true next state is a random variable <span class="arithmatex">\(S_{t+1}\)</span>, then MSE-based regression gives</p>
<div class="arithmatex">\[
\hat{s}_{t+1} \;=\; \mathbb{E}\bigl[S_{t+1}\,\big|\,(s_t,a_t)\bigr].
\]</div>
<p>Hence, if <span class="arithmatex">\(S_{t+1}\)</span> can take on several distinct modes with similar probabilities, a single mean prediction <span class="arithmatex">\(\hat{s}_{t+1}\)</span> may not capture the actual modes at all.</p>
<hr />
<h4 id="522-descriptive-distribution-models">5.2.2 Descriptive (Distribution) Models</h4>
<p>To capture multi-modal dynamics rigorously, one can represent the full <em>distribution</em> <span class="arithmatex">\(P(s_{t+1}\mid s_t,a_t)\)</span>. Common choices include:</p>
<ol>
<li>
<p><strong>Gaussian Distribution</strong><br />
    Assume</p>
<div class="arithmatex">\[
s_{t+1} \;\sim\; \mathcal{N}\bigl(\mu_\theta(s_t,a_t),\,\Sigma_\theta(s_t,a_t)\bigr),
\]</div>
<p>where <span class="arithmatex">\(\theta\)</span> denotes model parameters. Typically trained by maximizing log-likelihood of observed transitions.</p>
</li>
<li>
<p><strong>Gaussian Mixture Models (GMM)</strong><br />
    Use a sum of <span class="arithmatex">\(K\)</span> Gaussians:</p>
<div class="arithmatex">\[
s_{t+1} \;\sim\; \sum_{k=1}^{K}\;\alpha_k(\theta; s_t,a_t)\;\mathcal{N}\!\Bigl(\mu_k,\;\Sigma_k\Bigr).
\]</div>
<p>The mixture weights <span class="arithmatex">\(\alpha_k\)</span> sum to 1. This better captures multi-modality than a single Gaussian but can be more complex to train (e.g., via EM).</p>
</li>
<li>
<p><strong>Tabular/Histogram-Based</strong><br />
    For lower-dimensional or discrete states:</p>
<div class="arithmatex">\[
\hat{P}\bigl(s' \mid s,a\bigr) \;=\; \frac{n(s,a,s')}{\sum_{\tilde{s}}\,n(s,a,\tilde{s})},
\]</div>
<p>where <span class="arithmatex">\(n(\cdot)\)</span> counts observed transitions. This is often infeasible in large or continuous domains.</p>
</li>
</ol>
<hr />
<h4 id="523-generative-approaches">5.2.3 Generative Approaches</h4>
<p>Instead of closed-form probability distributions, one might learn a <strong>generative</strong> mapping that <em>samples</em> from <span class="arithmatex">\(P(s_{t+1}\mid s_t,a_t)\)</span>. Examples:</p>
<ol>
<li>
<p><strong>Variational Autoencoders (VAEs)</strong><br />
    Introduce a latent variable <span class="arithmatex">\(\mathbf{z}\)</span>. Then</p>
<div class="arithmatex">\[
    s_{t+1} \;=\; f_\theta\!\bigl(\mathbf{z},\,s_t,\,a_t\bigr), 
    \quad 
    \mathbf{z}\;\sim\;\mathcal{N}(\mathbf{0},\mathbf{I}),
\]</div>
<p>and fit <span class="arithmatex">\(\theta\)</span> via variational inference. Inference-time sampling of <span class="arithmatex">\(\mathbf{z}\)</span> yields diverse future states.</p>
</li>
<li>
<p><strong>Normalizing Flows</strong><br />
    Transform a simple base distribution (like a Gaussian) through a stack of invertible mappings <span class="arithmatex">\(\{f_\theta^{(\ell)}\}\)</span>:</p>
<div class="arithmatex">\[
\mathbf{z}\,\sim\,\mathcal{N}(\mathbf{0},\mathbf{I}), 
\quad
s_{t+1} \;=\; (f_\theta^{(L)} \circ \cdots \circ f_\theta^{(1)})(\mathbf{z}).
\]</div>
<p>Optimized via maximum likelihood, enabling expressive densities.</p>
</li>
<li>
<p><strong>Generative Adversarial Networks (GANs)</strong><br />
    A discriminator <span class="arithmatex">\(D\)</span> distinguishes real vs. generated next states, while the generator <span class="arithmatex">\(G\)</span> attempts to fool <span class="arithmatex">\(D\)</span>. Though flexible, GAN training can be unstable or prone to mode collapse.</p>
</li>
<li>
<p><strong>Autoregressive Models</strong><br />
    Factorize high-dimensional <span class="arithmatex">\(s_{t+1}\)</span> into a chain of conditionals. Useful for image-based transitions but can be computationally heavy.</p>
</li>
</ol>
<hr />
<h4 id="524-training-objectives">5.2.4 Training Objectives</h4>
<p>Most distribution models are trained by <em>maximizing likelihood</em> or <em>minimizing negative log-likelihood</em> over a dataset <span class="arithmatex">\(\{(s_t^{(i)}, a_t^{(i)}, s_{t+1}^{(i)})\}\)</span>. For example:</p>
<div class="arithmatex">\[
    \theta^* 
    = 
    \arg\max_\theta 
    \sum_{i=1}^N 
    \log P\bigl(s_{t+1}^{(i)} \mid s_t^{(i)},a_t^{(i)};\,\theta \bigr)
    -\;
    \Omega(\theta),
\]</div>
<p>where <span class="arithmatex">\(\Omega(\theta)\)</span> might be a regularization term. GAN-based models use a min-max objective, while VAE-based methods use an ELBO that includes a reconstruction term and a KL prior penalty on the latent space.</p>
<hr />
<h4 id="525-practical-considerations-and-challenges">5.2.5 Practical Considerations and Challenges</h4>
<ul>
<li>
<p><strong>Divergence in Multi-Step Rollouts</strong><br />
  Even with a stochastic model, errors can accumulate as predictions are fed back into the model. Mitigations include unrolling during training, multi-step loss functions, or specialized architectures.</p>
</li>
<li>
<p><strong>Mode Collapse / Rare Transitions</strong><br />
  Multi-modal distributions can be hard to learn in practice. Models must capture <em>all</em> critical modes, especially for safety or robotics contexts where minority transitions may be crucial.</p>
</li>
<li>
<p><strong>High-Dimensional Observations</strong><br />
  Image-based tasks often leverage latent-variable models (e.g., VAE-like) to reduce dimensionality. Encoding <span class="arithmatex">\(\rightarrow\)</span> (predict in latent space) <span class="arithmatex">\(\rightarrow\)</span> decoding is common.</p>
</li>
<li>
<p><strong>Expressiveness vs. Efficiency</strong><br />
  Complex generative models (e.g., large mixtures or flows) capture stochasticity better but are often slower to train and evaluate. Many real-world agents resort to simpler unimodal Gaussians, balancing speed and accuracy.</p>
</li>
</ul>
<hr />
<h4 id="526-example-gaussian-transitions-via-maximum-likelihood">5.2.6 Example: Gaussian Transitions via Maximum Likelihood</h4>
<p>A common assumption uses a unimodal Gaussian:</p>
<div class="arithmatex">\[
s_{t+1} \;\sim\; \mathcal{N}\Bigl(\mu_\theta(s_t,a_t),\;\Sigma_\theta(s_t,a_t)\Bigr).
\]</div>
<p>Assume diagonal covariance <span class="arithmatex">\(\Sigma_\theta=\mathrm{diag}(\sigma_1^2,\dots,\sigma_d^2)\)</span>. The log-likelihood for one observed transition is:</p>
<div class="arithmatex">\[
\log p\bigl(s_{t+1}\mid s_t,a_t;\,\theta\bigr)
\,=\,
\sum_{j=1}^d 
\Bigl[
-\tfrac12\,\ln\bigl(2\pi\,\sigma_j^2(\cdot)\bigr)
-\;
\tfrac{\bigl(s_{t+1}[j]-\mu_j(\cdot)\bigr)^2}{2\,\sigma_j^2(\cdot)}
\Bigr].
\]</div>
<p>Maximizing this finds a Gaussian best-fit to the empirical data. While unimodal, it remains a popular, tractable choice for continuous control.</p>
<hr />
<h4 id="527-concluding-remarks-on-stochastic-transitions">5.2.7 Concluding Remarks on Stochastic Transitions</h4>
<ul>
<li>
<p><strong>Why Model Stochasticity?</strong><br />
  Real-world dynamics often have multiple plausible next states. Failing to capture these can produce inaccurate planning and suboptimal policies.</p>
</li>
<li>
<p><strong>Descriptive vs. Generative</strong><br />
  Some tasks demand full density estimation (e.g., risk-sensitive planning), while others only require sampling plausible transitions (e.g., for Monte Carlo rollouts).</p>
</li>
<li>
<p><strong>Integration with RL</strong><br />
  Once a model is learned, the agent can plan by either <em>averaging</em> over transitions or <em>sampling</em> them. Handling many branching futures can be computationally expensive; practical approaches limit the depth or use heuristic expansions.</p>
</li>
</ul>
<p>In sum, real-world MDPs often present <strong>multi-modal</strong> and <strong>stochastic</strong> dynamics. A purely deterministic predictor trained via MSE collapses the distribution to a single mean. Instead, we can use <strong>distribution</strong> (e.g., GMM) or <strong>generative</strong> (e.g., VAE, flows, GANs) approaches, trained via maximum likelihood, adversarial losses, or variational inference. Proper handling of stochastic transitions is essential for robust planning, policy optimization, and multi-step simulation in model-based RL.</p>
<hr />
<h3 id="53-uncertainty">5.3 Uncertainty</h3>
<p>A critical challenge in MBRL is <strong>model uncertainty</strong>—the model is learned from limited data, so predictions can be unreliable in unfamiliar state-action regions. We distinguish:</p>
<ul>
<li><strong>Aleatoric (intrinsic) uncertainty</strong>: inherent stochasticity in transitions.  </li>
<li><strong>Epistemic (model) uncertainty</strong>: arises from limited training data. This can, in principle, be reduced by gathering more data.</li>
</ul>
<p>A rigorous approach is to <strong>maintain a distribution over possible models</strong>, then plan by integrating or sampling from that distribution to avoid catastrophic exploitation of untrusted model regions.</p>
<h4 id="bayesian-neural-networks">Bayesian Neural Networks</h4>
<p>One approach is a <strong>Bayesian neural network (BNN)</strong>:</p>
<div class="arithmatex">\[
\theta \sim p(\theta), \quad s_{t+1} \sim P_\theta(\cdot \mid s_t, a_t).
\]</div>
<p>We keep a posterior <span class="arithmatex">\(p(\theta\mid D)\)</span> over network weights <span class="arithmatex">\(\theta\)</span> given dataset <span class="arithmatex">\(D\)</span>. Predictive distribution for the next state is then:</p>
<div class="arithmatex">\[
p(s_{t+1}\mid s_t,a_t, D) = \int P_\theta(s_{t+1}\mid s_t,a_t)\,p(\theta\mid D)\,d\theta.
\]</div>
<p>In practice, approximations like <strong>variational dropout</strong> or <strong>Laplace approximation</strong> are used to sample from <span class="arithmatex">\(p(\theta)\)</span>.</p>
<h4 id="ensembles-and-bootstrapping">Ensembles and Bootstrapping</h4>
<p>Another popular method is an <strong>ensemble</strong> of <span class="arithmatex">\(N\)</span> models <span class="arithmatex">\(\{\hat{P}_{\theta_i}\}\)</span>. Each model is trained on a bootstrapped subset of the data (or with different initialization seeds). The variance across predictions:</p>
<div class="arithmatex">\[
\mathrm{Var}\bigl[\hat{P}_{\theta_i}(s_{t+1}\mid s_t,a_t)\bigr]
\]</div>
<p>indicates <strong>epistemic</strong> uncertainty. In practice:</p>
<div class="arithmatex">\[
\hat{\mu}_\mathrm{ensemble}(s_{t+1}) \approx \frac{1}{N}\sum_{i=1}^N \hat{\mu}_i(s_{t+1}),
\quad
\hat{\Sigma}_\mathrm{ensemble}(s_{t+1}) \approx \frac{1}{N}\sum_{i=1}^N \bigl(\hat{\mu}_i - \hat{\mu}_\mathrm{ensemble}\bigr)^2.
\]</div>
<p>During planning, one may:</p>
<ol>
<li>Sample one model from the ensemble at each step (like <strong>PETS</strong>).</li>
<li>Average the predictions or treat it as a Gaussian mixture model.</li>
</ol>
<p>Either way, uncertain regions typically manifest as a large disagreement among ensemble members, warning the planner not to trust that zone.</p>
<p><img alt="ensemble-based spread for uncertain states." src="../../assets/images/figures/model-based/bootstrap.png" /></p>
<p><strong>Mathematically</strong>: if the “true” dynamics distribution is <span class="arithmatex">\(P^\star\)</span> and each model <span class="arithmatex">\(\hat{P}_{\theta_i}\)</span> is an unbiased estimator, then large variance across <span class="arithmatex">\(\{\hat{P}_{\theta_i}\}\)</span> signals a region outside the training distribution. Minimizing that variance can guide exploration or help shape conservative planning.</p>
<hr />
<h3 id="54-partial-observability">5.4 Partial Observability</h3>
<p>Sometimes the environment is not fully observable. We can’t identify the full state <span class="arithmatex">\(s\)</span> from a single observation <span class="arithmatex">\(o\)</span>. Solutions:</p>
<ul>
<li><strong>Windowing</strong>: Keep last <span class="arithmatex">\(n\)</span> observations <span class="arithmatex">\((o_t, o_{t-1}, \dots)\)</span>.  </li>
<li><strong>Belief state</strong>: Use a hidden Markov model or Bayesian filter.  </li>
<li><strong>Recurrence</strong>: Use RNNs/LSTMs that carry hidden state <span class="arithmatex">\(\mathbf{h}_t\)</span>.  </li>
<li><strong>External Memory</strong>: Neural Turing Machines, etc., for long-range dependencies.</li>
</ul>
<hr />
<h3 id="55-non-stationarity">5.5 Non-Stationarity</h3>
<p><strong>Non-stationary</strong> dynamics means that <span class="arithmatex">\(P\)</span> or <span class="arithmatex">\(R\)</span> changes over time. A single learned model can become stale. Approaches include:</p>
<ul>
<li><strong>Partial models</strong> [Doya et al., 2002]: Maintain multiple submodels for different regimes, detect changes in transition error.  </li>
<li><strong>High learning-rate</strong> or <strong>forgetting</strong> older data to adapt quickly.</li>
</ul>
<hr />
<h3 id="56-multi-step-prediction">5.6 Multi-Step Prediction</h3>
<p>One-step predictions can accumulate error when rolled out repeatedly. Some solutions:</p>
<ul>
<li><strong>Train for multi-step</strong>: Unroll predictions for <span class="arithmatex">\(k\)</span> steps and backprop against ground truth <span class="arithmatex">\((s_{t+k})\)</span>.  </li>
<li><strong>Dedicated multi-step models</strong>: Instead of chaining one-step predictions, learn <span class="arithmatex">\(f^{(k)}(s_t,a_{t:k-1})\approx s_{t+k}\)</span>.</li>
</ul>
<p><img alt="The motivation for learning dynamics" src="../../assets/images/figures/model-based/why_learn_model.png" /></p>
<hr />
<h3 id="57-state-abstraction">5.7 State Abstraction</h3>
<p>In many domains, the raw observation space <span class="arithmatex">\(s\)</span> can be very high-dimensional (e.g., pixel arrays), making direct modeling of <span class="arithmatex">\((s_t, a_t) \mapsto s_{t+1}\)</span> intractable. <strong>State abstraction</strong> (or <strong>representation learning</strong>) tackles this issue by learning a more compact latent state <span class="arithmatex">\(\mathbf{z}_t\)</span>, capturing the essential factors of variation. Once in this latent space, the model learns transitions <span class="arithmatex">\(\mathbf{z}_{t+1} = f_{\text{trans}}(\mathbf{z}_t, a_t)\)</span>, and can <strong>decode</strong> back to the original space if needed:</p>
<div class="arithmatex">\[
\mathbf{z}_t = f_{\text{enc}}(s_t), 
\quad
\mathbf{z}_{t+1} = f_{\text{trans}}(\mathbf{z}_t, a_t), 
\quad
s_{t+1} \approx f_{\text{dec}}(\mathbf{z}_{t+1}).
\]</div>
<p>This structure reduces modeling complexity and can enable more efficient planning or control in the latent domain.</p>
<h4 id="571-common-approaches-to-representation-learning">5.7.1 Common Approaches to Representation Learning</h4>
<ol>
<li>
<p><strong>Autoencoders and Variational Autoencoders (VAEs)</strong><br />
    An autoencoder aims to learn an encoding <span class="arithmatex">\(\mathbf{z}=f_{\text{enc}}(s)\)</span> that, when passed through a decoder <span class="arithmatex">\(f_{\text{dec}}\)</span>, reconstructs the original state <span class="arithmatex">\(s\)</span>. In <strong>variational</strong> autoencoders (VAEs), one imposes a prior distribution <span class="arithmatex">\(p(\mathbf{z})\)</span> (often Gaussian) over the latent space, adding a Kullback–Leibler (KL) divergence penalty:</p>
<div class="arithmatex">\[
\max_{\theta,\phi} 
\;\;
\mathbb{E}_{q_\phi(\mathbf{z}\mid s)}
\Bigl[\log p_\theta(s\mid \mathbf{z})\Bigr]
\;-\;
D_{\mathrm{KL}}\bigl(q_\phi(\mathbf{z}\mid s)\,\|\,p(\mathbf{z})\bigr),
\]</div>
<p>where <span class="arithmatex">\(q_\phi(\mathbf{z}\mid s)\)</span> is the <em>encoder</em> distribution and <span class="arithmatex">\(p_\theta(s\mid \mathbf{z})\)</span> is the <em>decoder</em>. This ensures the learned latent code <span class="arithmatex">\(\mathbf{z}\)</span> both reconstructs well and remains “organized” under the prior <span class="arithmatex">\(p(\mathbf{z})\)</span>. Once learned, a <strong>latent dynamics</strong> model <span class="arithmatex">\(\mathbf{z}_{t+1}=f_{\text{trans}}(\mathbf{z}_t,a_t)\)</span> can be fitted by minimizing a prediction loss (e.g., mean-squared error on <span class="arithmatex">\(\mathbf{z}\)</span>-space).</p>
</li>
<li>
<p><strong>Object-Based Approaches</strong><br />
    For environments that can be factorized into distinct <strong>objects</strong> (e.g., multiple physical entities), <strong>Graph Neural Networks (GNNs)</strong> or <strong>object-centric</strong> models can more naturally capture the underlying structure. Concretely, each latent node <span class="arithmatex">\(\mathbf{z}_i\)</span> corresponds to an object’s state (e.g., location, velocity, shape), and edges model interactions among objects. Formally,</p>
<div class="arithmatex">\[
\mathbf{z}_{t+1}^{(i)} 
\;=\;
f_{\text{trans}}^{(i)}
\Bigl(
    \mathbf{z}_t^{(i)},\,a_t,\,
    \{\mathbf{z}_t^{(j)}\}_{j \in \mathcal{N}(i)}
\Bigr),
\]</div>
<p>where <span class="arithmatex">\(\mathcal{N}(i)\)</span> denotes neighbors of object <span class="arithmatex">\(i\)</span>. This is particularly effective in physics-based settings [Battaglia et al., 2016], allowing each object’s transition to depend primarily on relevant neighbors (e.g., collisions). Such structured representations often facilitate <strong>better generalization</strong> to new configurations (e.g., changing the number or arrangement of objects).</p>
</li>
<li>
<p><strong>Contrastive Losses for Semantic/Controllable Features</strong><br />
    Sometimes, a purely reconstruction-based loss can over-focus on visually salient but decision-irrelevant details. <strong>Contrastive</strong> methods use pairs (or triplets) of observations to emphasize meaningful relationships. For instance, if two states <span class="arithmatex">\(s\)</span> and <span class="arithmatex">\(s'\)</span> are known to be <em>dynamically close</em> (reachable with few actions), one encourages their embeddings <span class="arithmatex">\(\mathbf{z}\)</span> and <span class="arithmatex">\(\mathbf{z}'\)</span> to be close under some metric. Formally, a <strong>contrastive loss</strong> might look like:</p>
<div class="arithmatex">\[
\ell_{\mathrm{contrast}}(\mathbf{z}, \mathbf{z}') 
\;=\;
y\,\|\mathbf{z}-\mathbf{z}'\|^2 
\;+\;
(1-y)\,\bigl[\alpha - \|\mathbf{z}-\mathbf{z}'\|\bigr]_{+},
\]</div>
<p>where <span class="arithmatex">\(y=1\)</span> if the states should be similar, and <span class="arithmatex">\(y=0\)</span> otherwise, and <span class="arithmatex">\(\alpha\)</span> is a margin. Examples include time-contrastive approaches [Sermanet et al., 2018] that bring together frames from different camera angles but the <em>same</em> physical scene, or goal-oriented distances [Ghosh et al., 2018]. These tasks guide the learned <span class="arithmatex">\(\mathbf{z}\)</span>-space to capture features that matter for control, rather than trivial background details.</p>
</li>
</ol>
<hr />
<h4 id="572-planning-in-latent-space">5.7.2 Planning in Latent Space</h4>
<p>Once a latent representation is established, an agent can:</p>
<ol>
<li>
<p><strong>Plan directly in <span class="arithmatex">\(\mathbf{z}\)</span>-space</strong><br />
   For instance, run a forward-search or gradient-based policy optimization with states <span class="arithmatex">\(\mathbf{z}\)</span> and transitions <span class="arithmatex">\(f_{\text{trans}}(\mathbf{z}, a)\)</span>. If the decoder <span class="arithmatex">\(f_{\text{dec}}\)</span> is not explicitly needed (e.g., if the agent only needs to output actions), planning in the latent domain can reduce computational overhead and reduce the “curse of dimensionality.”</p>
</li>
<li>
<p><strong>Decode for interpretability or environment feedback</strong><br />
   If the environment requires real-world actions or if interpretability is desired, one can decode predicted latent states <span class="arithmatex">\(\mathbf{z}_{t+1}\)</span> to <span class="arithmatex">\(\hat{s}_{t+1}\)</span>. The environment then checks feasibility or yields a reward. This is especially relevant when the environment is external (like a simulator or the real world) expecting inputs in the original space.</p>
</li>
</ol>
<p>A primary challenge is <strong>rollout mismatch</strong>: if <span class="arithmatex">\(\mathbf{z}_{t+1}\)</span> is never trained to match <span class="arithmatex">\(f_{\text{enc}}(s_{t+1})\)</span>, repeated application of <span class="arithmatex">\(f_{\text{trans}}\)</span> might accumulate errors. Solutions include explicit <strong>consistency constraints</strong> (i.e., <span class="arithmatex">\(\|\mathbf{z}_{t+1} - f_{\text{enc}}(s_{t+1})\|\)</span> penalties) or <strong>probabilistic</strong> latent inference (e.g., Kalman filter variants, sequential VAEs).</p>
<hr />
<h3 id="58-temporal-abstraction">5.8 Temporal Abstraction</h3>
<p>In Markov Decision Processes (MDPs), each action typically spans one environment step. But many tasks have <em>natural subroutines</em> that can be chunked into <strong>higher-level actions</strong>. This is the essence of <strong>hierarchical reinforcement learning (HRL)</strong>: define “macro-actions” that unfold over multiple timesteps, reducing effective planning depth and often improving data efficiency.</p>
<h4 id="581-options-framework">5.8.1 Options Framework</h4>
<p>An <strong>Option</strong> <span class="arithmatex">\(\omega\)</span> is a tuple <span class="arithmatex">\((I_\omega, \pi_\omega, \beta_\omega)\)</span> [Sutton et al., 1999]:</p>
<ul>
<li><strong>Initiation set</strong> <span class="arithmatex">\(I_\omega \subseteq \mathcal{S}\)</span>: states from which <span class="arithmatex">\(\omega\)</span> can be invoked.</li>
<li><strong>Subpolicy</strong> <span class="arithmatex">\(\pi_\omega(a \mid s)\)</span>: governs low-level actions while the option runs.</li>
<li><strong>Termination condition</strong> <span class="arithmatex">\(\beta_\omega(s)\)</span>: probability that <span class="arithmatex">\(\omega\)</span> terminates upon reaching state <span class="arithmatex">\(s\)</span>.</li>
</ul>
<p>When executing option <span class="arithmatex">\(\omega\)</span>, the agent follows <span class="arithmatex">\(\pi_\omega\)</span> until a stochastic termination event triggers, transitioning back to the high-level policy’s control. The high-level policy thus selects from a <em>set</em> of options, each a multi-step “chunk” of actions. This can drastically reduce the horizon of the planning problem.</p>
<p>Mathematically, a <strong>semi-MDP</strong> formalism captures these temporally extended actions, where option <span class="arithmatex">\(\omega\)</span> yields a multi-step transition <span class="arithmatex">\((s, \omega)\mapsto s'\)</span>. One can learn <strong>option-value functions</strong> <span class="arithmatex">\(Q(s,\omega)\)</span> or plan with pseudo-rewards inside each option. In practice, good options can significantly accelerate learning and planning compared to primitive actions.</p>
<h4 id="582-goal-conditioned-policies">5.8.2 Goal-Conditioned Policies</h4>
<p>Alternatively, <strong>goal-conditioned</strong> or <strong>universal</strong> value functions [Schaul et al., 2015] define a function <span class="arithmatex">\(Q(s, a, g)\)</span> that specifies the expected return for taking action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span> while aiming to achieve <em>goal</em> <span class="arithmatex">\(g\)</span>. One can then plan by selecting subgoals:</p>
<div class="arithmatex">\[
g_1, g_2, \dots, g_K,
\]</div>
<p>where each “macro-step” is the agent trying to reach <span class="arithmatex">\(g_i\)</span> from the current state. Feudal RL frameworks [Dayan and Hinton, 1993] similarly treat higher-level “managers” that set subgoals for lower-level “workers.” A learned, <strong>goal-conditioned</strong> subpolicy <span class="arithmatex">\(\pi(a\mid s,g)\)</span> can generalize across different goals <span class="arithmatex">\(g\)</span>, unlike the options approach that typically uses separate subpolicies per option.</p>
<h4 id="583-subgoal-discovery">5.8.3 Subgoal Discovery</h4>
<p>A key research question is how to <strong>discover</strong> effective macro-actions (options) or subgoals. Approaches include:</p>
<ol>
<li>
<p><strong>Graph-based Bottlenecks</strong><br />
   Construct or approximate a graph of states/regions. Identify <strong>bottlenecks</strong> as states that connect densely connected regions. Formally, if a state <span class="arithmatex">\(s\)</span> lies on many shortest paths between subregions of the state space, it can be a strategic “bridge.” Setting it as a subgoal can simplify global planning [Menache et al., 2002].</p>
</li>
<li>
<p><strong>Empowerment / Coverage</strong><br />
   Encourage subpolicies that <em>cover</em> different parts of the state space or yield high controllability. For instance, one might maximize mutual information between subpolicy latent codes and resulting states, ensuring distinct subpolicies lead to distinct outcomes. This fosters diverse skill discovery.</p>
</li>
<li>
<p><strong>End-to-End Learning</strong><br />
   Methods like <strong>Option-Critic</strong> [Bacon et al., 2017] embed option structure into a differentiable policy architecture and optimize for return. The subpolicies and termination functions emerge from gradient-based training, though careful regularization is often needed to avoid degenerate solutions (e.g., a single subpolicy doing everything).</p>
</li>
</ol>
<hr />
<h4 id="584-benefits-of-temporal-abstraction">5.8.4 Benefits of Temporal Abstraction</h4>
<ul>
<li><strong>Reduced Planning Depth</strong><br />
  Since each macro-action can span multiple timesteps, the effective decision horizon shrinks, often simplifying search or dynamic programming.</li>
<li><strong>Transfer and Reuse</strong><br />
  Once discovered, options/subpolicies can be reused in related tasks. If those subroutines correspond to meaningful skills, the agent may quickly adapt to new goals.</li>
<li><strong>Data Efficiency</strong><br />
  Higher-level actions can yield more stable and purposeful exploration, collecting relevant experience faster than random primitive actions.</li>
</ul>
<hr />
<h2 id="6-integration-of-planning-and-learning">6. Integration of Planning and Learning</h2>
<p>With a learned model in hand (or a known one), we combine <strong>planning</strong> and <strong>learning</strong> to optimize a policy <span class="arithmatex">\(\pi\)</span>. We address four major questions:</p>
<ol>
<li><strong>Which state to start planning from?</strong>  </li>
<li><strong>Budget and frequency</strong>: how many real steps vs. planning steps?  </li>
<li><strong>Planning algorithm</strong>: forward search, MCTS, gradient-based, etc.  </li>
<li><strong>Integration</strong>: how planning outputs feed into policy/value updates and final action selection.</li>
</ol>
<p><img alt="Integration of Planning and Learning" src="../../assets/images/figures/model-based/dyn-plan.png" /></p>
<hr />
<h3 id="61-which-state-to-start-planning-from">6.1 Which State to Start Planning From?</h3>
<ul>
<li><strong>Uniform</strong> over all states (like classical dynamic programming).  </li>
<li><strong>Visited</strong> states only (like Dyna [Sutton, 1990], which samples from replay).  </li>
<li><strong>Prioritized</strong> (Prioritized Sweeping [Moore &amp; Atkeson, 1993]) if some states need urgent update.  </li>
<li><strong>Current</strong> state only (common in online MPC or MCTS from the real agent’s state).</li>
</ul>
<hr />
<h3 id="62-planning-budget-vs-real-data-collection">6.2 Planning Budget vs. Real Data Collection</h3>
<p>Two sub-questions:</p>
<ol>
<li>
<p><strong>Frequency</strong>: plan after every environment step, or collect entire episodes first?  </p>
<ul>
<li>Dyna plans after each step (like 100 imaginary updates per real step).  </li>
<li>PILCO [Deisenroth &amp; Rasmussen, 2011] fits a GP model after each episode.</li>
</ul>
</li>
<li>
<p><strong>Budget</strong>: how many model rollouts or expansions per planning cycle?  </p>
<ul>
<li>Dyna might do 100 short rollouts.  </li>
<li>AlphaZero expands a single MCTS iteration by up to 1600 × depth calls.</li>
</ul>
</li>
</ol>
<p>Some methods adaptively adjust planning vs. real data based on model uncertainty [Kalweit &amp; Boedecker, 2017]. The right ratio can significantly affect performance.</p>
<hr />
<h3 id="63-how-to-plan-planning-algorithms">6.3 How to Plan? (Planning Algorithms)</h3>
<p>Broadly:</p>
<ol>
<li>
<p><strong>Discrete</strong> (non-differentiable) search:</p>
<ul>
<li><strong>One-step</strong> lookahead  </li>
<li><strong>Tree search</strong> (MCTS, minimax)  </li>
<li><strong>Forward vs. backward</strong>: e.g., prioritized sweeping uses a reverse model to propagate value changes quickly</li>
</ul>
</li>
<li>
<p><strong>Differential</strong> (gradient-based) planning:</p>
<ul>
<li>Requires a differentiable model <span class="arithmatex">\(\hat{P}\)</span>.  </li>
<li>E.g., iterative LQR, or direct backprop through unrolled transitions (Dreamer).  </li>
<li>Suited for continuous control with smooth dynamics.</li>
</ul>
</li>
<li>
<p><strong>Depth &amp; Breadth</strong> choices:</p>
<ul>
<li>Some do short-horizon expansions (MBPO uses 1–5 step imaginary rollouts).  </li>
<li>Others do deeper expansions if computing resources allow (AlphaZero MCTS).</li>
</ul>
</li>
<li>
<p><strong>Uncertainty handling</strong>:</p>
<ul>
<li>Plan only near states with low model uncertainty or penalize uncertain states.  </li>
<li>Ensemble-based expansions [Chua et al., 2018].</li>
</ul>
</li>
</ol>
<div class="admonition example">
<p class="admonition-title">Cross-Entropy Method (CEM) – Pseudocode</p>
</div>
<pre><code class="language-python">    # Suppose we want to find the best action sequence of length H
    # that maximizes the expected return under our model.

    Initialize distribution params (mean mu, covariance Sigma)
    for iteration in range(N_iterations):
        # 1. Sample K sequences from current distribution
        candidate_sequences = sample_from_gaussian(mu, Sigma, K)

        # 2. Evaluate each sequence's return
        returns = []
        for seq in candidate_sequences:
            returns.append( evaluate_return(seq, model) )

        # 3. Select the top M (elite) sequences
        elite_indices = top_indices(returns, M)
        elites = [candidate_sequences[i] for i in elite_indices]

        # 4. Update mu, Sigma to fit the elites
        mu = mean(elites)
        Sigma = cov(elites)

    # Final distribution reflects the best action sequence
    best_action_seq = mu
    return best_action_seq
</code></pre>
<hr />
<h4 id="631-monte-carlo-tree-search-mcts">6.3.1 Monte Carlo Tree Search (MCTS)</h4>
<p><strong>Monte Carlo Tree Search</strong> is a powerful method for <strong>discrete action</strong> planning—famously used in AlphaGo, AlphaZero, MuZero. Key components:</p>
<ol>
<li>
<p><strong>Tree Representation</strong>  </p>
<ul>
<li>Each node is a state, edges correspond to actions.  </li>
<li>MCTS incrementally expands the search tree from a <strong>root</strong> (the current state).</li>
</ul>
</li>
<li>
<p><strong>Four Steps</strong> commonly described as:</p>
<ol>
<li><strong>Selection</strong>: Repeatedly choose child nodes (actions) from the root, typically via <strong>Upper Confidence Bound</strong> or policy heuristics, until reaching a leaf node.  </li>
<li><strong>Expansion</strong>: If the leaf is not terminal (or at max depth), add one or more child nodes for possible next actions.  </li>
<li><strong>Simulation</strong>: From that new node, simulate a <strong>rollout</strong> (random or policy-driven) until reaching a terminal state or horizon.  </li>
<li><strong>Backpropagation</strong>: Propagate the <strong>return</strong> from the simulation up the tree to update value/statistics at each node.</li>
</ol>
</li>
<li>
<p><strong>Mathematical Form</strong>  </p>
<ul>
<li>Let <span class="arithmatex">\(N(s,a)\)</span> be the number of visits to child action <span class="arithmatex">\(a\)</span> from state <span class="arithmatex">\(s\)</span>.  </li>
<li>Let <span class="arithmatex">\(\hat{Q}(s,a)\)</span> be the estimated action-value from MCTS.  </li>
<li>
<p>UCB selection uses:</p>
<div class="arithmatex">\[
a_\text{select} = \arg\max_{a}\Bigl[\hat{Q}(s,a) + c \sqrt{\frac{\ln \sum_{b} N(s,b)}{N(s,a)}}\Bigr].
\]</div>
<p>(One can also incorporate a learned prior policy <span class="arithmatex">\(\pi_\theta\)</span> to bias exploration.)</p>
</li>
</ul>
</li>
<li>
<p><strong>Planning &amp; Policy Extraction</strong>  </p>
<ul>
<li>After many simulations from the root, MCTS typically normalizes node visits or Q-values to produce a final policy distribution <span class="arithmatex">\(\alpha\)</span>.  </li>
<li>This policy <span class="arithmatex">\(\alpha\)</span> may be used for real action selection, or to train a global policy network (as in AlphaZero).</li>
</ul>
</li>
</ol>
<div class="admonition example">
<p class="admonition-title">MCTS Pseudocode</p>
</div>
<pre><code class="language-vbnet">MCTS(root_state, model, N_simulations)
Initialize the search tree with root_state

for simulation in 1 to N_simulations do
    node ← root of the tree

    # Selection
    while node is fully expanded and node is not terminal do
        action ← select child of node using UCB
        node ← child corresponding to action

    # Expansion
    if node is not terminal then
        expand node using model (generate all children)
        node ← select one of the new children

    # Simulation
    reward ← simulate from node.state using model

    # Backpropagation
    backpropagate reward up the tree from node

# Final decision
Return action from root with highest visit count
</code></pre>
<p><img alt="MCTS: Illustration of selection, expansion, simulation, backprop." src="../../assets/images/figures/model-based/mcts.png" /></p>
<hr />
<h3 id="64-integration-in-the-learning-and-acting-loop">6.4 Integration in the Learning and Acting Loop</h3>
<p><strong>Key integration channels</strong>:</p>
<ol>
<li>
<p><strong>Planning input</strong> from existing policy/value?  </p>
<ul>
<li>E.g., MCTS uses a prior policy to guide expansions.</li>
</ul>
</li>
<li>
<p><strong>Planning output</strong> as a <strong>training target</strong> for the global policy/value?  </p>
<ul>
<li>E.g., <strong>Dyna</strong> uses imaginary transitions to update Q-values.  </li>
<li><strong>AlphaZero</strong> uses MCTS results as a learning target for <span class="arithmatex">\(\pi\)</span> and <span class="arithmatex">\(V\)</span>.</li>
</ul>
</li>
<li>
<p><strong>Action selection</strong> from the planning procedure or from the learned policy?</p>
<ul>
<li>E.g., MPC picks the best action from a planned sequence.  </li>
<li>Or a final learned policy is used if no real-time planning is feasible.</li>
</ul>
</li>
</ol>
<p><img alt="Integration in the Learning and Acting Loop" src="../../assets/images/figures/model-based/integration.png" /></p>
<p>Various combinations exist: some methods rely mostly on the learned policy but refine or correct it with a short replan (MBPO), while others do a full MCTS at every step (MuZero).</p>
<hr />
<h3 id="65-dyna-and-dyna-style-methods">6.5 Dyna and Dyna-Style Methods</h3>
<p>One of the earliest and most influential frameworks for <strong>model-based RL</strong> is <strong>Dyna</strong> [Sutton, 1990]. The key insight is to integrate:</p>
<ul>
<li><strong>Real experience</strong> from the environment (sampled transitions)  </li>
<li><strong>Model learning</strong> from that real data  </li>
<li><strong>Imagined experience</strong> from the learned model to augment the policy/value updates.</li>
</ul>
<div class="admonition example">
<p class="admonition-title">Dyna Pseudocode</p>
</div>
<pre><code class="language-vbnet">Input: α (learning rate), γ (discount factor), ε (exploration rate), 
       n (number of planning steps), num_episodes

Initialize Q(s, a) arbitrarily for all states s ∈ S, actions a ∈ A
Initialize Model as an empty mapping: Model(s, a) → (r, s')

for each episode do
    Initialize state s ← starting state

    while s is not terminal do
        ▸ Action Selection (ε-greedy)
        With probability ε: choose random action a
        Else: choose a ← argmax_a Q(s, a)

        ▸ Real Interaction
        Take action a, observe reward r and next state s'

        ▸ Q-Learning Update
        Q(s, a) ← Q(s, a) + α [ r + γ · max_a' Q(s', a') − Q(s, a) ]

        ▸ Model Update
        Model(s, a) ← (r, s')

        ▸ Planning (n simulated updates)
        for i = 1 to n do
            Randomly select previously seen (ŝ, â)
            (r̂, ŝ') ← Model(ŝ, â)

            Q(ŝ, â) ← Q(ŝ, â) + α [ r̂ + γ · max_a' Q(ŝ', a') − Q(ŝ, â) ]

        ▸ Move to next real state
        s ← s'


</code></pre>
<ol>
<li><strong>Real Interaction</strong>: We pick action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span> using <span class="arithmatex">\(\epsilon\)</span>-greedy w.r.t. <span class="arithmatex">\(Q\)</span>.  </li>
<li><strong>Update Q</strong> from real transition <span class="arithmatex">\((s,a,s',r)\)</span>.  </li>
<li><strong>Update Model</strong>: store or learn to predict <span class="arithmatex">\(\hat{P}(s'\mid s,a)\)</span>, <span class="arithmatex">\(\hat{R}(s,a)\)</span>.  </li>
<li><strong>Imagination (N_planning steps)</strong>: randomly sample a state-action pair from replay or memory, query the model for <span class="arithmatex">\(\hat{s}', \hat{r}\)</span>. Update <span class="arithmatex">\(Q\)</span> with that synthetic transition.</li>
</ol>
<p><img alt="Dyna-Q characteristics" src="../../assets/images/figures/model-based/dyna-q.png" /></p>
<p><strong>Benefits</strong>:
      - Dyna can drastically reduce real environment interactions by effectively <strong>replaying</strong> or generating new transitions from the learned model.<br />
      - Even short rollouts or repeated “one-step planning” from random visited states helps refine Q-values more quickly.</p>
<p><strong>Dyna-Style</strong> in modern deep RL:
      - Many algorithms (e.g., <strong>MBPO</strong>) add short-horizon imaginary transitions to an off-policy buffer.<br />
      - They differ in details: how many model steps, how they sample states for imagination, how they manage uncertainty, etc.</p>
<hr />
<h2 id="7-modern-model-based-rl-algorithms">7. Modern Model-Based RL Algorithms</h2>
<p>Modern model-based RL builds on classical ideas (global/local models, MPC, iterative re-fitting) but incorporates powerful neural representations, uncertainty handling, and integrated planning-learning frameworks. Below are five influential algorithms that illustrate the <strong>state of the art</strong> in contemporary MBRL.</p>
<hr />
<h3 id="71-world-models-ha-schmidhuber-2018">7.1 World Models (Ha &amp; Schmidhuber, 2018)</h3>
<p><strong>Core Idea</strong><br />
Train a <strong>latent generative model</strong> of the environment (specifically from high-dimensional inputs like images), and then learn or optimize a policy entirely <strong>within this learned latent space</strong>—the so-called “dream environment.”</p>
<p><img alt="World Model" src="../../assets/images/figures/model-based/world-model.png" /></p>
<p><strong>Key Components</strong></p>
<ol>
<li>
<p><strong>Variational Autoencoder (VAE)</strong>:  </p>
<ul>
<li>Maps raw observation <span class="arithmatex">\(\mathbf{o}_t\)</span> to a compact latent representation <span class="arithmatex">\(\mathbf{z}_t\)</span>.  </li>
<li><span class="arithmatex">\(\mathbf{z}_t = E_\phi(\mathbf{o}_t)\)</span> where <span class="arithmatex">\(E_\phi\)</span> is the learned encoder.  </li>
<li>Reconstruction loss ensures <span class="arithmatex">\(E_\phi\)</span> and a corresponding decoder <span class="arithmatex">\(D_\phi\)</span> compress and reconstruct images effectively.</li>
</ul>
</li>
<li>
<p><strong>Recurrent Dynamics Model (MDN-RNN)</strong>:  </p>
<ul>
<li>Predicts the next latent <span class="arithmatex">\(\mathbf{z}_{t+1}\)</span> given <span class="arithmatex">\(\mathbf{z}_t\)</span> and action <span class="arithmatex">\(a_t\)</span>.  </li>
<li>
<p>Often parameterized as a <strong>Mixture Density Network</strong> inside an RNN:  </p>
<div class="arithmatex">\[
  \mathbf{z}_{t+1} \sim p_\theta(\mathbf{z}_{t+1} \mid \mathbf{z}_t, a_t).
\]</div>
</li>
<li>
<p>This distribution can be modeled by a mixture of Gaussians, providing a probabilistic estimate of the next latent state.</p>
</li>
</ul>
</li>
<li>
<p><strong>Controller (Policy)</strong>:  </p>
<ul>
<li>A small neural network <span class="arithmatex">\(\pi_\eta\)</span> that outputs actions <span class="arithmatex">\(a_t = \pi_\eta(\mathbf{z}_t)\)</span> in the latent space.  </li>
<li>Trained (in the original paper) via an evolutionary strategy (e.g., CMA-ES) <em>entirely in the dream world</em>.  </li>
</ul>
</li>
</ol>
<p><strong>Algorithmic Flow</strong></p>
<ol>
<li><strong>Unsupervised Phase</strong>: Run a random or exploratory policy in the real environment, collect observations <span class="arithmatex">\(\mathbf{o}_1, \mathbf{o}_2, ...\)</span>.  </li>
<li><strong>Train the VAE</strong> to learn <span class="arithmatex">\(\mathbf{z} = E_\phi(\mathbf{o})\)</span>.  </li>
<li><strong>Train the MDN-RNN</strong> on sequences <span class="arithmatex">\((\mathbf{z}_t, a_t, \mathbf{z}_{t+1})\)</span>.  </li>
<li><strong>“Dream”</strong>: Roll out the MDN-RNN from random latents and evaluate candidate controllers <span class="arithmatex">\(\pi_\eta\)</span>.  </li>
<li><strong>Update <span class="arithmatex">\(\pi_\eta\)</span></strong> based on the dream performance (e.g., via evolutionary search).</li>
</ol>
<p><strong>Significance</strong></p>
<ul>
<li>Demonstrated that an agent can learn a <strong>world model</strong> of high-dimensional environments (CarRacing, VizDoom) and train policies in “latent imagination.”  </li>
<li>Paved the way for subsequent latent-space MBRL (PlaNet, Dreamer).</li>
</ul>
<hr />
<h3 id="72-pets-chua-et-al-2018">7.2 PETS (Chua et al., 2018)</h3>
<p><strong>Core Idea</strong><br />
<strong>P</strong>robabilistic <strong>E</strong>nsembles with <strong>T</strong>rajectory <strong>S</strong>ampling (PETS) uses <strong>ensemble</strong> neural network dynamics models to capture epistemic uncertainty, combined with <strong>sampling-based planning</strong> (like the Cross-Entropy Method, CEM) for continuous control.</p>
<p><img alt="PETS" src="../../assets/images/figures/model-based/pets.png" /></p>
<p><strong>Modeling Uncertainty</strong></p>
<ul>
<li>Train <span class="arithmatex">\(N\)</span> distinct neural networks <span class="arithmatex">\(\{\hat{P}_{\theta_i}\}\)</span>, each predicting <span class="arithmatex">\(\mathbf{s}_{t+1}\)</span> given <span class="arithmatex">\(\mathbf{s}_t, a_t\)</span>.  </li>
<li>Each network outputs a mean <span class="arithmatex">\(\mathbf{\mu}_i\)</span> and variance <span class="arithmatex">\(\mathbf{\Sigma}_i\)</span> for <span class="arithmatex">\(\mathbf{s}_{t+1}\)</span>.  </li>
<li><strong>Ensemble Disagreement</strong> can signal model uncertainty, guiding more cautious or exploratory planning.</li>
</ul>
<p><strong>Planning via Trajectory Sampling</strong>  </p>
<ol>
<li>At state <span class="arithmatex">\(\mathbf{s}_0\)</span>, sample multiple candidate action sequences <span class="arithmatex">\(\{\mathbf{a}_{0:H}\}\)</span>.  </li>
<li>For each sequence, roll out in <strong>all</strong> or a subset of the ensemble models:</li>
<li>
<div class="arithmatex">\[
    \mathbf{s}_{t+1}^{(i)} \sim \hat{P}_{\theta_i}(\mathbf{s}_{t+1} \mid \mathbf{s}_t^{(i)}, a_t).
\]</div>
</li>
<li>
<p>Evaluate cumulative predicted reward <span class="arithmatex">\(\sum_{t=0}^{H-1} r(\mathbf{s}_t^{(i)}, a_t)\)</span>.  </p>
</li>
<li>(Optional) Refine the action distribution using <strong>CEM</strong>:  <ul>
<li>Fit a Gaussian to the top-performing sequences.  </li>
<li>Resample from that Gaussian, repeat until convergence.</li>
</ul>
</li>
</ol>
<p><strong>Mathematically</strong>, the planning objective is:</p>
<div class="arithmatex">\[
\max_{\{a_0, \ldots, a_{H-1}\}} \;\; \mathbb{E}_{\hat{P}_{\theta_i}}\!\Bigl[\sum_{t=0}^{H-1} \gamma^t r(\mathbf{s}_t, a_t)\Bigr],
\]</div>
<p>where the expectation is approximated by sampling from the ensemble.</p>
<p><strong>Significance</strong></p>
<ul>
<li>Achieved <strong>strong sample efficiency</strong> on continuous control (HalfCheetah, Ant, etc.), often matching model-free baselines (SAC, PPO) with far fewer environment interactions.  </li>
<li>Demonstrated the importance of <strong>probabilistic ensembling</strong> to avoid catastrophic model exploitation.</li>
</ul>
<hr />
<h3 id="73-mbpo-janner-et-al-2019">7.3 MBPO (Janner et al., 2019)</h3>
<p><strong>Core Idea</strong><br />
<strong>M</strong>odel-<strong>B</strong>ased <strong>P</strong>olicy <strong>O</strong>ptimization (MBPO) merges the Dyna-like approach (using a learned model to generate synthetic experience) with a <strong>short rollout horizon</strong> to control compounding errors. It then <strong>trains a model-free RL algorithm</strong> (Soft Actor-Critic, SAC) using both real and model-generated data.</p>
<p><strong>Algorithmic Steps</strong></p>
<ol>
<li><strong>Learn an ensemble</strong> of dynamics models <span class="arithmatex">\(\{\hat{P}_{\theta_i}\}\)</span> from real data.  </li>
<li>From each <em>real</em> state <span class="arithmatex">\(\mathbf{s}\)</span> in the replay buffer:</li>
<li>Sample a short-horizon trajectory (1–5 steps) using <span class="arithmatex">\(\hat{P}_{\theta_i}\)</span>, with actions from the current policy <span class="arithmatex">\(\pi_\phi\)</span>.  </li>
<li>Store these “imagined” transitions <span class="arithmatex">\(\bigl(\mathbf{s}, a, \hat{r}, \mathbf{s}'\bigr)\)</span> in the replay buffer.</li>
<li><strong>Train SAC</strong> on the combined real + model-generated transitions.  </li>
<li>Periodically collect more real data with the updated policy, re-fit the model ensemble, and repeat.</li>
</ol>
<p><strong>Key Equations</strong></p>
<ul>
<li>
<p>The model-based transitions:</p>
<div class="arithmatex">\[
    \mathbf{s}_{t+1}^\text{model} \sim \hat{P}_\theta(\mathbf{s}_{t+1} \mid \mathbf{s}_t, a_t),
    \quad
    r_t^\text{model} \sim \hat{R}_\theta(\mathbf{s}_t, a_t).
\]</div>
</li>
<li>
<p>The short horizon <span class="arithmatex">\(H_\text{roll}\)</span> is chosen to limit error accumulation, e.g. <span class="arithmatex">\(H_\text{roll} = 1\)</span> or <span class="arithmatex">\(5\)</span>.</p>
</li>
</ul>
<p><strong>Why Short Rollouts?</strong></p>
<ul>
<li>Long-horizon imagination can deviate quickly from real states =&gt; inaccurate transitions.  </li>
<li>By restricting to a small horizon, MBPO ensures the model is only used in near-realistic states, greatly reducing compounding bias.</li>
</ul>
<p><strong>Performance</strong></p>
<ul>
<li>MBPO matches or exceeds the final returns of top model-free algorithms using ~10% of the environment interactions, combining <strong>high sample efficiency</strong> with <strong>strong asymptotic performance</strong>.</li>
</ul>
<hr />
<h3 id="74-dreamer-hafner-et-al-20202023">7.4 Dreamer (Hafner et al., 2020–2023)</h3>
<p><strong>Core Idea</strong><br />
Learn a <strong>recurrent latent dynamics model</strong> from images, then <strong>backprop</strong> through multi-step model rollouts to train a policy and value function. Dreamer exemplifies a “learned simulator + actor-critic in latent space.”</p>
<p><img alt="Dreamer" src="../../assets/images/figures/model-based/dreamer.png" /></p>
<p><strong>Latent World Model</strong></p>
<ol>
<li><strong>Encoder</strong> <span class="arithmatex">\(e_\phi(\mathbf{o}_t)\)</span> compresses raw observation <span class="arithmatex">\(\mathbf{o}_t\)</span> into a latent state <span class="arithmatex">\(\mathbf{z}_t\)</span>.  </li>
<li><strong>Recurrent Transition</strong> <span class="arithmatex">\(p_\theta(\mathbf{z}_{t+1}\mid \mathbf{z}_t, a_t)\)</span> predicts the next latent, plus a reward model <span class="arithmatex">\(\hat{r}_\theta(\mathbf{z}_t,a_t)\)</span>.  </li>
<li><strong>Decoder</strong> <span class="arithmatex">\(d_\phi(\mathbf{z}_t)\)</span> (optional) can reconstruct <span class="arithmatex">\(\mathbf{o}_t\)</span> for training, but not necessarily used at inference.</li>
</ol>
<p><strong>Policy Learning in Imagination</strong></p>
<ul>
<li>
<p>An actor <span class="arithmatex">\(\pi_\psi(a_t\mid \mathbf{z}_t)\)</span> and critic <span class="arithmatex">\(V_\psi(\mathbf{z}_t)\)</span> are learned by <strong>backprop through the latent rollouts</strong>:</p>
<div class="arithmatex">\[
    \max_{\psi} \;\; \mathbb{E}_{\substack{\mathbf{z}_0 \sim q(\mathbf{z}_0|\mathbf{o}_0) \\ a_t \sim \pi_\psi(\cdot|\mathbf{z}_t) \\ \mathbf{z}_{t+1} \sim p_\theta(\cdot|\mathbf{z}_t,a_t)}}\!\biggl[\sum_{t=0}^{H-1} \gamma^t \hat{r}_\theta(\mathbf{z}_t, a_t)\biggr].
\]</div>
</li>
<li>
<p>Dreamer uses advanced techniques (e.g., <strong>reparameterization</strong>, <strong>actor-critic with value expansion</strong>, etc.) to stabilize training.</p>
</li>
</ul>
<p><strong>Highlights</strong></p>
<ul>
<li><strong>DreamerV1</strong>: SOTA results on DM Control from image inputs.  </li>
<li><strong>DreamerV2</strong>: Extended to Atari, surpassing DQN with a single architecture.  </li>
<li><strong>DreamerV3</strong>: Achieved multi-domain generality (Atari, ProcGen, DM Control, robotics, Minecraft). The first algorithm to solve “collect a diamond” in Minecraft from scratch without demonstrations.</li>
</ul>
<p><strong>Significance</strong></p>
<ul>
<li>Demonstrates that purely <strong>learning a latent world model</strong> + <strong>training by imagination</strong> can match or surpass leading model-free methods in terms of both sample efficiency and final returns.</li>
</ul>
<hr />
<h3 id="75-muzero-deepmind-2020">7.5 MuZero (DeepMind, 2020)</h3>
<p><strong>Core Idea</strong><br />
Combines <strong>Monte Carlo Tree Search (MCTS)</strong> with a <strong>learned latent state</strong> to achieve superhuman performance on Go, Chess, Shogi, and set records on Atari—without knowing the environment’s rules explicitly.</p>
<p><img alt="MuZero" src="../../assets/images/figures/model-based/muzero.png" /></p>
<p><strong>Network Architecture</strong></p>
<ol>
<li><strong>Representation Function</strong> <span class="arithmatex">\(h\)</span>:  </li>
<li>Maps the observation history to a latent state <span class="arithmatex">\(s_0 = h(\mathbf{o}_{1:t})\)</span>.  </li>
<li><strong>Dynamics Function</strong> <span class="arithmatex">\(g\)</span>:  </li>
<li>Predicts the next latent state <span class="arithmatex">\(s_{k+1}=g(s_k, a_k)\)</span> and immediate reward <span class="arithmatex">\(r_k\)</span>.  </li>
<li><strong>Prediction Function</strong> <span class="arithmatex">\(f\)</span>:  </li>
<li>From a latent state <span class="arithmatex">\(s_k\)</span>, outputs a <strong>policy</strong> <span class="arithmatex">\(\pi_k\)</span> (action logits) and a <strong>value</strong> <span class="arithmatex">\(v_k\)</span>.</li>
</ol>
<p><strong>MCTS in Latent Space</strong></p>
<ul>
<li>Starting from <span class="arithmatex">\(s_0\)</span>, expand a search tree by simulating actions via <span class="arithmatex">\(g\)</span>.  </li>
<li>Each node stores mean value estimates <span class="arithmatex">\(\hat{V}\)</span>, visit counts, etc.  </li>
<li>The final search policy <span class="arithmatex">\(\alpha\)</span> is used to update the network (target policy), and the environment reward is used to refine the reward/dynamics parameters.</li>
</ul>
<p><strong>Key Equations</strong></p>
<ul>
<li>
<p>MuZero is trained to <strong>minimize errors</strong> in reward, value, and policy predictions:</p>
<div class="arithmatex">\[
    \mathcal{L}(\theta) = \sum_{t=1}^{T} \bigl(\ell_\mathrm{value}(v_\theta(s_t), z_t) + \ell_\mathrm{policy}(\pi_\theta(s_t), \pi_t) + \ell_\mathrm{dyn}\bigl(g_\theta(s_t, a_t), s_{t+1}\bigr)\bigr),
\]</div>
</li>
</ul>
<p>with <span class="arithmatex">\(\pi_t\)</span> and <span class="arithmatex">\(z_t\)</span> from the improved MCTS-based targets.</p>
<p><strong>Achievements</strong></p>
<ul>
<li>Matches <strong>AlphaZero</strong> performance on Go, Chess, Shogi, but <strong>without</strong> an explicit rules model.  </li>
<li>Set new records on <strong>Atari-57</strong>.  </li>
<li>Demonstrates that an <strong>end-to-end learned model</strong> can be as effective for MCTS as a known simulator, provided it is trained to be “value-equivalent” (predict future rewards and values accurately).</li>
</ul>
<p><strong>Impact</strong></p>
<ul>
<li>Showed that <strong>“learning to model the environment’s reward and value structure is enough”</strong>—MuZero does not need pixel-perfect next-state reconstructions.  </li>
<li>Successfully extended MCTS-based planning to domains with unknown or complex dynamics.</li>
</ul>
<hr />
<h2 id="8-key-benefits-and-drawbacks-of-mbrl">8. Key Benefits (and Drawbacks) of MBRL</h2>
<h3 id="81-data-efficiency">8.1 Data Efficiency</h3>
<p>MBRL can yield higher sample efficiency:</p>
<ul>
<li>Simulating transitions in the model extracts more learning signal from each real step  </li>
<li>E.g., PETS, MBPO, Dreamer require fewer environment interactions than top model-free methods</li>
</ul>
<hr />
<h3 id="82-exploration">8.2 Exploration</h3>
<p>A learned <strong>uncertainty-aware</strong> model can direct exploration to uncertain states:</p>
<ul>
<li>Bayesian or ensemble-based MBRL  </li>
<li>Potentially more efficient than naive <span class="arithmatex">\(\epsilon\)</span>-greedy in high dimensions</li>
</ul>
<hr />
<h3 id="83-optimality">8.3 Optimality</h3>
<p>With a perfect model, MBRL can find better or equal policies vs. model-free. But if the model is imperfect, compounding errors can lead to suboptimal solutions. Research aims to close that gap (MBPO, Dreamer, MuZero).</p>
<hr />
<h3 id="84-transfer">8.4 Transfer</h3>
<p>A global <strong>dynamics model</strong> can be re-used across tasks or reward functions:</p>
<ul>
<li>E.g., a learned robotic physics model can quickly adapt to new goals</li>
<li>Saves extensive retraining</li>
</ul>
<hr />
<h3 id="85-safety">8.5 Safety</h3>
<p>In real-world tasks (robotics, healthcare), we can plan or verify constraints inside the model before acting. Uncertainty estimation is crucial.</p>
<hr />
<h3 id="86-explainability">8.6 Explainability</h3>
<p>A learned model can sometimes be probed or visualized, offering partial interpretability (though deep generative models remain somewhat opaque).</p>
<hr />
<h3 id="87-disbenefits">8.7 Disbenefits</h3>
<ol>
<li><strong>Model bias</strong>: Imperfect models =&gt; compounding errors  </li>
<li><strong>Computational overhead</strong>: Planning can be expensive  </li>
<li><strong>Implementation complexity</strong>: We must keep models accurate, stable, and do policy updates in tandem</li>
</ol>
<hr />
<h2 id="9-conclusion">9. Conclusion</h2>
<p>Model-Based RL integrates <strong>planning</strong> and <strong>learning</strong> in the RL framework, offering strong sample efficiency and structured decision-making. Algorithms like <strong>MBPO</strong>, <strong>Dreamer</strong>, and <strong>MuZero</strong> demonstrate that short rollouts, uncertainty estimates, or latent value-equivalent models can yield high final performance with fewer real samples.</p>
<p>Still, challenges remain:</p>
<ul>
<li>Robustness under partial observability, stochastic transitions, or non-stationary tasks  </li>
<li>Balancing planning vs. data collection adaptively  </li>
<li>Scaling to high-dimensional, real-world tasks with safety constraints</li>
</ul>
<p>Future work includes deeper hierarchical methods, advanced uncertainty modeling, bridging theory and practice, and constructing more interpretable or structured models.</p>
<hr />
<h2 id="10-references">10. References</h2>
<ul>
<li>
<p><strong>S. Levine (CS 294-112: Deep RL)</strong><br />
<em>Model-Based Reinforcement Learning, Lecture 9 slides.</em><br />
<a href="https://rail.eecs.berkeley.edu/deeprlcourse/">Lecture Site</a>, <a href="https://www.youtube.com/playlist?list=PLwRJQ4m4UJjOv1QCr2f1XDNRWGM4un3—">Video Repository</a><br />
<strong>Additional resources</strong>: <a href="http://rail.eecs.berkeley.edu/deeprlcourse/">Open course materials from UC Berkeley’s Deep RL class</a>  </p>
</li>
<li>
<p><strong>T. Moerland et al. (2022)</strong><br />
<em>“Model-based Reinforcement Learning: A Survey.”</em> <a href="https://arxiv.org/abs/2006.16712">arXiv:2006.16712v4</a><br />
<strong>Additional resources</strong>: <a href="https://github.com/tmoer/mbrl-survey">Official GitHub for references and code snippets mentioned in the paper</a>  </p>
</li>
<li>
<p><strong>Sutton, R.S. &amp; Barto, A.G.</strong><br />
<em>Reinforcement Learning: An Introduction (2nd edition).</em> MIT Press, 2018.<br />
<a href="http://incompleteideas.net/book/the-book-2nd.html">Online Draft</a><br />
<strong>Additional resources</strong>: <a href="https://www.reddit.com/r/reinforcementlearning/">Exercise solutions and discussion forum</a>  </p>
</li>
<li>
<p><strong>Puterman, M.L.</strong><br />
<em>Markov Decision Processes: Discrete Stochastic Dynamic Programming.</em> John Wiley &amp; Sons, 2014.<br />
<a href="https://www.wiley.com/en-us/Markov+Decision+Processes%3A+Discrete+Stochastic+Dynamic+Programming-p-9780471727828">Publisher Link</a><br />
<strong>Additional resources</strong>: <a href="https://www2.isye.gatech.edu/~harahang.course/6231/MDPnotes.pdf">Various lecture slides summarizing MDP fundamentals</a>  </p>
</li>
<li>
<p><strong>Deisenroth, M. &amp; Rasmussen, C.E.</strong> (2011)<br />
<em>PILCO: A Model-Based and Data-Efficient Approach to Policy Search.</em> ICML.<br />
<a href="https://proceedings.mlr.press/v15/deisenroth11a/deisenroth11a.pdf">Paper PDF</a><br />
<strong>Additional resources</strong>: <a href="https://github.com/mwhoffman/pilco">Official code release on GitHub</a>  </p>
</li>
<li>
<p><strong>Chua, K. et al.</strong> (2018)<br />
<em>“Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models (PETS).”</em> NeurIPS.<br />
<a href="https://proceedings.neurips.cc/paper_files/paper/2018/file/3de568f8597b94bda53149c7d7f5958c-Paper.pdf">Paper Link</a><br />
<strong>Additional resources</strong>: <a href="https://github.com/kchua/handful-of-trials">Author’s implementation</a>  </p>
</li>
<li>
<p><strong>Janner, M. et al.</strong> (2019)<br />
<em>“When to Trust Your Model: Model-Based Policy Optimization.”</em> NeurIPS (MBPO).<br />
<a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/5faf461eff3099671ad63c6f3f094f7f-Paper.pdf">Paper Link</a><br />
<strong>Additional resources</strong>: <a href="https://bair.berkeley.edu/blog/2019/12/12/mbpo/">Berkeley AI Research blog post</a>  </p>
</li>
<li>
<p><strong>Hafner, D. et al.</strong> (2020–2023)<br />
<em>“Dreamer” line of papers</em> (ICML, arXiv).<br />
<a href="https://github.com/danijar/dreamer">DreamerV2 Code</a>, <a href="https://danijar.com/dreamer/">Dreamer Blog</a><br />
<strong>Additional resources</strong>: <a href="https://danijar.com/">Tutorial videos by Danijar Hafner on latent world models</a>  </p>
</li>
<li>
<p><strong>Ha, D. &amp; Schmidhuber, J.</strong> (2018)<br />
<em>“World Models.”</em> NeurIPS.<br />
<a href="https://proceedings.neurips.cc/paper/2018/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf">Paper PDF</a>, <a href="https://worldmodels.github.io/">Project Site</a><br />
<strong>Additional resources</strong>: <a href="https://otoro.net/">Interactive demos and blog articles from David Ha</a>  </p>
</li>
<li>
<p><strong>Silver, D. et al.</strong> (various)<br />
<em>AlphaGo, AlphaZero, MuZero</em> lines of research.<br />
<a href="https://deepmind.com/blog/article/muzero-mastering-go-chess-shogi-and-atari">DeepMind’s MuZero Blog</a><br />
<strong>Additional resources</strong>: <a href="https://www.nature.com/articles/nature16961">Further reading on MCTS, AlphaGo, and AlphaZero in “Mastering the Game of Go” series</a>  </p>
</li>
</ul>







  
  



  



  <form class="md-feedback" name="feedback" hidden>
    <fieldset>
      <legend class="md-feedback__title">
        Was this page helpful?
      </legend>
      <div class="md-feedback__inner">
        <div class="md-feedback__list">
          
            <button class="md-feedback__icon md-icon" type="submit" title="This page was helpful" data-md-value="1">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m7 0c0 .8-.7 1.5-1.5 1.5S14 10.3 14 9.5 14.7 8 15.5 8s1.5.7 1.5 1.5m-5 7.73c-1.75 0-3.29-.73-4.19-1.81L9.23 14c.45.72 1.52 1.23 2.77 1.23s2.32-.51 2.77-1.23l1.42 1.42c-.9 1.08-2.44 1.81-4.19 1.81"/></svg>
            </button>
          
            <button class="md-feedback__icon md-icon" type="submit" title="This page could be improved" data-md-value="0">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10m-6.5-4c.8 0 1.5.7 1.5 1.5s-.7 1.5-1.5 1.5-1.5-.7-1.5-1.5.7-1.5 1.5-1.5M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m2 4.5c1.75 0 3.29.72 4.19 1.81l-1.42 1.42C14.32 16.5 13.25 16 12 16s-2.32.5-2.77 1.23l-1.42-1.42C8.71 14.72 10.25 14 12 14"/></svg>
            </button>
          
        </div>
        <div class="md-feedback__note">
          
            <div data-md-value="1" hidden>
              
              
                
              
              
              
                
                
              
              Thanks for your feedback!
            </div>
          
            <div data-md-value="0" hidden>
              
              
                
              
              
              
                
                
              
              Thanks for your feedback! Help us improve this page by using our <a href="..." target="_blank" rel="noopener">feedback form</a>.
            </div>
          
        </div>
      </div>
    </fieldset>
  </form>


<script src="https://giscus.app/client.js"
        data-repo="DeepRLCourse/DeepRLCourse.github.io"
        data-repo-id="R_kgDOMPOFbQ"
        data-category="Announcements"
        data-category-id="DIC_kwDOMPOFbc4CmGY6"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="1"
        data-input-position="top"
        data-theme="preferred_color_scheme"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>

                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Made with ❤️ in Robust and Interpretable Machine Learning Lab
    </div>
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.top", "toc.integrate"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>