
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="This page serves as a comprehensive introduction to Reinforcement Learning (RL), a key area of artificial intelligence. It explores the limitations of traditional AI methods, highlights the unique strengths of RL, and provides foundational knowledge on concepts like Markov Decision Processes (MDPs) and Partially Observable MDPs (POMDPs). Through examples such as Grid World and autonomous driving, the page illustrates how RL agents learn optimal policies by interacting with dynamic environments. Additionally, it delves into utility functions, the Bellman equation, and the challenges of exploration and sparse rewards, offering a solid foundation for understanding RL's principles and applications.">
      
      
      
        <link rel="canonical" href="https://DeepRL2026.github.io/course_notes/intro-to-rl/">
      
      
      
      
        
      
      
      <link rel="icon" href="../../assets/favicon/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.3">
    
    
      
        <title>Week 1: Introduction to RL - Deep RL Course</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300,300i,400,400i,700,700i%7CRed+Hat+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Ubuntu";--md-code-font:"Red Hat Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
      <link rel="stylesheet" href="../../stylesheets/vazirmatn.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-Q1YT8K39WE"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-Q1YT8K39WE",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-Q1YT8K39WE",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
  
<meta property="og:type" content="website" />
<meta property="og:title" content="Week 1: Introduction to RL - Deep RL Course" />
<meta property="og:description" content="This page serves as a comprehensive introduction to Reinforcement Learning (RL), a key area of artificial intelligence. It explores the limitations of traditional AI methods, highlights the unique strengths of RL, and provides foundational knowledge on concepts like Markov Decision Processes (MDPs) and Partially Observable MDPs (POMDPs). Through examples such as Grid World and autonomous driving, the page illustrates how RL agents learn optimal policies by interacting with dynamic environments. Additionally, it delves into utility functions, the Bellman equation, and the challenges of exploration and sparse rewards, offering a solid foundation for understanding RL's principles and applications." />
<meta property="og:image" content="https://DeepRL2026.github.io/assets/images/social/course_notes/intro-to-rl.png" />
<meta property="og:image:type" content="image/png" />
<meta property="og:image:width" content="1200" />
<meta property="og:image:height" content="630" />
<meta property="og:url" content="https://DeepRL2026.github.io/course_notes/intro-to-rl/" />
<meta property="twitter:card" content="summary_large_image" />
<meta property="twitter:title" content="Week 1: Introduction to RL - Deep RL Course" />
<meta property="twitter:description" content="This page serves as a comprehensive introduction to Reinforcement Learning (RL), a key area of artificial intelligence. It explores the limitations of traditional AI methods, highlights the unique strengths of RL, and provides foundational knowledge on concepts like Markov Decision Processes (MDPs) and Partially Observable MDPs (POMDPs). Through examples such as Grid World and autonomous driving, the page illustrates how RL agents learn optimal policies by interacting with dynamic environments. Additionally, it delves into utility functions, the Bellman equation, and the challenges of exploration and sparse rewards, offering a solid foundation for understanding RL's principles and applications." />
<meta property="twitter:image" content="https://DeepRL2026.github.io/assets/images/social/course_notes/intro-to-rl.png" />
</head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#week-1-introduction-to-rl" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Deep RL Course" class="md-header__button md-logo" aria-label="Deep RL Course" data-md-component="logo">
      
  <img src="../../assets/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Deep RL Course
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Week 1: Introduction to RL
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="red"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="red"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  Home

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  Course Notes

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../lectures/lecture1/" class="md-tabs__link">
          
  
  
  Lectures

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  TA Sessions

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  Homeworks

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  Projects

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../exams/" class="md-tabs__link">
          
  
  
  Exams

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../resources/" class="md-tabs__link">
          
  
  
  Resources

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
    
      

    
        
    

    

        

            
                  
            

        

            

        
    

    


    <!-- Navigation -->
    
        
        <div
            class="md-sidebar md-sidebar--primary"
            data-md-component="sidebar"
            data-md-type="navigation"
            
        >
            <div class="md-sidebar__scrollwrap">
            <div class="md-sidebar__inner">
                <!--
  Copyright (c) 2016-2025 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->



<!-- Determine classes -->


  


  


<!-- Navigation -->
<nav
  class="md-nav md-nav--primary md-nav--lifted"
  aria-label="Navigation"
  data-md-level="0"
>

  <!-- Site title -->
  <label class="md-nav__title" for="__drawer">
    <a
      href="../.."
      title="Deep RL Course"
      class="md-nav__button md-logo"
      aria-label="Deep RL Course"
      data-md-component="logo"
    >
      
  <img src="../../assets/logo.png" alt="logo">

    </a>
    Deep RL Course
  </label>

  <!-- Repository information -->
  

  <!-- Navigation list -->
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Home
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Course Notes
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Course Notes
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Lectures
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Lectures
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../lectures/lecture1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Lecture1
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../lectures/lecture2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Lecture2
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    TA Sessions
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    TA Sessions
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Homeworks
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    Homeworks
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Projects
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    Projects
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Exams
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            
  
    Exams
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../exams/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Previous Semesters
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Resources
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            
  
    Resources
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../resources/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Resources
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
            </div>
            </div>
        </div>
    

    <!-- Table of contents -->
    
        
        <div
            class="md-sidebar md-sidebar--secondary"
            data-md-component="sidebar"
            data-md-type="toc"
            
        >
            <div class="md-sidebar__scrollwrap">
            <div class="md-sidebar__inner">
                <!--
  Copyright (c) 2016-2025 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->

<!-- Determine title -->




<!-- Table of contents -->
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  

  <!--
    Check whether the content starts with a level 1 headline. If it does, the
    top-level anchor must be skipped, since it would be redundant to the link
    to the current page that is located just above the anchor. Therefore we
    directly continue with the children of the anchor.
  -->
  
  
    
  

  <!-- Table of contents title and list -->
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#why-do-we-need-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Why Do We Need Reinforcement Learning?
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Why Do We Need Reinforcement Learning?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#limitations-of-other-ai-methods" class="md-nav__link">
    <span class="md-ellipsis">
      
        Limitations of Other AI Methods
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#where-reinforcement-learning-shines" class="md-nav__link">
    <span class="md-ellipsis">
      
        Where Reinforcement Learning Shines
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-difference-between-rl-and-supervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Key Difference Between RL and Supervised Learning
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Key Difference Between RL and Supervised Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#supervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Supervised Learning:
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Reinforcement Learning
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-autonomous-driving" class="md-nav__link">
    <span class="md-ellipsis">
      
        Example: Autonomous Driving
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-concepts-in-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Key Concepts in Reinforcement Learning
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Key Concepts in Reinforcement Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-state-mathbfs_t" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. State (\(\mathbf{s}_t\))
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-action-mathbfa_t" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Action (\(\mathbf{a}_t\))
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-reward-r_t" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Reward (\(r_t\))
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-policy-pi_theta" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Policy (\(\pi_{\theta}\))
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-anatomy-of-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        The Anatomy of Reinforcement Learning
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Anatomy of Reinforcement Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-evaluate-the-action-how-well-was-our-choice-of-x-y-z-execute-gripping" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. Evaluate the Action: "How well was our choice of (x, y, z); execute gripping!"
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-fit-a-model" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Fit a Model
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-estimate-the-return-gripping-objects-estimate-the-return" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Estimate the Return: "Gripping objects estimate the return"
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-understand-physical-consequences-or-when-i-grip-some-object-on-a-specific-location-what-happens-physically-ie-how-its-x-y-zs-change" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Understand Physical Consequences: "Or ... when I grip some object on a specific location, what happens physically; i.e. how its (x, y, z)'s change?"
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-improve-the-policy-improve-the-policy-best-guess" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. Improve the Policy: "Improve the policy 'best guess'"
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#comparing-reinforcement-learning-with-other-learning-methods" class="md-nav__link">
    <span class="md-ellipsis">
      
        Comparing Reinforcement Learning with Other Learning Methods
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Comparing Reinforcement Learning with Other Learning Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#comparison-table" class="md-nav__link">
    <span class="md-ellipsis">
      
        Comparison Table
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-concepts" class="md-nav__link">
    <span class="md-ellipsis">
      
        Key Concepts
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#planning-vs-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Planning vs Learning
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduction-to-markov-decision-processes-mdps" class="md-nav__link">
    <span class="md-ellipsis">
      
        Introduction to Markov Decision Processes (MDPs)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Introduction to Markov Decision Processes (MDPs)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#components-of-an-mdp" class="md-nav__link">
    <span class="md-ellipsis">
      
        Components of an MDP
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#episodes-and-environment-types" class="md-nav__link">
    <span class="md-ellipsis">
      
        Episodes and Environment Types
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#difference-between-horizon-and-episode" class="md-nav__link">
    <span class="md-ellipsis">
      
        Difference Between Horizon and Episode
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-goal-of-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        The Goal of Reinforcement Learning
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Goal of Reinforcement Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#objective" class="md-nav__link">
    <span class="md-ellipsis">
      
        Objective:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-grid-world" class="md-nav__link">
    <span class="md-ellipsis">
      
        Example: Grid World
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#an-example-of-gridworld" class="md-nav__link">
    <span class="md-ellipsis">
      
        An Example of Gridworld
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stochastic-policy" class="md-nav__link">
    <span class="md-ellipsis">
      
        Stochastic Policy
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Stochastic Policy">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-gridworld-with-a-stochastic-policy" class="md-nav__link">
    <span class="md-ellipsis">
      
        Example: Gridworld with a Stochastic Policy
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#graphical-model-of-mdps" class="md-nav__link">
    <span class="md-ellipsis">
      
        Graphical Model of MDPs
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#partially-observable-mdps-pomdps" class="md-nav__link">
    <span class="md-ellipsis">
      
        Partially Observable MDPs (POMDPs)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Partially Observable MDPs (POMDPs)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#policy-as-a-function-of-state-mathbfs_t-or-observation-mathbfo_t" class="md-nav__link">
    <span class="md-ellipsis">
      
        Policy as a Function of State (\(\mathbf{s}_t\)) or Observation (\(\mathbf{o}_t\))
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#utility-function-in-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Utility Function in Reinforcement Learning
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Utility Function in Reinforcement Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#definition-of-utility-function" class="md-nav__link">
    <span class="md-ellipsis">
      
        Definition of Utility Function
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-is-the-utility-function-important" class="md-nav__link">
    <span class="md-ellipsis">
      
        Why is the Utility Function Important?
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bellman-equation-for-utility-functions" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bellman Equation for Utility Functions
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-grid-world_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Example: Grid World
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#authors" class="md-nav__link">
    <span class="md-ellipsis">
      
        Author(s)
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
            </div>
            </div>
        </div>
    

          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="week-1-introduction-to-rl">Week 1: Introduction to RL</h1>
<h2 id="why-do-we-need-reinforcement-learning">Why Do We Need Reinforcement Learning?</h2>
<p>Reinforcement Learning (RL) is a subfield of artificial intelligence (AI) that focuses on training agents to make sequences of decisions by interacting with an environment. Unlike other AI methods, RL is particularly well-suited for problems where the agent must learn optimal behavior through trial and error, often in dynamic and uncertain environments.</p>
<h3 id="limitations-of-other-ai-methods">Limitations of Other AI Methods</h3>
<ol>
<li><strong>Supervised Learning</strong>: </li>
<li><strong>What it does</strong>: Supervised learning requires a labeled dataset where the correct output (label) is provided for each input. The model learns to map inputs to outputs based on this data.</li>
<li>
<p><strong>Limitation</strong>: In many real-world scenarios, obtaining a labeled dataset is impractical or impossible. For example, consider training a robot to walk. It's not feasible to provide a labeled dataset of all possible states and actions the robot might encounter.</p>
</li>
<li>
<p><strong>Unsupervised Learning</strong>:</p>
</li>
<li><strong>What it does</strong>: Unsupervised learning deals with unlabeled data and tries to find hidden patterns or intrinsic structures within the data.</li>
<li>
<p><strong>Limitation</strong>: While unsupervised learning can identify patterns, it doesn't provide a way to make decisions or take actions based on those patterns. For instance, clustering similar states in a game doesn't tell the agent how to win the game.</p>
</li>
<li>
<p><strong>Traditional Control Methods</strong>:</p>
</li>
<li><strong>What it does</strong>: Traditional control methods are designed to maintain a system's state within a desired range using predefined rules.</li>
<li><strong>Limitation</strong>: These methods require a precise model of the environment and are not adaptable to complex, changing environments.</li>
</ol>
<h3 id="where-reinforcement-learning-shines">Where Reinforcement Learning Shines</h3>
<p>Reinforcement Learning excels in scenarios where:</p>
<ul>
<li>
<p><strong>No Labeled Data is Available</strong>: RL agents learn by interacting with the environment and receiving feedback in the form of rewards or penalties. This eliminates the need for a pre-labeled dataset.</p>
</li>
<li>
<p><strong>Sequential Decision-Making is Required</strong>: RL is designed to handle problems where decisions are made in a sequence, and each decision affects the future state of the environment. For example, in a game like Go or Chess, each move affects the board's state and influences future moves.</p>
</li>
<li>
<p><strong>The Environment is Dynamic and Uncertain</strong>: RL agents can adapt to changing environments and learn optimal policies even when the environment is not fully known or is stochastic. For instance, an RL agent can learn to navigate a maze even if the maze's layout changes over time.</p>
</li>
<li>
<p><strong>Non-i.i.d. Data</strong>: RL is capable of handling non-independent and identically distributed (non-i.i.d.) data. In many real-world scenarios, data points are not independent (e.g., the state of the environment at one time step depends on previous states) and may not be identically distributed (e.g., the distribution of states changes over time). A notable example is <strong>robotic control</strong>, where an autonomous robot learns to walk. Each movement directly affects the next state, and the terrain may change dynamically, requiring the RL agent to adapt its policy based on sequential dependencies. RL agents can learn from such data by considering the temporal dependencies and adapting to the evolving data distribution.</p>
</li>
<li>
<p><strong>Lack of Input Data</strong>: In supervised learning, the model is provided with input-output pairs <span class="arithmatex">\((x, y)\)</span>, where <span class="arithmatex">\(x\)</span> is the input data and <span class="arithmatex">\(y\)</span> is the corresponding label. In RL, not only are the labels (correct actions) not provided, but the "<span class="arithmatex">\(x\)</span>"s (input states) are also not explicitly given to the model. The agent must actively interact with the environment to observe and collect these states.</p>
</li>
<li>
<p><strong>Example</strong>: Consider training an RL agent to play a video game. In supervised learning, you would need a dataset of game states (<span class="arithmatex">\(x\)</span>) and the corresponding optimal actions (<span class="arithmatex">\(y\)</span>). In RL, the agent starts with no prior knowledge of the game states or actions. It must explore the game environment, observe the states, and learn which actions lead to higher rewards. This process of discovering and learning from the environment is what makes RL uniquely powerful for tasks where the input data is not readily available.</p>
</li>
</ul>
<h3 id="key-difference-between-rl-and-supervised-learning">Key Difference Between RL and Supervised Learning</h3>
<p>The primary distinction between RL and supervised learning is that, while feedback is provided in RL, the exact correct answer or action is not explicitly given. Let's delve deeper into this concept and explore the importance of exploration in RL, along with its challenges.</p>
<h4 id="supervised-learning">Supervised Learning:</h4>
<ul>
<li>In supervised learning, you are presented with a bunch of data and told exactly what the answer for each data point is. Your model adjusts its parameters to get the prediction right for the data you trained on, and the goal is to generalize to unseen data, but is doesn't try to do <strong>better</strong> than the data.</li>
<li>
<p><strong>Example</strong>: In a image classification task, the model is given images along with their correct labels (e.g., "cat" or "dog"). The model learns to predict the label for new, unseen images based on this labeled data.</p>
</li>
<li>
<p><a href="https://sites.google.com/view/icml2018-imitation-learning/"><strong>Imitation Learning</strong></a>: A specialized form of supervised learning used in decision-making problems is <strong>Imitation Learning (IL)</strong>. In IL, the model learns by mimicking expert demonstrations. The training data consists of state-action pairs provided by an expert, and the model learns to replicate the expert's behavior. Unlike traditional supervised learning, IL is applied to sequential decision-making tasks, making it a bridge between supervised learning and RL.</p>
</li>
</ul>
<h4 id="reinforcement-learning">Reinforcement Learning</h4>
<ul>
<li>
<p><strong>Agent-Environment Interaction</strong>: In RL, the agent interacts with the environment, takes actions, receives rewards, and adapts its policy based on these rewards. However, unlike supervised learning, the agent is never told which action was the right one for a given state or what the correct policy is for a given task. In other words, there are no labels! The agent must learn the optimal actions using the learning signals provided by the reward.</p>
</li>
<li>
<p><strong>Exploration</strong>: Exploration is a critical component of RL. Since the agent is not provided with labeled data or explicit instructions, it must explore the environment to discover which actions yield the highest rewards. This exploration allows the agent to learn from its experiences and improve its policy over time. Without exploration, the agent might get stuck in suboptimal behaviors, never discovering better strategies.</p>
</li>
<li>
<p><strong>Drawbacks and Difficulties of Exploration</strong>: While exploration is essential, it comes with its own set of challenges. One major difficulty is that <strong>wrong exploration</strong> can lead to poor learning outcomes. If the agent explores suboptimal or harmful actions excessively, it may reinforce bad behaviors, leading to a failure in learning the optimal policy. This is often summarized by the phrase <strong>"garbage in, garbage out"</strong>â€”if the agent explores poorly and collects low-quality data, the resulting policy will also be of low quality.</p>
</li>
<li>
<p><strong>Example</strong>: Consider an RL agent learning to navigate a maze. If the agent spends too much time exploring dead ends or repeatedly taking wrong turns, it may fail to discover the correct path to the goal. The agent's policy will be based on these poor explorations, resulting in a suboptimal or even failing strategy.</p>
</li>
<li>
<p><strong>Balancing Exploration and Exploitation</strong>: A key challenge in RL is balancing exploration (trying new actions to discover their effects) and exploitation (using known actions that yield high rewards). Too much exploration can lead to inefficiency, while too much exploitation can cause the agent to miss out on better strategies. Techniques like <strong>epsilon-greedy policies</strong> and <strong>Thompson sampling</strong> are often used to strike this balance.</p>
</li>
<li>
<p><strong>Outperforming Human Intelligence</strong>: Despite the challenges, when exploration is done effectively, the agent can discover novel strategies and solutions that humans might not consider. Since the data is collected by the agent itself through exploration, an RL agent can even <strong>outperform</strong> human intelligence and execute impressive actions that no one has thought of before.</p>
</li>
<li>
<p><strong>Example</strong>: In a game of chess, the RL agent might receive a reward for winning the game but won't be told which specific move led to the victory. It must figure out the sequence of optimal moves through trial and error. By exploring different moves and learning from the outcomes, the agent can develop a strategy that maximizes its chances of winning. However, if the agent explores ineffective moves too often, it may fail to learn a winning strategy.</p>
</li>
</ul>
<h3 id="example-autonomous-driving">Example: Autonomous Driving</h3>
<p>Consider the task of autonomous driving. </p>
<ul>
<li><strong>Supervised Learning Approach</strong>: You would need a massive labeled dataset of all possible driving scenarios, including rare events like a child running into the street. This is impractical.</li>
<li>
<p><strong>Imitation Learning</strong>: In autonomous driving, IL can be used to train a model by observing human drivers. The model is provided with data on how human drivers react in various driving scenarios (e.g., steering, braking, accelerating). The model learns to mimic these actions, effectively reducing the problem to a supervised learning task. However, the model's performance is limited by the quality of the expert demonstrations and may not discover strategies that outperform the expert.</p>
</li>
<li>
<p><strong>Reinforcement Learning Approach</strong>: An RL agent can learn to drive by interacting with a simulated environment. It receives rewards for safe driving and penalties for accidents or traffic violations. Over time, the agent learns an optimal policy for driving without needing a labeled dataset.</p>
</li>
</ul>
<h2 id="key-concepts-in-reinforcement-learning">Key Concepts in Reinforcement Learning</h2>
<p>To understand RL, it's essential to grasp some fundamental concepts: <strong>state</strong>, <strong>action</strong>, <strong>reward</strong>, and <strong>policy</strong>. Let's explore each of these concepts in detail, using an example to illustrate their roles.</p>
<h3 id="1-state-mathbfs_t">1. State (<span class="arithmatex">\(\mathbf{s}_t\)</span>)</h3>
<ul>
<li><strong>Definition</strong>: A <strong>state</strong> represents the current situation or configuration of the environment at a given time. It encapsulates all the relevant information that the agent needs to make a decision.</li>
<li><strong>Example</strong>: Consider a self-driving car navigating through a city. The state could include the car's current position, speed, the positions of other vehicles, traffic lights, and pedestrians. All these factors together define the state of the environment at any moment.</li>
</ul>
<h3 id="2-action-mathbfa_t">2. Action (<span class="arithmatex">\(\mathbf{a}_t\)</span>)</h3>
<ul>
<li><strong>Definition</strong>: An <strong>action</strong> is a decision or move made by the agent that affects the environment. The set of all possible actions that an agent can take is called the <strong>action space</strong>.</li>
<li><strong>Example</strong>: In the self-driving car scenario, possible actions might include accelerating, braking, turning left, turning right, or maintaining the current speed. Each action changes the state of the environment, such as the car's position or speed.</li>
</ul>
<h3 id="3-reward-r_t">3. Reward (<span class="arithmatex">\(r_t\)</span>)</h3>
<ul>
<li>
<p><strong>Definition</strong>: A <strong>reward</strong> is a feedback signal that the agent receives from the environment after taking an action. The reward indicates the immediate benefit or cost of the action taken. The goal of the agent is to maximize the cumulative reward over time. The reward can be provided by an <strong>expert</strong> or learned from demonstrations. This can be achieved by directly copying observed behavior or inferring rewards from observed behavior (<a href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa12/slides/inverseRL.pdf"><strong>Inverse RL</strong></a>) .</p>
</li>
<li>
<p><strong>Example</strong>: For the self-driving car, rewards could be assigned as follows:</p>
</li>
<li><strong>Positive Reward</strong>: Reaching the destination safely (+100), obeying traffic rules (+10).</li>
<li>
<p><strong>Negative Reward</strong>: Colliding with another vehicle (-100), running a red light (-50).</p>
</li>
<li>
<p><strong>Sparse Reward Environments</strong></p>
</li>
<li>
<p><strong>Definition</strong>: In <strong>sparse reward environments</strong>, the agent receives rewards very infrequently. Instead of getting feedback after every action, the agent might only receive a reward after completing a long sequence of actions or achieving a significant milestone. For example, in a game, the agent might only receive a reward upon winning or losing, with no feedback provided during the game.</p>
</li>
<li>
<p><strong>Challenges</strong>:</p>
<ul>
<li><strong>Difficulty in Learning</strong>: Sparse rewards make it challenging for the agent to learn which actions lead to positive outcomes. Since the agent receives little to no feedback during most of its interactions, it struggles to associate specific actions with rewards. This can lead to slow or ineffective learning.</li>
<li><strong>Exploration</strong>: In sparse reward environments, the agent must explore extensively to discover actions that yield rewards. Without frequent feedback, the agent may take a long time to stumble upon the correct sequence of actions, making the learning process inefficient.</li>
<li><strong>Credit Assignment Problem</strong>: Determining which actions in a sequence contributed to a reward is difficult in sparse reward settings. The agent may not be able to accurately attribute the reward to the correct actions, leading to suboptimal policies.</li>
</ul>
</li>
<li>
<p><strong>Example</strong>: Consider an RL agent learning to play a complex strategy game where the only reward is given at the end of the game (e.g., +1 for winning and -1 for losing). The agent must explore countless moves and strategies without any intermediate feedback, making it challenging to learn effective strategies. The agent might take a very long time to discover the sequence of actions that leads to a win.</p>
</li>
</ul>
<h3 id="4-policy-pi_theta">4. Policy (<span class="arithmatex">\(\pi_{\theta}\)</span>)</h3>
<ul>
<li><strong>Definition</strong>: A <strong>policy</strong> is a strategy or set of rules that the agent follows to decide which action to take in a given state. It maps states to actions and can be deterministic (always choosing a specific action in a state) or stochastic (choosing actions based on a probability distribution).</li>
<li><strong>Example</strong>: In the self-driving car example, a policy might dictate that the car should slow down when it detects a pedestrian crossing the street or stop when it encounters a red traffic light. The policy is what the agent learns and optimizes to maximize cumulative rewards.</li>
</ul>
<center> 
<img src="\assets\images\course_notes\intro-to-rl\RL_Framework.png"
     alt=""
     style="float: center; margin-right: 10px;" 
     />
      Fig1. RL Framework </center>

<h2 id="the-anatomy-of-reinforcement-learning">The Anatomy of Reinforcement Learning</h2>
<p>An RL agent's training is done through an interactive process between the agent and the environment. This process involves several key steps that allow the agent to learn and improve its decision-making capabilities. </p>
<h3 id="1-evaluate-the-action-how-well-was-our-choice-of-x-y-z-execute-gripping">1. <strong>Evaluate the Action: "How well was our choice of (x, y, z); execute gripping!"</strong></h3>
<ul>
<li>This step involves assessing the effectiveness of the action taken by the agent. The agent performs an action, such as gripping an object at a specific location (x, y, z), and then evaluates how well this action was executed.</li>
</ul>
<h3 id="2-fit-a-model">2. <strong>Fit a Model</strong></h3>
<ul>
<li>Fitting a model refers to creating a representation of the environment or the task based on the data collected from interactions. This model helps the agent predict the outcomes of future actions and understand the dynamics of the environment.</li>
</ul>
<h3 id="3-estimate-the-return-gripping-objects-estimate-the-return">3. <strong>Estimate the Return: "Gripping objects estimate the return"</strong></h3>
<ul>
<li>Estimating the return involves calculating the expected cumulative reward from a given state or action. The return is a key concept in RL, as the agent aims to maximize this cumulative reward over time.</li>
</ul>
<h3 id="4-understand-physical-consequences-or-when-i-grip-some-object-on-a-specific-location-what-happens-physically-ie-how-its-x-y-zs-change">4. <strong>Understand Physical Consequences: "Or ... when I grip some object on a specific location, what happens physically; i.e. how its (x, y, z)'s change?"</strong></h3>
<ul>
<li>This step emphasizes the importance of understanding the physical consequences of actions. The agent needs to know how its actions (like gripping an object at a specific location) affect the environment, particularly the object's position (x, y, z).</li>
</ul>
<h3 id="5-improve-the-policy-improve-the-policy-best-guess">5. <strong>Improve the Policy: "Improve the policy 'best guess'"</strong></h3>
<ul>
<li>Improving the policy involves refining the agent's strategy for choosing actions based on the feedback received from the environment. The "best guess" refers to the agent's current understanding of the optimal actions, which is continuously updated as the agent learns.</li>
</ul>
<center> 
<img src="\assets\images\course_notes\intro-to-rl\RL_Anatomy.png"
     alt=""
     style="float: center; margin-right: 10px;" 
     />
      Fig2. The Anatomy of RL </center>

<h2 id="comparing-reinforcement-learning-with-other-learning-methods">Comparing Reinforcement Learning with Other Learning Methods</h2>
<p>To better understand the unique characteristics of Reinforcement Learning, it's helpful to compare it with other learning methods. The table below provides a comparison of RL with other approaches such as Supervised Learning (SL), Unsupervised Learning (UL), and Imitation Learning (IL). Let's break down the table and discuss the key differences and similarities.</p>
<h3 id="comparison-table">Comparison Table</h3>
<table>
<thead>
<tr>
<th></th>
<th>AI Planning</th>
<th>SL</th>
<th>UL</th>
<th>RL</th>
<th>IL</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Optimization</strong></td>
<td>X</td>
<td></td>
<td></td>
<td>X</td>
<td>X</td>
</tr>
<tr>
<td><strong>Learns from experience</strong></td>
<td></td>
<td>X</td>
<td>X</td>
<td>X</td>
<td>X</td>
</tr>
<tr>
<td><strong>Generalization</strong></td>
<td>X</td>
<td>X</td>
<td>X</td>
<td>X</td>
<td>X</td>
</tr>
<tr>
<td><strong>Delayed Consequences</strong></td>
<td>X</td>
<td></td>
<td></td>
<td>X</td>
<td>X</td>
</tr>
<tr>
<td><strong>Exploration</strong></td>
<td></td>
<td></td>
<td></td>
<td>X</td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="key-concepts">Key Concepts</h3>
<ol>
<li>
<p><strong>Optimization</strong></p>
<ul>
<li><strong>AI Planning</strong>: Involves optimizing a sequence of actions to achieve a goal.</li>
<li><strong>RL</strong>: Focuses on optimizing policies to maximize cumulative rewards.</li>
<li><strong>IL</strong>: Also involves optimization, often by learning from demonstrations.</li>
</ul>
</li>
<li>
<p><strong>Learns from Experience</strong></p>
<ul>
<li><strong>SL</strong>: Learns from labeled data, where each input has a corresponding output.</li>
<li><strong>UL</strong>: Learns from unlabeled data by identifying patterns.</li>
<li><strong>RL</strong>: Learns by interacting with the environment and receiving feedback.</li>
<li><strong>IL</strong>: Learns from demonstrations of good policies.</li>
</ul>
</li>
<li>
<p><strong>Generalization</strong></p>
<ul>
<li>All methods (AI Planning, SL, UL, RL, IL) aim to generalize from training data to new, unseen situations.</li>
</ul>
</li>
<li>
<p><strong>Delayed Consequences</strong></p>
<ul>
<li><strong>AI Planning</strong>: Considers the long-term effects of actions.</li>
<li><strong>RL</strong>: Takes into account the delayed consequences of actions to maximize future rewards.</li>
<li><strong>IL</strong>: Can also consider delayed consequences if the demonstrations include long-term strategies.</li>
</ul>
</li>
<li>
<p><strong>Exploration</strong></p>
<ul>
<li><strong>RL</strong>: Requires exploration of the environment to discover optimal policies.</li>
<li>Other methods (SL, UL, IL) typically do not involve exploration in the same way.</li>
<li>In particular, IL is limited to the data and experiences provided by the expert model. Since IL relies on demonstrations, it cannot leverage the benefits of exploration to discover better strategies beyond what the expert has demonstrated. In contrast, RL has the capability to explore and learn from trial and error, allowing it to outperform expert demonstrations in some cases by discovering novel, more efficient policies.</li>
</ul>
</li>
</ol>
<h2 id="planning-vs-learning">Planning vs Learning</h2>
<p>Two fundamental problems in sequential decision making:</p>
<ol>
<li>
<p><strong>Reinforcement learning</strong>:</p>
<ul>
<li>The environment is initially <strong>unknown</strong></li>
<li>The agent <strong>interacts</strong> with the environment</li>
<li>The agent <strong>improves</strong> its policy</li>
</ul>
</li>
<li>
<p><strong>Planning</strong>:</p>
<ul>
<li>A model of the environment is <strong>known</strong></li>
<li>The agent performs computations with its model (w<strong>ithout any external
interaction</strong>)</li>
<li>The agent <strong>improves</strong> its policy, a.k.a. deliberation, reasoning, introspection, pondering, thought, search </li>
</ul>
</li>
</ol>
<h2 id="introduction-to-markov-decision-processes-mdps">Introduction to Markov Decision Processes (MDPs)</h2>
<p>Markov Decision Processes (MDPs) are a fundamental framework used in Reinforcement Learning to model decision-making problems. MDPs provide a mathematical foundation for describing an environment in which an agent interacts, takes actions, and receives rewards. Let's break down the components of an MDP.</p>
<h3 id="components-of-an-mdp">Components of an MDP</h3>
<p>An MDP is defined by the following components:</p>
<ol>
<li>
<p><strong>Set of States (<span class="arithmatex">\(S\)</span>)</strong>: </p>
<ul>
<li>The set of all possible states that the environment can be in. A state <span class="arithmatex">\(s \in S\)</span> represents a specific configuration or situation of the environment at a given time.</li>
</ul>
</li>
<li>
<p><strong>Set of Actions (<span class="arithmatex">\(A\)</span>)</strong>: </p>
<ul>
<li>The set of all possible actions that the agent can take. An action <span class="arithmatex">\(a \in A\)</span> is a decision made by the agent that affects the environment.</li>
</ul>
</li>
<li>
<p><strong>Transition Function (<span class="arithmatex">\(P(s' | s, a)\)</span>)</strong>:</p>
<ul>
<li>The transition function defines the probability of transitioning from state <span class="arithmatex">\(s\)</span> to state <span class="arithmatex">\(s'\)</span> when action <span class="arithmatex">\(a\)</span> is taken. This function captures the dynamics of the environment.</li>
<li><strong>Markov Property</strong>: The transition function satisfies the Markov property, which states that the future state <span class="arithmatex">\(s'\)</span> depends only on the current state <span class="arithmatex">\(s\)</span> and action <span class="arithmatex">\(a\)</span>, and not on the sequence of states and actions that preceded it. This is why the process is called "Markovian."</li>
</ul>
</li>
<li>
<p><strong>Reward Function (<span class="arithmatex">\(R(s, a, s')\)</span>)</strong>:</p>
<ul>
<li>The reward function specifies the immediate reward received by the agent after transitioning from state <span class="arithmatex">\(s\)</span> to state <span class="arithmatex">\(s'\)</span> by taking action <span class="arithmatex">\(a\)</span>. The reward is a scalar value that indicates the benefit or cost of the action.</li>
</ul>
</li>
<li>
<p><strong>Start State (<span class="arithmatex">\(s_0\)</span>)</strong>:</p>
<ul>
<li>The initial state from which the agent starts its interaction with the environment.</li>
</ul>
</li>
<li>
<p><strong>Discount Factor (<span class="arithmatex">\(\gamma\)</span>)</strong>:</p>
<ul>
<li>The discount factor <span class="arithmatex">\(\gamma\)</span> (where <span class="arithmatex">\(0 \leq \gamma \leq 1\)</span>) determines the present value of future rewards. A discount factor close to 1 makes the agent prioritize long-term rewards, while a discount factor close to 0 makes the agent focus on immediate rewards.</li>
</ul>
</li>
<li>
<p><strong>Horizon (<span class="arithmatex">\(H\)</span>)</strong>:</p>
<ul>
<li>The horizon <span class="arithmatex">\(H\)</span> represents the time horizon over which the agent interacts with the environment. It can be finite (fixed number of steps) or infinite (continuous interaction).</li>
</ul>
</li>
</ol>
<h3 id="episodes-and-environment-types">Episodes and Environment Types</h3>
<ul>
<li>An <strong>episode</strong> is a sequence of interactions that starts from an initial state and ends when a terminal condition is met.  </li>
<li><strong>Episodic Environments</strong>: The interaction consists of episodes, meaning the agent's experience is divided into separate episodes with a clear start and end (e.g., playing a game with levels or a robot completing a task like picking up an object).  </li>
<li><strong>Non-Episodic (Continuous) Environments</strong>: There is no clear termination, and the agent continuously interacts with the environment without resetting (e.g., stock market trading, autonomous vehicle navigation).  </li>
</ul>
<h3 id="difference-between-horizon-and-episode">Difference Between Horizon and Episode</h3>
<ul>
<li>The <strong>episode</strong> refers to a full sequence of interactions that has a clear beginning and an end.  </li>
<li>The <strong>horizon (<span class="arithmatex">\(H\)</span>)</strong> defines the length of time the agent considers while making decisions, which can be within a single episode (in episodic environments) or over an ongoing interaction (in non-episodic environments).  </li>
<li>In <strong>finite-horizon episodic tasks</strong>, the episode length is usually equal to the horizon. However, in infinite-horizon tasks, the agent keeps interacting with the environment indefinitely.</li>
</ul>
<h3 id="the-goal-of-reinforcement-learning">The Goal of Reinforcement Learning</h3>
<p>The goal of RL in the context of an MDP is to find a policy <span class="arithmatex">\(\pi\)</span> that maximizes the expected cumulative reward over time. The policy <span class="arithmatex">\(\pi\)</span> is a function that maps states to actions, and it can be deterministic or stochastic.</p>
<h4 id="objective">Objective:</h4>
<div class="arithmatex">\[
\max_{\pi} \mathbb{E}\left[\sum_{t=0}^{H} \gamma^{t} R(S_t, A_t, S_{t+1}) \mid \pi\right]
\]</div>
<ul>
<li>
<p><strong>Expected Value</strong>: The use of the expected value <span class="arithmatex">\(\mathbb{E}\)</span> is necessary because the transition function <span class="arithmatex">\(P(s' | s, a)\)</span> is probabilistic. The agent does not know exactly which state <span class="arithmatex">\(s'\)</span> it will end up in after taking action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span>. Therefore, the agent must consider the expected reward over all possible future states, weighted by their probabilities.</p>
</li>
<li>
<p><strong>Cumulative Reward</strong>: The agent aims to maximize the sum of discounted rewards over time. The discount factor <span class="arithmatex">\(\gamma\)</span> ensures that the agent balances immediate rewards with future rewards. A higher <span class="arithmatex">\(\gamma\)</span> makes the agent more farsighted, while a lower <span class="arithmatex">\(\gamma\)</span> makes it more shortsighted.</p>
</li>
</ul>
<h3 id="example-grid-world">Example: Grid World</h3>
<p>Consider a simple grid world where the agent must navigate from a start state to a goal state while avoiding obstacles. The states <span class="arithmatex">\(S\)</span> are the grid cells, the actions <span class="arithmatex">\(A\)</span> are movements (up, down, left, right), and the reward function <span class="arithmatex">\(R(s, a, s')\)</span> provides positive rewards for reaching the goal and negative rewards for hitting obstacles. The transition function <span class="arithmatex">\(P(s' | s, a)\)</span> defines the probability of actually moving to a neighboring cell when an action is taken.</p>
<center> 
<img src="\assets\images\course_notes\intro-to-rl\gridworld.png"
     alt=""
     style="float: center; margin-right: 10px;" 
     /> 

   Fig3. Grid World Game </center>

<h3 id="an-example-of-gridworld">An Example of Gridworld</h3>
<p>In the provided Gridworld example, the agent starts from the yellow square and has to navigate to a goal while avoiding the cliff. The rewards are:
- <strong>+1 for reaching the close exit</strong>
- <strong>+10 for reaching the distant exit</strong>
- <strong>-10 penalty for stepping into the cliff</strong> (red squares)</p>
<center> 
<img src="\assets\images\course_notes\intro-to-rl\gridworld_example.png"
     alt=""
     style="float: center; margin-right: 10px;" 
     /> 

   Fig4. Grid World Example </center>

<p>The agent's choice depends on:
- The <strong>discount factor (<span class="arithmatex">\(\gamma\)</span>)</strong>, which determines whether it prioritizes short-term or long-term rewards.
- The <strong>noise level</strong>, which introduces randomness into actions.</p>
<p>Depending on the values of <span class="arithmatex">\(\gamma\)</span> and noise, the agent's behavior varies:
1. <strong><span class="arithmatex">\(\gamma\)</span> = 0.1, noise = 0.5:</strong><br />
   - The agent <strong>prefers the close exit (+1) but takes the risk of stepping into the cliff (-10).</strong><br />
2. <strong><span class="arithmatex">\(\gamma\)</span> = 0.99, noise = 0:</strong><br />
   - The agent <strong>prefers the distant exit (+10) while avoiding the cliff (-10).</strong><br />
3. <strong><span class="arithmatex">\(\gamma\)</span> = 0.99, noise = 0.5:</strong><br />
   - The agent <strong>still prefers the distant exit (+10), but due to noise, it risks the cliff (-10).</strong><br />
4. <strong><span class="arithmatex">\(\gamma\)</span> = 0.1, noise = 0:</strong><br />
   - The agent <strong>chooses the close exit (+1) while avoiding the cliff.</strong>  </p>
<h3 id="stochastic-policy">Stochastic Policy</h3>
<p>Another source of randomness in MDPs comes from <strong>stochastic policies</strong>. Unlike the transition function <span class="arithmatex">\(P(s' | s, a)\)</span>, which describes the environmentâ€™s inherent randomness in executing actions, a <strong>stochastic policy</strong> <span class="arithmatex">\(\pi(a | s)\)</span> defines the probability of selecting an action <span class="arithmatex">\(a\)</span> when in state <span class="arithmatex">\(s\)</span>. This means that even if the environment were fully deterministic, the agent itself may act probabilistically.</p>
<h4 id="example-gridworld-with-a-stochastic-policy"><strong>Example: Gridworld with a Stochastic Policy</strong></h4>
<p>Consider a modified version of the previous Gridworld example. Instead of always choosing the action with the highest expected return, the agent follows a <strong>stochastic policy</strong> where it selects each possible action with a certain probability:</p>
<ul>
<li>With <strong>99% probability</strong>, the agent follows its optimal policy.</li>
<li>With <strong>1% probability</strong>, it selects a random action.</li>
</ul>
<p><strong>Transition Probability <span class="arithmatex">\(P(s'|s, a)\)</span></strong>  is the probability that taking action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span> results in transitioning to state <span class="arithmatex">\(s'\)</span>. This is determined by the <strong>environment</strong>. If the environment is slippery, moving "right" from <span class="arithmatex">\((2,2)\)</span> may lead to <span class="arithmatex">\((2,3)\)</span> with 80% probability, but also to <span class="arithmatex">\((3,2)\)</span> with 20%.
<strong>Stochastic Policy <span class="arithmatex">\(\pi(a | s)\)</span></strong> determines the probability that the agent <strong>chooses</strong> action <span class="arithmatex">\(a\)</span> when in state <span class="arithmatex">\(s\)</span>. This is determined by the <strong>agent's strategy</strong>.</p>
<ul>
<li>If the policy <span class="arithmatex">\(\pi(a | s)\)</span> is deterministic, the agent <strong>always selects the same action</strong> in a given state.</li>
<li>If the policy <span class="arithmatex">\(\pi(a | s)\)</span> is stochastic, the agent <strong>introduces randomness in its decision-making process</strong>, which can be beneficial in <strong>exploration</strong> and <strong>avoiding local optima</strong>.</li>
</ul>
<h3 id="graphical-model-of-mdps">Graphical Model of MDPs</h3>
<p>MDPs can be represented graphically as a sequence of <strong>states (<span class="arithmatex">\(\mathbf{s}\)</span>)</strong>, <strong>actions (<span class="arithmatex">\(\mathbf{a}\)</span>)</strong>, and <strong>transitions (<span class="arithmatex">\(p\)</span>)</strong>:</p>
<ul>
<li>The agent starts at state <span class="arithmatex">\(\mathbf{s}_1\)</span>.</li>
<li>It selects an action <span class="arithmatex">\(\mathbf{a}_1\)</span>, which moves it to <span class="arithmatex">\(\mathbf{s}_2\)</span> based on the probability <span class="arithmatex">\(p(\mathbf{s}_2 | \mathbf{s}_1, \mathbf{a}_1)\)</span>.</li>
<li>The process continues, forming a <strong>decision-making chain</strong> where each action influences future states and rewards.</li>
</ul>
<center> 
<img src="\assets\images\course_notes\intro-to-rl\graphical_MDP.png"
     alt=""
     style="float: center; margin-right: 10px;" 
     /> 

   Fig5. Graphical Model of MDPs </center>

<h2 id="partially-observable-mdps-pomdps">Partially Observable MDPs (POMDPs)</h2>
<p>In real-world scenarios, the agent may not have full visibility of the environment, leading to <strong>Partially Observable MDPs (POMDPs)</strong>.
- <strong>Hidden states:</strong> The true state <span class="arithmatex">\(\mathbf{s}_t\)</span> is not fully known to the agent.
- <strong>Observations (<span class="arithmatex">\(O\)</span>):</strong> Instead of directly knowing <span class="arithmatex">\(\mathbf{s}_t\)</span>, the agent receives a noisy or incomplete observation <span class="arithmatex">\(\mathbf{o}_t\)</span>.
- <strong>Decision-making challenge:</strong> The agent must infer the state from past observations and actions.</p>
<center> 
<img src="\assets\images\course_notes\intro-to-rl\POMDP.png"
     alt=""
     style="float: center; margin-right: 10px;" 
     /> 

   Fig6. Partially Observable MDPs (POMDPs) </center>

<h3 id="policy-as-a-function-of-state-mathbfs_t-or-observation-mathbfo_t">Policy as a Function of State (<span class="arithmatex">\(\mathbf{s}_t\)</span>) or Observation (<span class="arithmatex">\(\mathbf{o}_t\)</span>)</h3>
<ul>
<li><strong>Fully Observed Policy</strong>: When the agent has access to the full state <span class="arithmatex">\(\mathbf{s}_t\)</span>, the policy is denoted as <span class="arithmatex">\(\pi_{\theta}(\mathbf{a}_t | \mathbf{s}_t)\)</span>. This means the action <span class="arithmatex">\(\mathbf{a}_t\)</span> is chosen based on the current state <span class="arithmatex">\(\mathbf{s}_t\)</span>.</li>
<li>
<p><strong>Partially Observed Policy</strong>: When the agent only has access to observations <span class="arithmatex">\(\mathbf{o}_t\)</span>, the policy is denoted as <span class="arithmatex">\(\pi_{\theta}(\mathbf{a}_t | \mathbf{o}_t)\)</span>. This means the action <span class="arithmatex">\(\mathbf{a}_t\)</span> is chosen based on the current observation <span class="arithmatex">\(\mathbf{o}_t\)</span>.</p>
<ul>
<li>
<p><strong>Observation</strong>: At each time step, the agent receives an observation <span class="arithmatex">\(\mathbf{o}_t\)</span> that provides partial information about the current state <span class="arithmatex">\(\mathbf{s}_t\)</span>. For example, <span class="arithmatex">\(\mathbf{o}_1\)</span> is the observation corresponding to state <span class="arithmatex">\(\mathbf{s}_1\)</span>.</p>
</li>
<li>
<p><strong>Policy</strong>: The policy <span class="arithmatex">\(\pi_{\theta}\)</span> maps observations to actions. For instance, <span class="arithmatex">\(\pi_{\theta}(\mathbf{a}_1 | \mathbf{o}_1)\)</span> determines the action <span class="arithmatex">\(\mathbf{a}_1\)</span> based on the observation <span class="arithmatex">\(\mathbf{o}_1\)</span>.</p>
</li>
</ul>
</li>
</ul>
<center> 
<img src="\assets\images\course_notes\intro-to-rl\policy_POMDP.png"
     alt=""
     style="float: center; margin-right: 10px;" 
     /> 

   Fig7. POMDP Policy </center>

<h2 id="utility-function-in-reinforcement-learning">Utility Function in Reinforcement Learning</h2>
<p>In Reinforcement Learning, the <strong>utility function</strong> plays a central role in evaluating the long-term desirability of states or state-action pairs. The utility function quantifies the expected cumulative reward that an agent can achieve from a given state or state-action pair, following a specific policy. Let's explore the concept of the utility function and its mathematical formulation.</p>
<h3 id="definition-of-utility-function">Definition of Utility Function</h3>
<p>The utility function measures the expected cumulative reward that an agent can accumulate over time, starting from a particular state or state-action pair, and following a given policy. There are two main types of utility functions:</p>
<ol>
<li>
<p><strong>State Value Function (<span class="arithmatex">\(V^{\pi}(s)\)</span>)</strong>:</p>
<ul>
<li>The state value function <span class="arithmatex">\(V^{\pi}(s)\)</span> represents the expected cumulative reward when starting from state <span class="arithmatex">\(s\)</span> and following policy <span class="arithmatex">\(\pi\)</span> thereafter.</li>
<li>Mathematically, it is defined as:
    $<span class="arithmatex">\(V^{\pi}(s) = \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t R(S_t, A_t, S_{t+1}) \mid S_0 = s, \pi \right]\)</span>$</li>
<li>Here, <span class="arithmatex">\(\gamma\)</span> is the discount factor, and <span class="arithmatex">\(R(S_t, A_t, S_{t+1})\)</span> is the reward received at time <span class="arithmatex">\(t\)</span>.</li>
</ul>
</li>
<li>
<p><strong>Action-Value Function (<span class="arithmatex">\(Q^{\pi}(s, a)\)</span>)</strong>:</p>
<ul>
<li>The action-value function <span class="arithmatex">\(Q^{\pi}(s, a)\)</span> represents the expected cumulative reward when starting from state <span class="arithmatex">\(s\)</span>, taking action <span class="arithmatex">\(a\)</span>, and following policy <span class="arithmatex">\(\pi\)</span> thereafter.</li>
<li>Mathematically, it is defined as:
    $<span class="arithmatex">\(Q^{\pi}(s, a) = \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t R(S_t, A_t, S_{t+1}) \mid S_0 = s, A_0 = a, \pi \right]\)</span>$</li>
</ul>
</li>
</ol>
<h3 id="why-is-the-utility-function-important">Why is the Utility Function Important?</h3>
<p>The utility function is crucial for several reasons:</p>
<ul>
<li><strong>Policy Evaluation</strong>: It allows the agent to evaluate how good a particular policy <span class="arithmatex">\(\pi\)</span> is by estimating the expected cumulative reward for each state or state-action pair.</li>
<li><strong>Policy Improvement</strong>: By comparing the utility of different actions, the agent can improve its policy by choosing actions that lead to higher cumulative rewards.</li>
<li><strong>Optimal Policy</strong>: The ultimate goal of RL is to find the optimal policy <span class="arithmatex">\(\pi^*\)</span> that maximizes the utility function for all states or state-action pairs.</li>
</ul>
<h3 id="bellman-equation-for-utility-functions">Bellman Equation for Utility Functions</h3>
<p>The utility functions satisfy the <strong>Bellman equation</strong>, which provides a recursive relationship between the value of a state (or state-action pair) and the values of its successor states. The Bellman equation is fundamental for solving MDPs and is used in many RL algorithms.</p>
<ol>
<li><strong>Bellman Equation for State Value Function</strong>:</li>
</ol>
<div class="arithmatex">\[V^{\pi}(s) = \sum_{a} \pi(a | s) \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma V^{\pi}(s') \right]\]</div>
<ul>
<li>
<p>This equation states that the value of a state <span class="arithmatex">\(s\)</span> under policy <span class="arithmatex">\(\pi\)</span> is the expected immediate reward plus the discounted value of the next state <span class="arithmatex">\(s'\)</span>.</p>
</li>
<li>
<p><strong>Bellman Equation for Action-Value Function</strong>:</p>
</li>
</ul>
<div class="arithmatex">\[Q^{\pi}(s, a) = \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma \sum_{a'} \pi(a' | s') Q^{\pi}(s', a') \right]\]</div>
<ul>
<li>This equation states that the value of taking action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span> under policy <span class="arithmatex">\(\pi\)</span> is the expected immediate reward plus the discounted value of the next state <span class="arithmatex">\(s'\)</span> and action <span class="arithmatex">\(a'\)</span>.</li>
</ul>
<h3 id="example-grid-world_1">Example: Grid World</h3>
<p>Consider the Grid World example where the agent navigates to a goal while avoiding obstacles. The utility function helps the agent evaluate the long-term desirability of each cell (state) in the grid:</p>
<ul>
<li><strong>State Value Function (<span class="arithmatex">\(V^{\pi}(s)\)</span>)</strong>: The agent calculates the expected cumulative reward for each cell, considering the rewards for reaching the goal and penalties for hitting obstacles.</li>
<li><strong>Action-Value Function (<span class="arithmatex">\(Q^{\pi}(s, a)\)</span>)</strong>: The agent evaluates the expected cumulative reward for each possible action (up, down, left, right) in each cell, helping it decide the best action to take.</li>
</ul>
<center> 
<img src="\assets\images\course_notes\intro-to-rl\gridworld_V.png"
     alt=""
     style="float: center; margin-right: 10px;" 
     /> 

   Fig8. An example of estimated $V^{\pi}(s)$ values in grid world  </center>

<center> 
<img src="\assets\images\course_notes\intro-to-rl\gridworld_Q.png"
     alt=""
     style="float: center; margin-right: 10px;" 
     /> 

   Fig9. An example of estimated $Q^{\pi}(s, a)$ values in grid world </center>

<h2 id="authors">Author(s)</h2>
<div class="grid cards">
<ul>
<li><img align="left" alt="Instructor Avatar" src="/assets/images/staff/Masoud-Tahmasbi.jpg" width="150" />
    <span class="description">
        <p><strong>Masoud Tahmasbi</strong></p>
        <p>Teaching Assistant</p>
        <p><a href="mailto:masoudtahmasbifard@gmail.com">masoudtahmasbifard@gmail.com</a></p>
        <p>
        <a href="https://scholar.google.com/citations?hl=en&amp;user=BUiXXIYAAAAJ" target="_blank"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M390.9 298.5s0 .1.1.1c9.2 19.4 14.4 41.1 14.4 64C405.3 445.1 338.5 512 256 512s-149.3-66.9-149.3-149.3c0-22.9 5.2-44.6 14.4-64 1.7-3.6 3.6-7.2 5.6-10.7q6.6-11.4 15-21.3c27.4-32.6 68.5-53.3 114.4-53.3 33.6 0 64.6 11.1 89.6 29.9 9.1 6.9 17.4 14.7 24.8 23.5 5.6 6.6 10.6 13.8 15 21.3 2 3.4 3.8 7 5.5 10.5zm26.4-18.8c-30.1-58.4-91-98.4-161.3-98.4s-131.2 40-161.3 98.4L0 202.7 256 0l256 202.7-94.7 77.1z"/></svg></span></a>
        <a href="https://github.com/masoudtfard" target="_blank"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg></span></a>
        <a href="https://www.linkedin.com/in/masoud-tahmasbi-fard/" target="_blank"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M100.3 448H7.4V148.9h92.9zM53.8 108.1C24.1 108.1 0 83.5 0 53.8c0-14.3 5.7-27.9 15.8-38S39.6 0 53.8 0s27.9 5.7 38 15.8 15.8 23.8 15.8 38c0 29.7-24.1 54.3-53.8 54.3M447.9 448h-92.7V302.4c0-34.7-.7-79.2-48.3-79.2-48.3 0-55.7 37.7-55.7 76.7V448h-92.8V148.9h89.1v40.8h1.3c12.4-23.5 42.7-48.3 87.9-48.3 94 0 111.3 61.9 111.3 142.3V448z"/></svg></span></a>
        </p>
    </span></li>
</ul>
</div>







  
  



  



  <form class="md-feedback" name="feedback" hidden>
    <fieldset>
      <legend class="md-feedback__title">
        Was this page helpful?
      </legend>
      <div class="md-feedback__inner">
        <div class="md-feedback__list">
          
            <button class="md-feedback__icon md-icon" type="submit" title="This page was helpful" data-md-value="1">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m7 0c0 .8-.7 1.5-1.5 1.5S14 10.3 14 9.5 14.7 8 15.5 8s1.5.7 1.5 1.5m-5 7.73c-1.75 0-3.29-.73-4.19-1.81L9.23 14c.45.72 1.52 1.23 2.77 1.23s2.32-.51 2.77-1.23l1.42 1.42c-.9 1.08-2.44 1.81-4.19 1.81"/></svg>
            </button>
          
            <button class="md-feedback__icon md-icon" type="submit" title="This page could be improved" data-md-value="0">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10m-6.5-4c.8 0 1.5.7 1.5 1.5s-.7 1.5-1.5 1.5-1.5-.7-1.5-1.5.7-1.5 1.5-1.5M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m2 4.5c1.75 0 3.29.72 4.19 1.81l-1.42 1.42C14.32 16.5 13.25 16 12 16s-2.32.5-2.77 1.23l-1.42-1.42C8.71 14.72 10.25 14 12 14"/></svg>
            </button>
          
        </div>
        <div class="md-feedback__note">
          
            <div data-md-value="1" hidden>
              
              
                
              
              
              
                
                
              
              Thanks for your feedback!
            </div>
          
            <div data-md-value="0" hidden>
              
              
                
              
              
              
                
                
              
              Thanks for your feedback! Help us improve this page by using our <a href="..." target="_blank" rel="noopener">feedback form</a>.
            </div>
          
        </div>
      </div>
    </fieldset>
  </form>


<script src="https://giscus.app/client.js"
        data-repo="DeepRLCourse/DeepRLCourse.github.io"
        data-repo-id="R_kgDOMPOFbQ"
        data-category="Announcements"
        data-category-id="DIC_kwDOMPOFbc4CmGY6"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="1"
        data-input-position="top"
        data-theme="preferred_color_scheme"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>

                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Made with â¤ï¸ in Robust and Interpretable Machine Learning Lab
    </div>
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.top", "toc.integrate"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>