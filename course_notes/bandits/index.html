
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="This page provides an in-depth exploration of the Multi-Armed Bandit (MAB) problem, a foundational concept in reinforcement learning and decision-making under uncertainty. It covers the theoretical framework, key algorithms, and practical applications of MABs, including strategies for balancing exploration and exploitation. Topics include action-value estimation, regret analysis, and advanced methods like UCB, Thompson Sampling, and contextual bandits. The content is designed to equip readers with both theoretical insights and practical tools for solving MAB problems in diverse real-world scenarios.">
      
      
      
        <link rel="canonical" href="https://DeepRL2026.github.io/course_notes/bandits/">
      
      
      
      
        
      
      
      <link rel="icon" href="../../assets/favicon/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.3">
    
    
      
        <title>Week 6: Multi-Armed Bandits - Deep RL Course</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300,300i,400,400i,700,700i%7CRed+Hat+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Ubuntu";--md-code-font:"Red Hat Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
      <link rel="stylesheet" href="../../stylesheets/vazirmatn.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-Q1YT8K39WE"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-Q1YT8K39WE",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-Q1YT8K39WE",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
  
<meta property="og:type" content="website" />
<meta property="og:title" content="Week 6: Multi-Armed Bandits - Deep RL Course" />
<meta property="og:description" content="This page provides an in-depth exploration of the Multi-Armed Bandit (MAB) problem, a foundational concept in reinforcement learning and decision-making under uncertainty. It covers the theoretical framework, key algorithms, and practical applications of MABs, including strategies for balancing exploration and exploitation. Topics include action-value estimation, regret analysis, and advanced methods like UCB, Thompson Sampling, and contextual bandits. The content is designed to equip readers with both theoretical insights and practical tools for solving MAB problems in diverse real-world scenarios." />
<meta property="og:image" content="https://DeepRL2026.github.io/assets/images/social/course_notes/bandits.png" />
<meta property="og:image:type" content="image/png" />
<meta property="og:image:width" content="1200" />
<meta property="og:image:height" content="630" />
<meta property="og:url" content="https://DeepRL2026.github.io/course_notes/bandits/" />
<meta property="twitter:card" content="summary_large_image" />
<meta property="twitter:title" content="Week 6: Multi-Armed Bandits - Deep RL Course" />
<meta property="twitter:description" content="This page provides an in-depth exploration of the Multi-Armed Bandit (MAB) problem, a foundational concept in reinforcement learning and decision-making under uncertainty. It covers the theoretical framework, key algorithms, and practical applications of MABs, including strategies for balancing exploration and exploitation. Topics include action-value estimation, regret analysis, and advanced methods like UCB, Thompson Sampling, and contextual bandits. The content is designed to equip readers with both theoretical insights and practical tools for solving MAB problems in diverse real-world scenarios." />
<meta property="twitter:image" content="https://DeepRL2026.github.io/assets/images/social/course_notes/bandits.png" />
</head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#week-6-multi-armed-bandits" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Deep RL Course" class="md-header__button md-logo" aria-label="Deep RL Course" data-md-component="logo">
      
  <img src="../../assets/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Deep RL Course
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Week 6: Multi-Armed Bandits
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="red"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="red"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  Home

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  Course Notes

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../lectures/lecture1/" class="md-tabs__link">
          
  
  
  Lectures

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  TA Sessions

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  Homeworks

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  
  Projects

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../exams/" class="md-tabs__link">
          
  
  
  Exams

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../resources/" class="md-tabs__link">
          
  
  
  Resources

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
    
      

    

    

        

            
                  
            

        

            

        
    

    


    <!-- Navigation -->
    
        
        <div
            class="md-sidebar md-sidebar--primary"
            data-md-component="sidebar"
            data-md-type="navigation"
            
        >
            <div class="md-sidebar__scrollwrap">
            <div class="md-sidebar__inner">
                <!--
  Copyright (c) 2016-2025 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->



<!-- Determine classes -->


  


  


<!-- Navigation -->
<nav
  class="md-nav md-nav--primary md-nav--lifted"
  aria-label="Navigation"
  data-md-level="0"
>

  <!-- Site title -->
  <label class="md-nav__title" for="__drawer">
    <a
      href="../.."
      title="Deep RL Course"
      class="md-nav__button md-logo"
      aria-label="Deep RL Course"
      data-md-component="logo"
    >
      
  <img src="../../assets/logo.png" alt="logo">

    </a>
    Deep RL Course
  </label>

  <!-- Repository information -->
  

  <!-- Navigation list -->
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Home
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Course Notes
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Course Notes
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Lectures
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Lectures
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../lectures/lecture1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Lecture1
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../lectures/lecture2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Lecture2
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    TA Sessions
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    TA Sessions
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Homeworks
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    Homeworks
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Projects
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    Projects
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Welcome
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Exams
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            
  
    Exams
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../exams/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Previous Semesters
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Resources
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            
  
    Resources
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../resources/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Resources
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
            </div>
            </div>
        </div>
    

    <!-- Table of contents -->
    
        
        <div
            class="md-sidebar md-sidebar--secondary"
            data-md-component="sidebar"
            data-md-type="toc"
            
        >
            <div class="md-sidebar__scrollwrap">
            <div class="md-sidebar__inner">
                <!--
  Copyright (c) 2016-2025 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->

<!-- Determine title -->




<!-- Table of contents -->
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  

  <!--
    Check whether the content starts with a level 1 headline. If it does, the
    top-level anchor must be skipped, since it would be redundant to the link
    to the current page that is located just above the anchor. Therefore we
    directly continue with the children of the anchor.
  -->
  
  
    
  

  <!-- Table of contents title and list -->
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      
        Introduction
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#formal-problem-statement" class="md-nav__link">
    <span class="md-ellipsis">
      
        Formal Problem Statement
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Formal Problem Statement">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#action-value-functions-q-values" class="md-nav__link">
    <span class="md-ellipsis">
      
        Action-Value Functions (Q-values)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimal-action-and-optimal-value" class="md-nav__link">
    <span class="md-ellipsis">
      
        Optimal Action and Optimal Value
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exploration-vs-exploitation-core-difficulty" class="md-nav__link">
    <span class="md-ellipsis">
      
        Exploration vs. Exploitation: Core Difficulty
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#non-associativity-property" class="md-nav__link">
    <span class="md-ellipsis">
      
        Non-Associativity Property
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Non-Associativity Property">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#real-world-applications" class="md-nav__link">
    <span class="md-ellipsis">
      
        Real-World Applications
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#action-value-methods-and-types" class="md-nav__link">
    <span class="md-ellipsis">
      
        Action-Value Methods and Types
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Action-Value Methods and Types">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sample-average-estimation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Sample-Average Estimation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#incremental-update-rule-for-efficient-computation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Incremental Update Rule for Efficient Computation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#constant-step-size-update-for-nonstationary-problems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Constant Step-Size Update for Nonstationary Problems
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regret-measuring-suboptimality" class="md-nav__link">
    <span class="md-ellipsis">
      
        Regret: Measuring Suboptimality
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Regret: Measuring Suboptimality">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#concept-of-regret" class="md-nav__link">
    <span class="md-ellipsis">
      
        Concept of Regret
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#total-regret" class="md-nav__link">
    <span class="md-ellipsis">
      
        Total Regret
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-regret-gap-and-action-counts" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Regret, Gap, and Action Counts
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regret-dynamics-and-algorithmic-insights" class="md-nav__link">
    <span class="md-ellipsis">
      
        Regret Dynamics and Algorithmic Insights
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lower-bound-on-regret-lai-robbins-bound-this-topic-is-beyond-the-scope-of-this-course" class="md-nav__link">
    <span class="md-ellipsis">
      
        Lower Bound on Regret (Lai-Robbins Bound) (This topic is beyond the scope of this course.)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bernoulli-bandit-case" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bernoulli Bandit Case
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#problem-dependent-versus-minimax-regret" class="md-nav__link">
    <span class="md-ellipsis">
      
        Problem-Dependent versus Minimax Regret
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-explorationexploitation-trade-off" class="md-nav__link">
    <span class="md-ellipsis">
      
        The Exploration–Exploitation Trade-off
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Exploration–Exploitation Trade-off">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#formal-definition-and-intuition" class="md-nav__link">
    <span class="md-ellipsis">
      
        Formal Definition and Intuition
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#risks-of-pure-exploitation-and-exploration-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      
        Risks of Pure Exploitation and Exploration Strategies
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regret-minimization-and-the-concept-of-optimism" class="md-nav__link">
    <span class="md-ellipsis">
      
        Regret Minimization and the Concept of Optimism
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#common-approaches-to-balancing-exploration-and-exploitation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Common Approaches to Balancing Exploration and Exploitation
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Common Approaches to Balancing Exploration and Exploitation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-epsilon-greedy-strategy" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. \(\epsilon\)-Greedy Strategy
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-optimistic-initial-values" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Optimistic Initial Values
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-upper-confidence-bound-ucb-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Upper Confidence Bound (UCB) Algorithms
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-thompson-sampling-bayesian-probability-matching" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Thompson Sampling (Bayesian Probability Matching)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#exploration-strategies-for-multi-armed-bandits" class="md-nav__link">
    <span class="md-ellipsis">
      
        Exploration Strategies for Multi-Armed Bandits
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Exploration Strategies for Multi-Armed Bandits">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-epsilon-greedy-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      
        The \(\epsilon\)-Greedy Algorithm
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The \(\epsilon\)-Greedy Algorithm">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#overview-and-motivation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Overview and Motivation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#formal-definition" class="md-nav__link">
    <span class="md-ellipsis">
      
        Formal Definition
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regret-analysis-with-constant-epsilon" class="md-nav__link">
    <span class="md-ellipsis">
      
        Regret Analysis with Constant \(\epsilon\)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convergence-of-hatq_t-estimates" class="md-nav__link">
    <span class="md-ellipsis">
      
        Convergence of \(\hat{Q}_t\) Estimates
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#decaying-epsilon_t-and-sublinear-regret" class="md-nav__link">
    <span class="md-ellipsis">
      
        Decaying \(\epsilon_t\) and Sublinear Regret
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimistic-initial-values" class="md-nav__link">
    <span class="md-ellipsis">
      
        Optimistic Initial Values
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Optimistic Initial Values">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#motivating-intuition" class="md-nav__link">
    <span class="md-ellipsis">
      
        Motivating Intuition
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#formal-definition-and-mathematical-formulation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Formal Definition and Mathematical Formulation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#behavioral-dynamics" class="md-nav__link">
    <span class="md-ellipsis">
      
        Behavioral Dynamics
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#comparison-to-epsilon-greedy" class="md-nav__link">
    <span class="md-ellipsis">
      
        Comparison to \(\epsilon\)-Greedy
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regret-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      
        Regret Analysis
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#upper-confidence-bound-ucb-algorithms-a-detailed-exploration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Upper Confidence Bound (UCB) Algorithms: A Detailed Exploration
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Upper Confidence Bound (UCB) Algorithms: A Detailed Exploration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#introduction-and-motivation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Introduction and Motivation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#problem-setup" class="md-nav__link">
    <span class="md-ellipsis">
      
        Problem Setup
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimism-in-the-face-of-uncertainty" class="md-nav__link">
    <span class="md-ellipsis">
      
        Optimism in the Face of Uncertainty
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#derivation-via-hoeffdings-inequality" class="md-nav__link">
    <span class="md-ellipsis">
      
        Derivation via Hoeffding's Inequality
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Derivation via Hoeffding&#39;s Inequality">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hoeffdings-inequality" class="md-nav__link">
    <span class="md-ellipsis">
      
        Hoeffding’s Inequality:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-ucb1-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      
        The UCB1 Algorithm
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interpretation-and-intuition" class="md-nav__link">
    <span class="md-ellipsis">
      
        Interpretation and Intuition
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regret-analysis-of-ucb1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Regret Analysis of UCB1
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#thompson-sampling-bayesian-probability-matching" class="md-nav__link">
    <span class="md-ellipsis">
      
        Thompson Sampling (Bayesian Probability Matching)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Thompson Sampling (Bayesian Probability Matching)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bayesian-framework" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bayesian Framework
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bernoulli-bandits-beta-bernoulli-model" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bernoulli Bandits: Beta-Bernoulli Model
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#algorithm-steps" class="md-nav__link">
    <span class="md-ellipsis">
      
        Algorithm Steps
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#extension-beyond-bernoulli-rewards" class="md-nav__link">
    <span class="md-ellipsis">
      
        Extension Beyond Bernoulli Rewards
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regret-analysis_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Regret Analysis
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regret-definition" class="md-nav__link">
    <span class="md-ellipsis">
      
        Regret Definition
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contextual-bandits" class="md-nav__link">
    <span class="md-ellipsis">
      
        Contextual Bandits
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Contextual Bandits">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#motivation-and-setup" class="md-nav__link">
    <span class="md-ellipsis">
      
        Motivation and Setup
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distinction-from-standard-mab" class="md-nav__link">
    <span class="md-ellipsis">
      
        Distinction from Standard MAB
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-use-cases" class="md-nav__link">
    <span class="md-ellipsis">
      
        Example Use Cases
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#linucb-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      
        LinUCB Algorithm
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="LinUCB Algorithm">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#linear-contextual-model" class="md-nav__link">
    <span class="md-ellipsis">
      
        Linear Contextual Model
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#algorithm-structure" class="md-nav__link">
    <span class="md-ellipsis">
      
        Algorithm Structure
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#usage-of-linucb-in-contextual-bandits" class="md-nav__link">
    <span class="md-ellipsis">
      
        Usage of LinUCB in Contextual Bandits
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regret-analysis-of-linucb" class="md-nav__link">
    <span class="md-ellipsis">
      
        Regret Analysis of LinUCB
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#thompson-sampling-in-contextual-bandits" class="md-nav__link">
    <span class="md-ellipsis">
      
        Thompson Sampling in Contextual Bandits
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Thompson Sampling in Contextual Bandits">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    <span class="md-ellipsis">
      
        Overview
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#usage-of-thompson-sampling-algorithm-in-contextual-bandits" class="md-nav__link">
    <span class="md-ellipsis">
      
        Usage of Thompson Sampling Algorithm in Contextual Bandits
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regret-analysis-of-thompson-sampling-in-contextual-bandits" class="md-nav__link">
    <span class="md-ellipsis">
      
        Regret Analysis of Thompson Sampling in Contextual Bandits
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
            </div>
            </div>
        </div>
    

          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="week-6-multi-armed-bandits">Week 6: Multi-Armed Bandits</h1>
<h2 id="introduction">Introduction</h2>
<p>The <strong>multi-armed bandit (MAB)</strong> problem represents one of the simplest yet profoundly insightful frameworks for analyzing the fundamental dilemma known as the <strong>exploration-exploitation tradeoff</strong> in decision-making under uncertainty. This tradeoff arises naturally whenever an agent faces multiple choices whose outcomes are uncertain, requiring it to continually balance between <strong>exploring</strong> unknown actions to discover their potential rewards and <strong>exploiting</strong> known actions to maximize immediate returns. The elegance and simplicity of the MAB setup enable rigorous theoretical analysis while maintaining relevance to numerous practical scenarios.</p>
<h2 id="formal-problem-statement">Formal Problem Statement</h2>
<p>Formally, a multi-armed bandit problem can be modeled as a simplified form of a Markov Decision Process (MDP) characterized solely by an <strong>action set</strong> and <strong>reward functions</strong>, without state dynamics. Specifically, the bandit setup is represented by the tuple <span class="arithmatex">\((\mathcal{A}, \mathcal{R})\)</span>, where:</p>
<ul>
<li>
<p><strong>Action Set:</strong> <span class="arithmatex">\(\mathcal{A}\)</span> is a finite set of discrete actions, often referred to as "bandit arms," indexed by <span class="arithmatex">\(a = 1, 2, \dots, k\)</span>. Here, <span class="arithmatex">\(k\)</span> denotes the total number of available actions.</p>
</li>
<li>
<p><strong>Reward Distributions:</strong> Each action <span class="arithmatex">\(a \in \mathcal{A}\)</span> is associated with a distinct probability distribution over rewards, denoted by <span class="arithmatex">\(\mathcal{R}^a\)</span>. Formally, the reward obtained from action <span class="arithmatex">\(a\)</span> at time step <span class="arithmatex">\(t\)</span>, represented as <span class="arithmatex">\(R_t\)</span>, is sampled from this distribution:</p>
</li>
</ul>
<div class="arithmatex">\[
R_t \sim \mathcal{R}^{A_t}, \quad \text{where } A_t \in \mathcal{A}
\]</div>
<p>This means the reward for choosing action <span class="arithmatex">\(a\)</span> is a random variable with a specific but unknown probability distribution.</p>
<ul>
<li><strong>Objective:</strong> The goal of the agent in this setting is explicitly to maximize the <strong>cumulative reward</strong> collected over a finite horizon of <span class="arithmatex">\(T\)</span> steps:</li>
</ul>
<div class="arithmatex">\[
G_T = \sum_{t=1}^{T} R_t
\]</div>
<h4 id="action-value-functions-q-values">Action-Value Functions (Q-values)</h4>
<p>To formally analyze and optimize decisions in the multi-armed bandit problem, we define an essential concept known as the <strong>action-value</strong> or <strong>Q-value</strong> of an action. The Q-value of an action represents its expected or average reward:</p>
<div class="arithmatex">\[
q(a) = \mathbb{E}[R \mid A = a] = \int_{-\infty}^{\infty} r \cdot \mathcal{R}^{a}(r) \,dr
\]</div>
<p>In simpler terms, the action-value <span class="arithmatex">\(q(a)\)</span> captures the average reward the agent can expect if it repeatedly selects action <span class="arithmatex">\(a\)</span>. Estimating these action-values accurately is central to solving bandit problems, as optimal actions will naturally correspond to those with higher Q-values.</p>
<h4 id="optimal-action-and-optimal-value">Optimal Action and Optimal Value</h4>
<p>Within the multi-armed bandit framework, there always exists at least one optimal action, denoted by <span class="arithmatex">\(a^\star\)</span>, that maximizes the expected reward. The corresponding maximum Q-value, known as the <strong>optimal value</strong>, is defined as:</p>
<div class="arithmatex">\[
v_\star = q(a^\star) = \max_{a \in \mathcal{A}} q(a)
\]</div>
<p>Identifying the optimal action is the primary challenge, as the agent initially lacks knowledge about the reward distributions and must learn through interaction.</p>
<h4 id="exploration-vs-exploitation-core-difficulty">Exploration vs. Exploitation: Core Difficulty</h4>
<p>The fundamental difficulty faced by agents in the MAB scenario arises precisely from the lack of initial knowledge about the underlying reward distributions. The agent must simultaneously accomplish two conflicting tasks:</p>
<ul>
<li>
<p><strong>Exploration:</strong> By choosing less-understood or infrequently selected arms, the agent gathers crucial information about their reward structures. Exploration can yield long-term benefits by identifying potentially superior actions.</p>
</li>
<li>
<p><strong>Exploitation:</strong> By selecting the actions known to yield high rewards, the agent maximizes immediate returns. Excessive exploitation, however, risks prematurely converging to suboptimal actions due to inadequate exploration.</p>
</li>
</ul>
<p>Balancing these two aspects to maximize cumulative reward over time forms the crux of solving any bandit problem effectively.</p>
<h3 id="non-associativity-property">Non-Associativity Property</h3>
<p>One unique characteristic of the multi-armed bandit setting, which significantly simplifies its theoretical analysis compared to general MDPs, is the property of <strong>non-associativity</strong>. Formally:</p>
<ul>
<li>
<p>Non-associativity means the optimal action does <strong>not depend on any notion of "state" or previous actions</strong>. In other words, the bandit problem does not include state transitions—each action choice is independent of any past or future decision.</p>
</li>
<li>
<p>Therefore, the optimal action <span class="arithmatex">\(a^\star\)</span> remains constant for all time steps, unaffected by previously selected actions. Mathematically, no state-based transition probabilities or value functions conditioned on states are necessary, making the bandit problem a purely action-oriented optimization scenario.</p>
</li>
</ul>
<p>This non-associativity greatly simplifies both theoretical and practical treatment, allowing researchers to isolate the core exploration-exploitation dynamics from more complex temporal or state-dependent phenomena.</p>
<h4 id="real-world-applications">Real-World Applications</h4>
<p>Despite its apparent simplicity, the multi-armed bandit framework finds extensive applications across diverse fields, where efficient decision-making under uncertainty directly influences outcomes. Some key areas include:</p>
<ul>
<li>
<p><strong>Medical Trials:</strong> Clinical research often faces the challenge of testing multiple treatments while minimizing patient risk. MAB strategies help researchers adaptively assign treatments, effectively balancing learning (exploring treatment efficacy) and optimizing patient outcomes (exploiting effective treatments).</p>
</li>
<li>
<p><strong>Online Advertising:</strong> Digital platforms utilize MAB algorithms to dynamically select advertisements that maximize user engagement and revenue. By continuously balancing exploration of new ads and exploitation of proven performers, businesses optimize long-term profits.</p>
</li>
<li>
<p><strong>Recommendation Systems:</strong> Platforms like streaming services or e-commerce websites employ MAB methods to personalize content delivery. Adaptive recommendation algorithms efficiently learn user preferences by experimenting with various content while maintaining user satisfaction.</p>
</li>
<li>
<p><strong>Financial Investment:</strong> Asset allocation and portfolio management tasks naturally map onto bandit problems, where investment decisions must balance immediate financial returns against uncertainty about future asset performance. Using MAB-based decision frameworks, investors systematically explore financial instruments to identify strategies that yield superior long-term returns.</p>
</li>
</ul>
<p>In all these applications, the fundamental logic of balancing exploration and exploitation captured by the multi-armed bandit problem remains central to achieving optimal performance under uncertainty.</p>
<h2 id="action-value-methods-and-types">Action-Value Methods and Types</h2>
<p>To effectively approach and solve the Multi-Armed Bandit (MAB) problem, we require a method for accurately estimating the value associated with each action. This value, commonly referred to as the <strong>action-value function</strong>, denoted by <span class="arithmatex">\(Q_t(a)\)</span>, represents the estimated expected reward of choosing a particular action <span class="arithmatex">\(a\)</span> at time step <span class="arithmatex">\(t\)</span>. Formally, the goal is for <span class="arithmatex">\(Q_t(a)\)</span> to approximate the true expected reward <span class="arithmatex">\(q_*(a)\)</span>, as closely as possible:</p>
<div class="arithmatex">\[
Q_t(a) \approx q_*(a).
\]</div>
<p>In practice, the exact values <span class="arithmatex">\(q_*(a)\)</span> are unknown and must be estimated through experience.</p>
<h4 id="sample-average-estimation">Sample-Average Estimation</h4>
<p>A straightforward approach for estimating the action-value is known as <strong>sample-average estimation</strong>. Under this method, the value of an action <span class="arithmatex">\(a\)</span> is estimated by averaging all the observed rewards obtained from selecting action <span class="arithmatex">\(a\)</span> up to time step <span class="arithmatex">\(t\)</span>. The sample-average estimator is formally defined as:</p>
<div class="arithmatex">\[
Q_t(a) = \frac{1}{N_t(a)} \sum_{i=1}^{N_t(a)} R_i,
\]</div>
<p>where:
- <span class="arithmatex">\(N_t(a)\)</span> is the total number of times action <span class="arithmatex">\(a\)</span> has been selected up to time step <span class="arithmatex">\(t\)</span>.</p>
<ul>
<li><span class="arithmatex">\(R_i\)</span> is the reward received at the <span class="arithmatex">\(i^{th}\)</span> time action <span class="arithmatex">\(a\)</span> was selected.</li>
</ul>
<details class="tip">
<summary>Intuition</summary>
<p>This method relies on the Law of Large Numbers, where averaging a large number of observations converges to the true expected reward. Initially, the estimates are inaccurate due to limited observations, but as the action is repeatedly selected, the estimate    <span class="arithmatex">\(Q_t(a)\)</span> increasingly stabilizes and converges towards the true mean reward <span class="arithmatex">\(q_*(a)\)</span>.</p>
</details>
<h4 id="incremental-update-rule-for-efficient-computation">Incremental Update Rule for Efficient Computation</h4>
<p>While computing the action-value through sample-average estimation, it would be computationally inefficient and memory-intensive to store and sum all previous rewards each time a new reward is obtained. Instead, an efficient, incremental update rule can be derived, allowing the estimate <span class="arithmatex">\(Q_t(a)\)</span> to be updated using only the previously calculated estimate and the most recent reward.</p>
<p>This incremental rule is given by:</p>
<div class="arithmatex">\[
Q_{t+1}(a) = Q_t(a) + \frac{1}{N_t(a)} \left(R_t - Q_t(a)\right).
\]</div>
<details class="note" open="open">
<summary>Derivation of the Incremental Update Rule</summary>
<p>Starting from the definition of the sample-average estimate at the next time step <span class="arithmatex">\(t+1\)</span>, we have:</p>
<div class="arithmatex">\[
Q_{t+1}(a) = \frac{1}{N_{t+1}(a)} \sum_{i=1}^{N_{t+1}(a)} R_i.
\]</div>
<p>Breaking this down into the previous <span class="arithmatex">\(N_t(a)\)</span> rewards plus the most recent reward <span class="arithmatex">\(R_t\)</span>, we have:</p>
<div class="arithmatex">\[
Q_{t+1}(a) = \frac{1}{N_{t+1}(a)} \left(\sum_{i=1}^{N_t(a)} R_i + R_t\right).
\]</div>
<p>We already have from the previous step:</p>
<div class="arithmatex">\[
Q_t(a) = \frac{1}{N_t(a)} \sum_{i=1}^{N_t(a)} R_i \quad \Rightarrow \quad \sum_{i=1}^{N_t(a)} R_i = N_t(a)Q_t(a).
\]</div>
<p>Substituting this into the equation above gives:</p>
<div class="arithmatex">\[
Q_{t+1}(a) = \frac{1}{N_{t+1}(a)} \left(N_t(a)Q_t(a) + R_t\right).
\]</div>
<p>Recognizing that <span class="arithmatex">\(N_{t+1}(a) = N_t(a) + 1\)</span>, we can rewrite this as:</p>
<div class="arithmatex">\[
Q_{t+1}(a) = Q_t(a) + \frac{1}{N_t(a) + 1}\left(R_t - Q_t(a)\right),
\]</div>
<p>which is precisely the incremental update rule. This formulation clearly demonstrates that updating action-value estimates does not require retaining all historical  rewards—only the current estimate, <span class="arithmatex">\(Q_t(a)\)</span>, and the most recent observation, <span class="arithmatex">\(R_t\)</span>, are needed.</p>
</details>
<h4 id="constant-step-size-update-for-nonstationary-problems">Constant Step-Size Update for Nonstationary Problems</h4>
<p>The previously discussed sample-average estimation assumes the reward distributions are stationary (constant over time). However, many practical problems involve nonstationary environments, where the true action values can change over time. To handle such scenarios, we introduce a modified update rule that uses a <strong>constant step-size parameter</strong> <span class="arithmatex">\(\alpha\)</span> instead of the diminishing factor <span class="arithmatex">\(\frac{1}{N_t(a)}\)</span>:</p>
<div class="arithmatex">\[
Q_{t+1}(a) = Q_t(a) + \alpha(R_t - Q_t(a)),
\]</div>
<p>where <span class="arithmatex">\(0 &lt; \alpha \leq 1\)</span> determines how much emphasis is placed on recent rewards.</p>
<ul>
<li>If <span class="arithmatex">\(\alpha = \frac{1}{N_t(a)}\)</span>, this formulation reverts back to the sample-average method.</li>
<li>If <span class="arithmatex">\(\alpha\)</span> is constant and fixed, recent rewards have greater influence, making the estimates more responsive to changes in the environment.</li>
</ul>
<details class="note" open="open">
<summary>Exponential Weighted Averaging</summary>
<p>When employing a constant step-size, the estimate effectively becomes an exponentially weighted average of past rewards, giving exponentially decreasing weights to older observations. This becomes clear by expanding the incremental update recursively:</p>
<div class="arithmatex">\[
Q_{t+1}(a) = (1 - \alpha)Q_t(a) + \alpha R_t
\]</div>
<p>Continuing recursively for additional steps, we have:</p>
<div class="arithmatex">\[
Q_{t+2}(a) = (1 - \alpha)^2 Q_t(a) + \alpha(1 - \alpha) R_t + \alpha R_{t+1}.
\]</div>
<p>Generalizing this recursive expansion, the influence of the initial estimate <span class="arithmatex">\(Q_0(a)\)</span> decreases exponentially, and we have the general form:</p>
<div class="arithmatex">\[
Q_t(a) = (1 - \alpha)^t Q_0(a) + \sum_{i=0}^{t-1} \alpha(1 - \alpha)^i R_{t-i}.
\]</div>
<p>This explicitly illustrates the exponential weighting mechanism: recent rewards (closer to the current time <span class="arithmatex">\(t\)</span>) exert a higher influence on the current estimate, while older rewards have their influence gradually diminished by a factor of <span class="arithmatex">\((1 - \alpha)\)</span> per time step.</p>
<p>This exponential weighting characteristic makes the constant step-size update particularly well-suited for dynamic, nonstationary environments, where quickly adapting to changes in action-value distributions is critical.</p>
</details>
<h2 id="regret-measuring-suboptimality">Regret: Measuring Suboptimality</h2>
<h3 id="concept-of-regret">Concept of Regret</h3>
<p>In sequential decision-making, especially within reinforcement learning and multi-armed bandit frameworks, a central concept is the <strong>regret</strong>. Regret quantifies the notion of lost opportunity incurred by choosing suboptimal actions over optimal ones. Intuitively, regret measures how much better the agent could have performed had it always selected the best possible action available, denoted by <span class="arithmatex">\(a^\star\)</span>. Formally, we define the instantaneous regret at iteration <span class="arithmatex">\(t\)</span> as the expected difference between the reward from the optimal action and the reward received from the chosen action <span class="arithmatex">\(A_t\)</span>:</p>
<div class="arithmatex">\[
I_t = \mathbb{E}[v_\star - q(A_t)],
\]</div>
<p>where <span class="arithmatex">\(v_\star\)</span> represents the expected reward of the optimal action <span class="arithmatex">\(a^\star\)</span>, and <span class="arithmatex">\(q(A_t)\)</span> represents the expected reward from the action actually taken at step <span class="arithmatex">\(t\)</span>.</p>
<h3 id="total-regret">Total Regret</h3>
<p>To evaluate the performance of an agent over a sequence of decisions, we typically consider the cumulative effect of these instantaneous regrets. The <strong>total regret</strong> over a horizon of <span class="arithmatex">\(t\)</span> steps is thus:</p>
<div class="arithmatex">\[
L_t = \mathbb{E}\left[\sum_{\tau=1}^{t} (v_\star - q(A_\tau))\right].
\]</div>
<p>Minimizing total regret is directly equivalent to maximizing cumulative reward, making regret a natural performance metric for learning algorithms in reinforcement learning contexts.</p>
<h3 id="3-regret-gap-and-action-counts">3. Regret, Gap, and Action Counts</h3>
<p>To analyze regret in greater detail, we introduce two important concepts:</p>
<ul>
<li>The <strong>action-count</strong> <span class="arithmatex">\(N_t(a)\)</span>, which denotes the expected number of times an action <span class="arithmatex">\(a\)</span> has been selected up to iteration <span class="arithmatex">\(t\)</span>.</li>
<li>The <strong>gap</strong> <span class="arithmatex">\(\Delta_a\)</span>, defined as the difference between the optimal action's expected value and the expected value of action <span class="arithmatex">\(a\)</span>:</li>
</ul>
<div class="arithmatex">\[
\Delta_a = v_\star - q(a).
\]</div>
<p>Given these definitions, the total regret <span class="arithmatex">\(L_t\)</span> can also be expressed in terms of the gaps and action counts. Specifically, by decomposing the regret according to how often each suboptimal action is chosen, we have:</p>
<div class="arithmatex">\[
\begin{aligned}
L_t &amp;= \mathbb{E}\left[\sum_{\tau=1}^{t} (v_\star - q(A_\tau))\right] \\
&amp;= \sum_{a \in \mathcal{A}} \mathbb{E}[N_t(a)](v_\star - q(a)) \\
&amp;= \sum_{a \in \mathcal{A}} \mathbb{E}[N_t(a)] \Delta_a.
\end{aligned}
\]</div>
<p>Thus, the problem of regret minimization reduces to minimizing the expected count of suboptimal actions chosen, particularly those with large gaps.</p>
<h3 id="regret-dynamics-and-algorithmic-insights">Regret Dynamics and Algorithmic Insights</h3>
<p>An important insight about regret is how it grows as a function of time <span class="arithmatex">\(t\)</span> under various strategies. For instance, a purely greedy algorithm—one that selects actions solely based on current value estimates—will exhibit linear regret. This linear growth occurs because the algorithm might prematurely "lock in" on a suboptimal action indefinitely, accruing constant regret at each step.</p>
<p>One powerful mitigation strategy is known as <strong>optimistic initialization</strong>, where we deliberately overestimate initial action values. Formally, the action-value estimates <span class="arithmatex">\(Q(a)\)</span> are updated using an averaging process:</p>
<div class="arithmatex">\[
Q(a) = \frac{1}{N_t(a)} \sum_{\tau=1}^{t} \mathbf{1}(A_\tau = a) R_\tau.
\]</div>
<p>This optimistic approach incentivizes initial exploration, reducing the chance of permanently settling on a suboptimal action, thereby improving long-term regret performance.</p>
<h3 id="lower-bound-on-regret-lai-robbins-bound-this-topic-is-beyond-the-scope-of-this-course">Lower Bound on Regret (Lai-Robbins Bound) (This topic is beyond the scope of this course.)</h3>
<p>An essential theoretical result by Lai and Robbins (1985) provides a fundamental lower bound on achievable regret growth for any "consistent" algorithm—that is, any algorithm whose regret grows sublinearly for all problem instances. Formally, the Lai-Robbins bound is stated as:</p>
<div class="arithmatex">\[
\liminf_{t \to \infty} \frac{L_t}{\ln t} \geq \sum_{a \mid \Delta_a &gt; 0} \frac{\Delta_a}{D_{\text{KL}}(\mathcal{R}^a \|\| \mathcal{R}^{a^\star})},
\]</div>
<p>where <span class="arithmatex">\(D_{\text{KL}}(\mathcal{R}^a || \mathcal{R}^{a^\star})\)</span> is the Kullback–Leibler (KL) divergence between the reward distributions of a suboptimal arm <span class="arithmatex">\(a\)</span> and the optimal arm <span class="arithmatex">\(a^\star\)</span>. Intuitively, this bound indicates that arms with smaller gaps (<span class="arithmatex">\(\Delta_a\)</span> close to zero) or similar reward distributions to the optimal arm (small KL divergence) inherently require more exploration, resulting in greater regret.</p>
<details class="tip">
<summary>liminf</summary>
<p>In mathematics, the <strong>limit inferior</strong> (or <strong>liminf</strong>) of a sequence <span class="arithmatex">\(\{a_n\}\)</span> is defined as:</p>
<div class="arithmatex">\[
\liminf_{n \to \infty} a_n = \lim_{n \to \infty} \left( \inf \{a_k : k \geq n\} \right)
\]</div>
<p>This expression represents the greatest lower bound of the tail of the sequence, effectively capturing the "largest eventual minimum" of the sequence.   </p>
</details>
<h3 id="bernoulli-bandit-case">Bernoulli Bandit Case</h3>
<p>In practical scenarios such as Bernoulli bandits, where each action's reward distribution is Bernoulli(<span class="arithmatex">\(\mu_a\)</span>), the KL divergence has a closed-form expression:</p>
<div class="arithmatex">\[
D_{\text{KL}}(\text{Bern}(\mu_a) \|\| \text{Bern}(\mu^\star)) = \mu^\star \ln \frac{\mu^\star}{\mu_a} + (1 - \mu^\star) \ln \frac{1 - \mu^\star}{1 - \mu_a}.
\]</div>
<p>For large <span class="arithmatex">\(t\)</span>, the Lai-Robbins bound simplifies approximately to:</p>
<div class="arithmatex">\[
L_t \gtrsim \sum_{a \mid \mu_a &lt; \mu^\star} \frac{\ln t}{\mu^\star - \mu_a},
\]</div>
<p>clearly demonstrating the logarithmic lower bound on regret growth. Thus, no algorithm can improve beyond a logarithmic rate of regret growth for these problem instances.</p>
<h3 id="problem-dependent-versus-minimax-regret">Problem-Dependent versus Minimax Regret</h3>
<p>The regret bounds discussed so far are <strong>problem-dependent</strong>, reflecting intrinsic characteristics of specific problem instances (such as gaps between arms). Another view is the minimax regret, which considers the worst-case regret across all possible problem instances. For stochastic bandits with fixed reward distributions, the problem-dependent bound (<span class="arithmatex">\(\Theta(\ln t)\)</span>) is generally more informative and achievable compared to the minimax bound, which typically scales as <span class="arithmatex">\(\Theta(\sqrt{Kt})\)</span> in adversarial settings.</p>
<p>Several algorithms, including Upper Confidence Bound (UCB) and Thompson Sampling, have been shown to achieve regret growth matching the logarithmic Lai-Robbins lower bound, both asymptotically and in some cases even in constant factors. This optimal performance contrasts starkly with naive strategies such as fixed <span class="arithmatex">\(\varepsilon\)</span>-greedy methods, which incur linear regret due to continual exploration.</p>
<h2 id="the-explorationexploitation-trade-off">The Exploration–Exploitation Trade-off</h2>
<p>In sequential decision-making tasks, particularly in the multi-armed bandit problem, maintaining accurate estimates of the value or expected reward of each available action (often called an "arm") is essential. However, accurate estimation alone is insufficient. A fundamental and challenging issue emerges naturally from this setting: the exploration–exploitation trade-off. This trade-off encapsulates a strategic decision every agent must confront repeatedly: should it exploit its current knowledge by choosing actions known (or estimated) to yield high rewards, or should it explore uncertain options to gather more information and potentially discover even more rewarding choices?</p>
<h3 id="formal-definition-and-intuition">Formal Definition and Intuition</h3>
<p>Formally, the exploration–exploitation trade-off can be characterized as follows. Consider a bandit problem with a set of arms <span class="arithmatex">\(\mathcal{A} = \{1, 2, \dots, K\}\)</span>, each arm <span class="arithmatex">\(i\)</span> associated with a fixed but unknown reward distribution characterized by a mean reward <span class="arithmatex">\(\mu_i\)</span>. At any time step <span class="arithmatex">\(t\)</span>, the agent selects an arm <span class="arithmatex">\(A_t \in \mathcal{A}\)</span>, observes a reward <span class="arithmatex">\(R_t \sim \text{distribution}(\mu_{A_t})\)</span>, and updates its value estimates accordingly. If we denote the agent's estimate of the expected reward of arm <span class="arithmatex">\(i\)</span> at time <span class="arithmatex">\(t\)</span> by <span class="arithmatex">\(\hat{Q}_t(i)\)</span>, the decision about which arm to pull next becomes critical.</p>
<p>The key tension arises because of incomplete knowledge: the agent does not initially know the true mean rewards <span class="arithmatex">\(\mu_i\)</span>. Thus, it must decide between:</p>
<ul>
<li><strong>Exploitation</strong>: Selecting the arm with the highest current estimated value <span class="arithmatex">\(\hat{Q}_t(i)\)</span> (greedy choice) to maximize immediate reward.</li>
<li><strong>Exploration</strong>: Selecting a less certain arm to refine value estimates and possibly discover a superior arm for future exploitation.</li>
</ul>
<p>Intuitively, excessive exploitation risks converging prematurely to a suboptimal arm due to misleading early observations. On the other hand, excessive exploration continuously incurs opportunity costs by potentially sacrificing immediate rewards. Hence, effective strategies must delicately balance these competing objectives to achieve minimal long-term regret.</p>
<h3 id="risks-of-pure-exploitation-and-exploration-strategies">Risks of Pure Exploitation and Exploration Strategies</h3>
<p>Consider first a purely exploitative approach—commonly referred to as the <strong>greedy strategy</strong>. Under this policy, at every step after an initial brief exploration period, the agent always selects the arm that currently has the highest empirical mean reward:</p>
<div class="arithmatex">\[
A_t = \arg\max_{i \in \mathcal{A}} \hat{Q}_t(i)
\]</div>
<p>At first glance, this might seem optimal, as the agent is consistently choosing the "best" known option. However, such a strategy is vulnerable to early randomness. For example, if the true best arm <span class="arithmatex">\(i^*\)</span> initially yields a low reward due to chance, while an inferior arm <span class="arithmatex">\(j\)</span> provides unusually high early rewards, the greedy policy will mistakenly favor the suboptimal arm <span class="arithmatex">\(j\)</span> indefinitely. Consequently, the agent fails to discover the superior reward potential of arm <span class="arithmatex">\(i^*\)</span>, causing substantial long-term regret. Formally, it can be rigorously proven that the purely greedy strategy incurs linear regret:</p>
<div class="arithmatex">\[
R(T) = \Theta(T), \quad \text{as } T \rightarrow \infty
\]</div>
<p>In contrast, a purely exploratory strategy, which chooses arms uniformly at random or with constant probability regardless of their past performance, guarantees discovery of each arm’s true expected value but at an excessive cost. Because exploration is indiscriminate, the agent continues to select suboptimal arms frequently, incurring unnecessary losses. This continuous exploration also leads to linear regret in expectation:</p>
<div class="arithmatex">\[
R(T) = \Theta(T) \quad \text{(pure exploration strategy)}
\]</div>
<p>Therefore, neither extreme—pure exploitation nor pure exploration—is desirable. A systematic, controlled approach is required to reduce regret from linear to sublinear growth.</p>
<h3 id="regret-minimization-and-the-concept-of-optimism">Regret Minimization and the Concept of Optimism</h3>
<p>Regret, denoted <span class="arithmatex">\(R(T)\)</span>, measures the cumulative loss of reward compared to always choosing the best possible arm <span class="arithmatex">\(i^*\)</span> with mean reward <span class="arithmatex">\(\mu^* = \max_i \mu_i\)</span>. Mathematically, regret after <span class="arithmatex">\(T\)</span> steps is defined as:</p>
<div class="arithmatex">\[
R(T) = T \mu^* - \sum_{t=1}^{T} \mu_{A_t}
\]</div>
<p>Strategies addressing the exploration–exploitation trade-off aim for sublinear regret growth, typically logarithmic or polynomial, ensuring the average regret per step diminishes as time progresses. This goal motivates the concept of <strong>optimism in the face of uncertainty</strong>, a foundational principle guiding many effective algorithms. Optimism assumes uncertain actions potentially hold better rewards than currently estimated, encouraging exploration of less well-known arms. As the uncertainty around an arm's estimated value decreases (through repeated selection and reward observation), the optimism naturally decreases, favoring exploitation once the uncertainty sufficiently narrows.</p>
<h3 id="common-approaches-to-balancing-exploration-and-exploitation">Common Approaches to Balancing Exploration and Exploitation</h3>
<p>Several classic strategies systematically manage exploration and exploitation, each embodying optimism in a different way. We'll now skim through them briefly and then dive deeper into each one.</p>
<h4 id="1-epsilon-greedy-strategy">1. <span class="arithmatex">\(\epsilon\)</span>-Greedy Strategy</h4>
<p>The <span class="arithmatex">\(\epsilon\)</span>-greedy algorithm selects the greedy action with probability <span class="arithmatex">\(1-\epsilon\)</span>, and explores randomly chosen arms uniformly with probability <span class="arithmatex">\(\epsilon\)</span>. This approach guarantees continuous exploration but at a simple and fixed rate, allowing eventual convergence toward the optimal arm. The key limitation is the fixed exploration rate, which may remain unnecessarily high as uncertainty decreases, leading to avoidable regret.</p>
<h4 id="2-optimistic-initial-values">2. Optimistic Initial Values</h4>
<p>This approach deliberately initializes all arms’ value estimates <span class="arithmatex">\(\hat{Q}_0(i)\)</span> optimistically high. The optimism encourages initial exploration since arms must be repeatedly tested to reduce inflated estimates toward their true values. Eventually, as real performance emerges clearly, exploitation naturally takes over. While effective initially, this method relies heavily on appropriate initial values and may lack flexibility at later stages.</p>
<h4 id="3-upper-confidence-bound-ucb-algorithms">3. Upper Confidence Bound (UCB) Algorithms</h4>
<p>UCB methods use statistical confidence intervals around the estimated values of arms. The algorithm selects actions by:</p>
<div class="arithmatex">\[
A_t = \arg\max_{i} \left[ \hat{Q}_t(i) + \sqrt{\frac{2\ln t}{N_t(i)}} \right]
\]</div>
<p>where <span class="arithmatex">\(N_t(i)\)</span> denotes the number of times arm <span class="arithmatex">\(i\)</span> has been chosen by time <span class="arithmatex">\(t\)</span>. The term added to the estimate is larger when arm <span class="arithmatex">\(i\)</span> is less explored (small <span class="arithmatex">\(N_t(i)\)</span>), creating optimism toward uncertain arms. This systematic exploration results in a provably logarithmic regret bound, making UCB highly appealing from a theoretical perspective.</p>
<h4 id="4-thompson-sampling-bayesian-probability-matching">4. Thompson Sampling (Bayesian Probability Matching)</h4>
<p>Thompson Sampling employs a Bayesian framework. At each step, the agent draws random samples from posterior distributions representing its belief about arm values and chooses the arm associated with the highest sampled value. This probabilistic matching naturally balances exploration and exploitation, with uncertainty directly encoded in the posterior distributions. Thompson sampling frequently demonstrates excellent empirical and theoretical performance, often achieving state-of-the-art regret bounds.</p>
<h2 id="exploration-strategies-for-multi-armed-bandits">Exploration Strategies for Multi-Armed Bandits</h2>
<p>Multi-armed bandit (MAB) problems embody the fundamental challenge of balancing exploration (gathering information about the uncertain environment) and exploitation (leveraging existing knowledge to maximize rewards). Several exploration strategies have emerged, each employing distinct mechanisms to navigate this critical trade-off. Below, we elaborate on two common strategies—<strong>the ε-Greedy algorithm</strong> and <strong>Optimistic Initial Values</strong>—examining their theoretical underpinnings, implementation specifics, and intuitive rationale.</p>
<h3 id="the-epsilon-greedy-algorithm">The <strong><span class="arithmatex">\(\epsilon\)</span></strong>-Greedy Algorithm</h3>
<h4 id="overview-and-motivation">Overview and Motivation</h4>
<p>The <span class="arithmatex">\(\epsilon\)</span>-greedy algorithm is one of the most fundamental and widely used strategies for balancing the exploration-exploitation trade-off in sequential decision-making problems, particularly in the context of the stochastic multi-armed bandit problem. Its appeal lies in its simplicity and intuitive structure: the agent typically selects what appears to be the best action according to its current knowledge (exploitation), but occasionally takes a random action to gather more information about alternatives (exploration).</p>
<p>Formally, consider a <span class="arithmatex">\(K\)</span>-armed bandit problem, where each arm <span class="arithmatex">\(i \in \{1, \dots, K\}\)</span> provides i.i.d. rewards drawn from an unknown distribution with mean <span class="arithmatex">\(\mu_i\)</span>. The goal is to maximize the cumulative reward over time, or equivalently, minimize the <em>regret</em> with respect to always playing the optimal arm <span class="arithmatex">\(i^* = \arg\max_i \mu_i\)</span>.</p>
<p>The <span class="arithmatex">\(\epsilon\)</span>-greedy algorithm addresses this by injecting randomness into the action selection process. At each time step <span class="arithmatex">\(t\)</span>, it behaves as follows:</p>
<div class="arithmatex">\[
A_t = 
\begin{cases}
\text{random arm from } \{1,\dots,K\}, &amp; \text{with probability } \epsilon, \\
\arg\max_i \hat{Q}_{t-1}(i), &amp; \text{with probability } 1 - \epsilon.
\end{cases}
\]</div>
<p>Here, <span class="arithmatex">\(\hat{Q}_{t-1}(i)\)</span> is the estimated mean reward of arm <span class="arithmatex">\(i\)</span> based on observations up to time <span class="arithmatex">\(t-1\)</span>.</p>
<h4 id="formal-definition">Formal Definition</h4>
<p>Let us define the following notation:</p>
<ul>
<li>Let <span class="arithmatex">\(K\)</span> be the number of arms.</li>
<li>Let <span class="arithmatex">\(X_{i, s}\)</span> be the <span class="arithmatex">\(s\)</span>-th observed reward from arm <span class="arithmatex">\(i\)</span>.</li>
<li>Let <span class="arithmatex">\(N_t(i)\)</span> denote the number of times arm <span class="arithmatex">\(i\)</span> has been selected up to time <span class="arithmatex">\(t\)</span>.</li>
<li>Let <span class="arithmatex">\(\hat{Q}_t(i) = \frac{1}{N_t(i)} \sum_{s=1}^{N_t(i)} X_{i,s}\)</span> denote the empirical mean reward for arm <span class="arithmatex">\(i\)</span> at time <span class="arithmatex">\(t\)</span>.</li>
</ul>
<p>At time <span class="arithmatex">\(t+1\)</span>, the <span class="arithmatex">\(\epsilon\)</span>-greedy algorithm proceeds as:</p>
<div class="arithmatex">\[
A_{t+1} =
\begin{cases}
\text{randomly select an arm from } \{1, \dots, K\}, &amp; \text{with probability } \epsilon, \\
\arg\max_i \hat{Q}_t(i), &amp; \text{with probability } 1 - \epsilon.
\end{cases}
\]</div>
<p>The value of <span class="arithmatex">\(\epsilon \in [0,1]\)</span> is typically a small constant (e.g., <span class="arithmatex">\(\epsilon = 0.1\)</span>), ensuring occasional exploration while primarily exploiting the current knowledge.</p>
<details class="tip">
<summary>Intuition Behind the Algorithm</summary>
<p>The core idea of <span class="arithmatex">\(\epsilon\)</span>-greedy is to ensure that all arms are explored with non-zero probability. This addresses the fundamental problem of <em>uncertainty</em> in estimating the rewards of each arm. Initially, all estimates <span class="arithmatex">\(\hat{Q}_t(i)\)</span> are inaccurate due to limited samples. If the algorithm only exploits the current maximum, it risks becoming overconfident in suboptimal arms and permanently ignoring better alternatives.</p>
<p>Exploration allows the algorithm to collect more data about all arms, improving the estimates and preventing premature convergence to a suboptimal policy. Exploitation ensures that we are using the best known option most of the time, thus maximizing the expected reward in the short term.</p>
<p>The balance is governed by <span class="arithmatex">\(\epsilon\)</span>: high <span class="arithmatex">\(\epsilon\)</span> means more exploration (potentially higher short-term regret), while low <span class="arithmatex">\(\epsilon\)</span> means more exploitation (potentially poor long-term performance if the optimal arm is missed early).</p>
</details>
<hr />
<h4 id="regret-analysis-with-constant-epsilon">Regret Analysis with Constant <span class="arithmatex">\(\epsilon\)</span></h4>
<p>The regret of a bandit algorithm at time <span class="arithmatex">\(T\)</span> is defined as:</p>
<div class="arithmatex">\[
R(T) = T \mu^* - \mathbb{E} \left[ \sum_{t=1}^T \mu_{A_t} \right],
\]</div>
<p>where <span class="arithmatex">\(\mu^* = \max_i \mu_i\)</span> is the mean reward of the optimal arm.</p>
<p>For constant <span class="arithmatex">\(\epsilon\)</span>, the agent explores with probability <span class="arithmatex">\(\epsilon\)</span> in each round. During exploration, it selects a random arm uniformly from the <span class="arithmatex">\(K\)</span> options. Hence, even as time progresses and the estimate <span class="arithmatex">\(\hat{Q}_t\)</span> of the optimal arm improves, the algorithm will still spend <span class="arithmatex">\(\epsilon T\)</span> steps (in expectation) exploring randomly.</p>
<p>Let <span class="arithmatex">\(\Delta_i = \mu^* - \mu_i\)</span> denote the expected reward <em>gap</em> between arm <span class="arithmatex">\(i\)</span> and the optimal arm. Then, the expected regret due to exploration is roughly:</p>
<div class="arithmatex">\[
R_{\text{explore}}(T) \approx \epsilon T \cdot \Delta_{\text{avg}},
\]</div>
<p>where <span class="arithmatex">\(\Delta_{\text{avg}} = \frac{1}{K} \sum_{i=1}^K \Delta_i\)</span> is the average regret per random pull.</p>
<p>The expected regret due to exploitation is smaller. With enough exploration, the agent will learn to identify the optimal arm, and during the <span class="arithmatex">\((1 - \epsilon)T\)</span> exploitation steps, it will mostly choose the correct arm. Hence, the dominant contribution to regret comes from exploration.</p>
<p>Thus, for constant <span class="arithmatex">\(\epsilon\)</span>, we have:</p>
<div class="arithmatex">\[
R(T) = \Theta(T), \quad \text{(linear regret)}
\]</div>
<p>and the <em>average</em> regret <span class="arithmatex">\(\frac{R(T)}{T} \to \epsilon \Delta_{\text{avg}}\)</span> as <span class="arithmatex">\(T \to \infty\)</span>. Therefore, constant-<span class="arithmatex">\(\epsilon\)</span> greedy is <em>not asymptotically optimal</em>.</p>
<hr />
<h4 id="convergence-of-hatq_t-estimates">Convergence of <span class="arithmatex">\(\hat{Q}_t\)</span> Estimates</h4>
<p>Despite its linear regret, constant-<span class="arithmatex">\(\epsilon\)</span> greedy does guarantee convergence of estimates. Since there is a fixed, non-zero probability of selecting each arm at every time step, the number of times any given arm <span class="arithmatex">\(i\)</span> is selected satisfies:</p>
<div class="arithmatex">\[
\mathbb{E}[N_T(i)] \geq \epsilon \cdot \frac{T}{K}.
\]</div>
<p>By the Law of Large Numbers, this ensures:</p>
<div class="arithmatex">\[
\hat{Q}_T(i) \xrightarrow{a.s.} \mu_i \quad \text{as } T \to \infty,
\]</div>
<p>for all <span class="arithmatex">\(i\)</span>. Thus, in the limit, the algorithm identifies the optimal arm correctly, but continues to explore forever at a constant rate — causing the regret to grow linearly over time.</p>
<hr />
<h4 id="decaying-epsilon_t-and-sublinear-regret">Decaying <span class="arithmatex">\(\epsilon_t\)</span> and Sublinear Regret</h4>
<p>To improve performance, one can use a time-dependent exploration rate <span class="arithmatex">\(\epsilon_t\)</span> that decreases with <span class="arithmatex">\(t\)</span>. The motivation is that early on, when little is known, exploration should be frequent. As estimates improve, less exploration is needed, and exploitation becomes safer.</p>
<p>Common choices for decaying schedules include:</p>
<ul>
<li><strong>Inverse time decay</strong>: <span class="arithmatex">\(\epsilon_t = \frac{1}{t}\)</span></li>
<li><strong>Logarithmic decay</strong>: <span class="arithmatex">\(\epsilon_t = \frac{c \ln t}{t}\)</span>, for some constant <span class="arithmatex">\(c &gt; 0\)</span></li>
<li><strong>Gap-aware decay</strong>: <span class="arithmatex">\(\epsilon_t = \min\left\{1, \frac{K}{t \Delta^2}\right\}\)</span> (requires knowledge of gap <span class="arithmatex">\(\Delta\)</span>)</li>
</ul>
<p>Under such schedules, we can show that:</p>
<div class="arithmatex">\[
R(T) = O(\ln T),
\]</div>
<p>i.e., the regret grows logarithmically, which is the best one can hope for in the stochastic setting (matching the lower bound of Lai and Robbins for the asymptotic regret of consistent algorithms).</p>
<p><strong>Sketch of proof (informal intuition):</strong> With <span class="arithmatex">\(\epsilon_t = \frac{c \ln t}{t}\)</span>, the total number of exploration steps up to time <span class="arithmatex">\(T\)</span> is approximately:</p>
<div class="arithmatex">\[
\sum_{t=1}^T \epsilon_t = \sum_{t=1}^T \frac{c \ln t}{t} \leq c \ln^2 T.
\]</div>
<p>This means that suboptimal arms are chosen much less frequently over time, and the cumulative regret remains sublinear.</p>
<h3 id="optimistic-initial-values">Optimistic Initial Values</h3>
<p>In the context of multi-armed bandit problems, where an agent must choose among several actions (or "arms") to maximize cumulative reward, one of the core challenges is managing the exploration-exploitation trade-off. That is, the agent must balance the need to <strong>explore</strong> lesser-known actions to gather information with the desire to <strong>exploit</strong> currently believed-to-be optimal actions to maximize rewards.</p>
<p>While a common strategy like <strong><span class="arithmatex">\(\epsilon\)</span>-greedy</strong> addresses this by injecting randomness into action selection, another elegant and deterministic alternative is <strong>optimistic initialization</strong>, or <strong>optimistic initial values</strong>. This approach leverages <em>prior optimism</em> to naturally induce exploration, without relying on explicit stochasticity.</p>
<hr />
<h4 id="motivating-intuition">Motivating Intuition</h4>
<p>The intuition behind optimistic initial values stems from a simple psychological metaphor: the agent begins with <strong>overly optimistic beliefs</strong> about the potential payoff of every action. It “believes” every arm is excellent — better than it realistically could be — and therefore is compelled to try each arm at least once to confirm or refute this belief. Upon playing an arm and observing actual rewards (which are, on average, lower than the initial belief), the agent adjusts its estimate downward. Thus, exploration arises <strong>not from randomness</strong>, but from <em>disappointment</em> in inflated expectations.</p>
<p>This method of optimistic bias is especially powerful in <strong>stationary environments</strong>, where the underlying reward distributions do not change over time. In such settings, an intense burst of early exploration can suffice, after which the agent can greedily exploit the best-known option based on refined value estimates.</p>
<hr />
<h4 id="formal-definition-and-mathematical-formulation">Formal Definition and Mathematical Formulation</h4>
<p>Let us consider the standard <span class="arithmatex">\(k\)</span>-armed bandit problem. Each arm <span class="arithmatex">\(i \in \{1, 2, \dots, k\}\)</span> yields stochastic rewards drawn from an unknown and stationary distribution with true mean <span class="arithmatex">\(\mu_i\)</span>.</p>
<p>The agent maintains an estimate <span class="arithmatex">\(\hat{Q}_t(i)\)</span> of the mean reward for each arm <span class="arithmatex">\(i\)</span> at time <span class="arithmatex">\(t\)</span>, which is updated incrementally as rewards are observed. The standard sample average update rule is:</p>
<div class="arithmatex">\[
\hat{Q}_{t+1}(i) = \hat{Q}_t(i) + \alpha_t(i) \left( R_t(i) - \hat{Q}_t(i) \right),
\]</div>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(R_t(i)\)</span> is the reward observed after playing arm <span class="arithmatex">\(i\)</span> at time <span class="arithmatex">\(t\)</span>,</li>
<li><span class="arithmatex">\(\alpha_t(i)\)</span> is the step size, typically set to <span class="arithmatex">\(1/N_t(i)\)</span> where <span class="arithmatex">\(N_t(i)\)</span> is the number of times arm <span class="arithmatex">\(i\)</span> has been selected by time <span class="arithmatex">\(t\)</span>.</li>
</ul>
<p>In <strong>optimistic initialization</strong>, we initialize the estimates as follows:</p>
<div class="arithmatex">\[
\hat{Q}_0(i) = Q^+ \quad \text{for all } i,
\]</div>
<p>where <span class="arithmatex">\(Q^+\)</span> is a constant such that <span class="arithmatex">\(Q^+ &gt; \max_i \mu_i\)</span>, i.e., it exceeds all plausible true reward means. For example, if rewards are bounded in the interval <span class="arithmatex">\([0, 1]\)</span>, a typical choice is <span class="arithmatex">\(Q^+ = 1\)</span> or even slightly higher.</p>
<p>At each time step <span class="arithmatex">\(t\)</span>, the agent selects the arm with the highest estimated value:</p>
<div class="arithmatex">\[
A_t = \arg\max_i \hat{Q}_t(i),
\]</div>
<p>which is a purely greedy policy.</p>
<hr />
<h4 id="behavioral-dynamics">Behavioral Dynamics</h4>
<p>Initially, all estimates <span class="arithmatex">\(\hat{Q}_0(i) = Q^+\)</span> are equal and maximal, so the agent arbitrarily picks one. Upon selecting an arm, the estimate is updated based on the reward received. Because actual rewards are typically lower than <span class="arithmatex">\(Q^+\)</span>, the new estimate will decrease. Since all other arms still retain their high initial estimates, the agent is then drawn to try those next. This <strong>cyclic effect</strong> continues until all arms have been sampled and their estimates revised downward, in proportion to their observed performance.</p>
<p>Once each arm has been sampled sufficiently to provide reliable estimates of their true means, the arm with the highest empirical average is selected going forward. At this point, the agent effectively switches from exploration to exploitation — but crucially, <strong>without any explicit exploration parameter</strong>.</p>
<hr />
<h4 id="comparison-to-epsilon-greedy">Comparison to <span class="arithmatex">\(\epsilon\)</span>-Greedy</h4>
<p>In contrast to <span class="arithmatex">\(\epsilon\)</span>-greedy — where the agent continues to explore with fixed probability <span class="arithmatex">\(\epsilon\)</span> indefinitely — optimistic initialization focuses exploration into the <strong>early stage</strong> of learning. Once the overly optimistic estimates are corrected, the algorithm becomes purely greedy. This concentrated exploration phase often leads to <strong>faster convergence</strong> to optimal behavior, especially when the environment is stationary.</p>
<p>Furthermore, optimistic initialization <strong>avoids persistent exploration</strong> of obviously suboptimal arms, a common downside of <span class="arithmatex">\(\epsilon\)</span>-greedy. This leads to reduced long-term regret in many practical scenarios.</p>
<hr />
<h4 id="regret-analysis">Regret Analysis</h4>
<p>Let us now examine the regret of optimistic initialization from a theoretical standpoint. Define the <strong>regret</strong> at time <span class="arithmatex">\(T\)</span> as:</p>
<div class="arithmatex">\[
\text{Regret}(T) = T\mu^* - \sum_{t=1}^T \mathbb{E}[\mu_{A_t}],
\]</div>
<p>where <span class="arithmatex">\(\mu^* = \max_i \mu_i\)</span> is the mean reward of the optimal arm.</p>
<p>Even though the policy is greedy after a short initial phase, optimistic initialization ensures that every arm is sampled sufficiently many times to detect suboptimality. Suppose that <span class="arithmatex">\(Q^+\)</span> is set such that each suboptimal arm <span class="arithmatex">\(i\)</span> is pulled at most <span class="arithmatex">\(O\left(\frac{1}{\Delta_i^2} \log T\right)\)</span> times, where <span class="arithmatex">\(\Delta_i = \mu^* - \mu_i\)</span> is the suboptimality gap. This yields:</p>
<div class="arithmatex">\[
\text{Regret}(T) = O\left( \sum_{i: \Delta_i &gt; 0} \frac{\log T}{\Delta_i} \right),
\]</div>
<p>which matches the asymptotic regret bound of more sophisticated algorithms like <strong>Upper Confidence Bound (UCB)</strong>. Hence, optimistic initialization, despite its simplicity, can achieve logarithmic regret.</p>
<p>However, if <span class="arithmatex">\(Q^+\)</span> is set <em>too optimistically</em>, the agent may spend unnecessary time validating even poor arms. Conversely, if it is set <em>insufficiently optimistically</em>, some arms may not be explored at all. Therefore, <strong>the choice of <span class="arithmatex">\(Q^+\)</span> must be made carefully</strong>, ideally based on prior knowledge of the reward bounds.</p>
<h3 id="upper-confidence-bound-ucb-algorithms-a-detailed-exploration">Upper Confidence Bound (UCB) Algorithms: A Detailed Exploration</h3>
<h4 id="introduction-and-motivation">Introduction and Motivation</h4>
<p>In the study of the exploration-exploitation dilemma in stochastic multi-armed bandit (MAB) problems, one of the most elegant and foundational strategies is the <strong>Upper Confidence Bound (UCB)</strong> algorithm. UCB embodies a principle of decision-making known as <strong>optimism in the face of uncertainty</strong>. This heuristic encourages an agent to behave optimistically about less-explored actions by constructing upper confidence bounds for their expected rewards and then selecting actions as if these bounds were true estimates of the actual value.</p>
<p>The UCB framework is based on a rigorous statistical foundation: if we can form a high-probability upper bound on the true reward of each arm, then choosing the arm with the largest such bound encourages both <strong>exploitation</strong> of arms that are empirically promising and <strong>exploration</strong> of arms about which we remain uncertain. </p>
<p>To put it simply: the algorithm behaves as if the true reward of each arm is the most optimistic plausible value consistent with the observed data. This naturally balances the dual needs of exploration (gathering information about uncertain arms) and exploitation (using the current best knowledge to make good decisions).</p>
<hr />
<h4 id="problem-setup">Problem Setup</h4>
<p>We formalize the stochastic K-armed bandit setting as follows:</p>
<ul>
<li>Let <span class="arithmatex">\(\mathcal{A} = \{1, 2, \dots, K\}\)</span> denote the set of <span class="arithmatex">\(K\)</span> actions or arms.</li>
<li>At each time step <span class="arithmatex">\(t = 1, 2, \dots, T\)</span>, the agent selects an arm <span class="arithmatex">\(A_t \in \mathcal{A}\)</span> and receives a reward <span class="arithmatex">\(R_t \in [0,1]\)</span> drawn from a fixed, unknown distribution associated with that arm.</li>
<li>Let <span class="arithmatex">\(\mu_i = \mathbb{E}[R_t \mid A_t = i]\)</span> denote the true expected reward of arm <span class="arithmatex">\(i\)</span>.</li>
<li>Let <span class="arithmatex">\(\mu^* = \max_i \mu_i\)</span> denote the value of the optimal arm.</li>
<li>The goal is to minimize <strong>regret</strong>, defined as the expected difference between the reward accumulated by always playing the optimal arm and the reward collected by the algorithm:</li>
</ul>
<div class="arithmatex">\[
R(T) = T\mu^* - \mathbb{E}\left[\sum_{t=1}^T R_t\right] = \sum_{i=1}^K \Delta_i \mathbb{E}[N_i(T)],
\]</div>
<p>where <span class="arithmatex">\(\Delta_i = \mu^* - \mu_i\)</span> and <span class="arithmatex">\(N_i(T)\)</span> is the number of times arm <span class="arithmatex">\(i\)</span> was selected up to time <span class="arithmatex">\(T\)</span>.</p>
<hr />
<h4 id="optimism-in-the-face-of-uncertainty">Optimism in the Face of Uncertainty</h4>
<p>At the heart of UCB algorithms lies the idea of <strong>confidence intervals</strong>. Suppose for each arm <span class="arithmatex">\(i\)</span>, we maintain an estimate <span class="arithmatex">\(\hat{Q}_{t-1}(i)\)</span> of its true mean <span class="arithmatex">\(\mu_i\)</span>, based on observed rewards. Alongside this estimate, we compute an upper confidence term <span class="arithmatex">\(U_{t-1}(i)\)</span>, such that with high probability:</p>
<div class="arithmatex">\[
\mu_i \leq \hat{Q}_{t-1}(i) + U_{t-1}(i).
\]</div>
<p>Rather than just selecting the arm with the highest empirical mean, we select the arm with the highest <em>upper bound</em>, i.e.,</p>
<div class="arithmatex">\[
A_t = \arg\max_{i \in \mathcal{A}} \hat{Q}_{t-1}(i) + U_{t-1}(i).
\]</div>
<p>This choice reflects <strong>optimism</strong>: we act as if each arm is as good as it could plausibly be, given the data so far. The key insight is that this encourages exploration of less-frequently pulled arms because their confidence intervals are wider (i.e., larger <span class="arithmatex">\(U_{t-1}(i)\)</span>), and this naturally decreases as more data accumulates.</p>
<hr />
<h4 id="derivation-via-hoeffdings-inequality">Derivation via Hoeffding's Inequality</h4>
<p>To construct the upper confidence term <span class="arithmatex">\(U_{t-1}(i)\)</span>, we rely on <strong>Hoeffding's inequality</strong>, a concentration bound for bounded random variables.</p>
<h5 id="hoeffdings-inequality">Hoeffding’s Inequality:</h5>
<p>Let <span class="arithmatex">\(X_1, ..., X_n\)</span> be i.i.d. random variables with values in <span class="arithmatex">\([0, 1]\)</span>, and let <span class="arithmatex">\(\bar{X}_n = \frac{1}{n} \sum_{j=1}^n X_j\)</span> be their sample mean. Then for any <span class="arithmatex">\(u &gt; 0\)</span>,</p>
<div class="arithmatex">\[
\Pr\left\{ \mathbb{E}[X] &gt; \bar{X}_n + u \right\} \leq e^{-2nu^2}.
\]</div>
<p>This inequality bounds the probability that the true mean exceeds the empirical mean by more than <span class="arithmatex">\(u\)</span>. Rearranging it gives us a confidence interval: with probability at least <span class="arithmatex">\(1 - \delta\)</span>,</p>
<div class="arithmatex">\[
\mathbb{E}[X] \leq \bar{X}_n + \sqrt{\frac{\ln(1/\delta)}{2n}}.
\]</div>
<p>We apply this inequality to each arm <span class="arithmatex">\(i\)</span>, where <span class="arithmatex">\(X_j\)</span> is the reward from the <span class="arithmatex">\(j\)</span>-th time we pulled arm <span class="arithmatex">\(i\)</span>, <span class="arithmatex">\(\bar{X}_n = \hat{Q}_{t-1}(i)\)</span>, and <span class="arithmatex">\(n = N_{t-1}(i)\)</span> is the number of times we’ve pulled arm <span class="arithmatex">\(i\)</span>.</p>
<p>This leads us to define:</p>
<div class="arithmatex">\[
U_{t-1}(i) = \sqrt{\frac{\ln(1/\delta_{t-1})}{2N_{t-1}(i)}},
\]</div>
<p>so that with high probability,</p>
<div class="arithmatex">\[
\mu_i \leq \hat{Q}_{t-1}(i) + U_{t-1}(i).
\]</div>
<hr />
<h4 id="the-ucb1-algorithm">The UCB1 Algorithm</h4>
<p>To ensure the confidence holds for all time steps (so that the overall regret is bounded), we define a schedule for <span class="arithmatex">\(\delta_t\)</span> that decays with <span class="arithmatex">\(t\)</span>, e.g., <span class="arithmatex">\(\delta_t = 1/t^2\)</span> or <span class="arithmatex">\(\delta_t = 1/t^4\)</span>. Plugging this into our formula gives the well-known UCB1 index:</p>
<div class="arithmatex">\[
\text{UCB}_t(i) = \hat{Q}_{t-1}(i) + \sqrt{\frac{2 \ln t}{N_{t-1}(i)}}.
\]</div>
<p>Then the action selection rule is:</p>
<div class="arithmatex">\[
A_t = \arg\max_{i \in \mathcal{A}} \left[ \hat{Q}_{t-1}(i) + \sqrt{\frac{2 \ln t}{N_{t-1}(i)}} \right].
\]</div>
<p>This algorithm guarantees that each arm is explored enough to maintain confidence, while also converging to exploiting the optimal arm.</p>
<hr />
<h4 id="interpretation-and-intuition">Interpretation and Intuition</h4>
<p>This formula can be interpreted in two parts:</p>
<ul>
<li><strong>Exploitation</strong>: <span class="arithmatex">\(\hat{Q}_{t-1}(i)\)</span> is the empirical mean of rewards — it represents our current best estimate.</li>
<li><strong>Exploration Bonus</strong>: <span class="arithmatex">\(\sqrt{\frac{2 \ln t}{N_{t-1}(i)}}\)</span> is large when the arm has been pulled only a few times (small <span class="arithmatex">\(N\)</span>) or early in time (small <span class="arithmatex">\(t\)</span>), and shrinks as <span class="arithmatex">\(N\)</span> grows.</li>
</ul>
<p>Crucially, <span class="arithmatex">\(\ln t\)</span> grows slowly (logarithmically), so even at large time steps, the algorithm still occasionally re-explores arms with low <span class="arithmatex">\(N_i\)</span>. This ensures that no arm is neglected forever, and yet the frequency of exploration diminishes as the confidence in <span class="arithmatex">\(\hat{Q}\)</span> grows.</p>
<hr />
<h4 id="regret-analysis-of-ucb1">Regret Analysis of UCB1</h4>
<p>The strength of UCB1 lies in its theoretical guarantees. Let <span class="arithmatex">\(\Delta_i = \mu^* - \mu_i\)</span> denote the suboptimality gap of arm <span class="arithmatex">\(i\)</span>.</p>
<p>Auer et al. (2002) proved that:</p>
<div class="arithmatex">\[
\mathbb{E}[N_i(T)] \leq \frac{8 \ln T}{\Delta_i^2} + O(1),
\]</div>
<p>meaning the number of times a suboptimal arm is pulled grows logarithmically with <span class="arithmatex">\(T\)</span>. This leads to the following bound on expected cumulative regret:</p>
<div class="arithmatex">\[
\mathbb{E}[R(T)] = \sum_{i: \Delta_i &gt; 0} \Delta_i \mathbb{E}[N_i(T)] \leq \sum_{i: \Delta_i &gt; 0} \left( \frac{8 \ln T}{\Delta_i} + O(\Delta_i) \right),
\]</div>
<p>which simplifies to:</p>
<div class="arithmatex">\[
\mathbb{E}[R(T)] = O\left( \sum_{i: \Delta_i &gt; 0} \frac{\ln T}{\Delta_i} \right) = O(\ln T).
\]</div>
<p>This is <strong>order-optimal</strong>, matching the lower bound for regret in stochastic bandits up to constant factors. Importantly, this bound is <em>problem dependent</em>: larger suboptimality gaps <span class="arithmatex">\(\Delta_i\)</span> lead to fewer required explorations, and hence lower regret.</p>
<h3 id="thompson-sampling-bayesian-probability-matching">Thompson Sampling (Bayesian Probability Matching)</h3>
<p>Thompson Sampling (TS), originally proposed by William R. Thompson in 1933, is a foundational algorithm in the domain of sequential decision-making under uncertainty, particularly within the <strong>multi-armed bandit (MAB)</strong> framework. It embodies a Bayesian philosophy, maintaining a probabilistic belief about the reward-generating distribution of each arm and using this belief to guide arm selection.</p>
<p>At its core, Thompson Sampling follows the principle of <strong>probability matching</strong>: it selects actions (i.e., arms) in proportion to the probability that each is the optimal choice, conditioned on observed data. This leads to a dynamic and adaptive strategy that naturally balances <strong>exploration</strong> (gathering information about uncertain arms) and <strong>exploitation</strong> (leveraging the current best guess to maximize reward).</p>
<hr />
<h4 id="bayesian-framework">Bayesian Framework</h4>
<p>Suppose we have a stochastic K-armed bandit problem with arms indexed by <span class="arithmatex">\(i=1,2,\dots,K\)</span>. Each arm <span class="arithmatex">\(i\)</span> is associated with an unknown reward distribution parameterized by <span class="arithmatex">\(\theta_i\)</span>, and our goal is to maximize cumulative reward over <span class="arithmatex">\(T\)</span> rounds. At each time <span class="arithmatex">\(t\)</span>, the learner chooses an arm <span class="arithmatex">\(A_t\)</span>, observes a reward <span class="arithmatex">\(R_t\sim\mathcal{D}_{A_t}\)</span>, and updates their belief about the corresponding <span class="arithmatex">\(\theta_{A_t}\)</span>.</p>
<p>From a Bayesian standpoint, we place a <strong>prior distribution</strong> <span class="arithmatex">\(p(\theta_i)\)</span> over each <span class="arithmatex">\(\theta_i\)</span>, and after observing data <span class="arithmatex">\(\mathcal{H}_t\)</span> (history up to time <span class="arithmatex">\(t\)</span>), we update the posterior <span class="arithmatex">\(p(\theta_i|\mathcal{H}_t)\)</span> via Bayes' theorem:</p>
<div class="arithmatex">\[
p(\theta_i|\mathcal{H}_t)=\frac{p(\mathcal{H}_t|\theta_i)p(\theta_i)}{p(\mathcal{H}_t)}
\]</div>
<p>Thompson Sampling samples <span class="arithmatex">\(\tilde{\theta}_i^{(t)}\sim p(\theta_i|\mathcal{H}_t)\)</span> for each arm and chooses the arm with the highest sampled expected reward.</p>
<p>Formally:</p>
<div class="arithmatex">\[
A_t=\arg\max_{i\in\{1,\dots,K\}}\mathbb{E}[R_t|\tilde{\theta}_i^{(t)}]
\]</div>
<hr />
<h4 id="bernoulli-bandits-beta-bernoulli-model">Bernoulli Bandits: Beta-Bernoulli Model</h4>
<p>To make these ideas concrete, consider the special case where each arm yields <strong>Bernoulli rewards</strong>:</p>
<div class="arithmatex">\[
R_t\in\{0,1\} \quad \text{with} \quad R_t\sim\text{Bernoulli}(\theta_i)
\]</div>
<p>We assume that the success probability <span class="arithmatex">\(\theta_i\in[0,1]\)</span> is unknown. The natural conjugate prior for the Bernoulli distribution is the <strong>Beta distribution</strong>, defined as:</p>
<div class="arithmatex">\[
\theta_i\sim\text{Beta}(\alpha_i,\beta_i), \quad \text{with density:} \quad p(\theta_i)=\frac{\theta_i^{\alpha_i-1}(1-\theta_i)^{\beta_i-1}}{B(\alpha_i,\beta_i)}
\]</div>
<p>where <span class="arithmatex">\(B(\alpha,\beta)\)</span> is the beta function (a normalization constant). The Beta distribution is flexible and allows us to express varying degrees of prior belief. For instance, the <strong>uninformative uniform prior</strong> is <span class="arithmatex">\(\text{Beta}(1,1)\)</span>.</p>
<h4 id="algorithm-steps">Algorithm Steps</h4>
<p>At each round <span class="arithmatex">\(t\)</span>, the Thompson Sampling algorithm proceeds as follows:</p>
<ol>
<li><strong>Posterior Sampling</strong>:<br />
   For each arm <span class="arithmatex">\(i=1,\dots,K\)</span>, draw:</li>
</ol>
<div class="arithmatex">\[
\tilde{\theta}_i^{(t)}\sim\text{Beta}(\alpha_i,\beta_i)
\]</div>
<ol>
<li><strong>Action Selection</strong>:<br />
   Choose the arm with the highest sampled value:</li>
</ol>
<div class="arithmatex">\[
A_t=\arg\max_{i}\tilde{\theta}_i^{(t)}
\]</div>
<ol>
<li>
<p><strong>Reward Observation</strong>:<br />
   Pull arm <span class="arithmatex">\(A_t\)</span>, observe reward <span class="arithmatex">\(R_t\in\{0,1\}\)</span></p>
</li>
<li>
<p><strong>Posterior Update</strong>:<br />
   Update the Beta parameters:</p>
</li>
</ol>
<div class="arithmatex">\[
\alpha_{A_t}\leftarrow\alpha_{A_t}+R_t, \quad \beta_{A_t}\leftarrow\beta_{A_t}+(1-R_t)
\]</div>
<p>The rest of the arms' parameters remain unchanged.</p>
<details class="tip">
<summary>Intuition Behind Exploration and Exploitation</summary>
<p>This process allows the algorithm to <strong>explore uncertain arms</strong> and <strong>exploit promising ones</strong> in a naturally balanced way. Consider an arm <span class="arithmatex">\(i\)</span> with a high mean estimate but low certainty (wide posterior). There's a non-negligible chance that its sampled <span class="arithmatex">\(\tilde{\theta}_i\)</span> will be large, leading to selection. Conversely, an arm with high empirical reward but tight posterior still occasionally gets out-sampled by a more uncertain one.</p>
<p>This phenomenon is called <strong>randomized optimism</strong>: sometimes, by chance, an uncertain arm is sampled optimistically, leading to its exploration. The more we pull an arm, the narrower its posterior becomes, reducing unnecessary exploration over time.</p>
</details>
<hr />
<h4 id="extension-beyond-bernoulli-rewards">Extension Beyond Bernoulli Rewards</h4>
<p>While the Beta-Bernoulli setup is particularly elegant due to its conjugacy (posterior is analytically tractable), Thompson Sampling extends naturally to other reward models:</p>
<ul>
<li><strong>Gaussian rewards with unknown mean (known variance)</strong>: Use a normal prior <span class="arithmatex">\(\theta_i\sim\mathcal{N}(\mu_i,\sigma_i^2)\)</span>  </li>
<li><strong>Poisson rewards</strong>: Use a Gamma prior on the rate parameter <span class="arithmatex">\(\lambda_i\)</span>  </li>
<li><strong>General likelihoods</strong>: Use approximate inference (e.g., Monte Carlo methods, variational inference)</li>
</ul>
<p>In non-conjugate or complex settings, one often resorts to <strong>sampling-based approximations</strong> of the posterior, such as particle filters or MCMC methods.</p>
<hr />
<h4 id="regret-analysis_1">Regret Analysis</h4>
<p>Thompson Sampling was initially justified from a <strong>Bayesian</strong> perspective — minimizing expected regret under a prior over reward distributions. However, rigorous analysis has shown that TS also achieves <strong>strong frequentist guarantees</strong>.</p>
<h4 id="regret-definition">Regret Definition</h4>
<p>Let <span class="arithmatex">\(\mu_i=\mathbb{E}[R_t|A_t=i]\)</span> be the expected reward of arm <span class="arithmatex">\(i\)</span>, and let <span class="arithmatex">\(\mu^*=\max_i\mu_i\)</span> be the optimal reward. Then the cumulative regret over <span class="arithmatex">\(T\)</span> rounds is:</p>
<div class="arithmatex">\[
R(T)=T\mu^*-\sum_{t=1}^T\mathbb{E}[\mu_{A_t}]
\]</div>
<p>For Bernoulli bandits, let <span class="arithmatex">\(\Delta_i=\mu^*-\mu_i\)</span>. Then under mild conditions, the <strong>expected regret</strong> of Thompson Sampling satisfies:</p>
<div class="arithmatex">\[
\mathbb{E}[R(T)]=O\left(\sum_{i:\Delta_i&gt;0}\frac{\ln T}{\Delta_i^2}\right)
\]</div>
<p>This bound is only slightly looser than the regret of UCB algorithms, which have regret scaling as <span class="arithmatex">\(\sum_i\frac{\ln T}{\Delta_i}\)</span>. Despite this, Thompson Sampling often <strong>outperforms UCB in practice</strong> due to better constant factors and more flexible adaptation.</p>
<p>In fact, for many distributions, it has been shown that TS <strong>asymptotically matches the Lai–Robbins lower bound</strong>:</p>
<div class="arithmatex">\[
\liminf_{T\to\infty}\frac{\mathbb{E}[R(T)]}{\ln T}\geq\sum_{i:\Delta_i&gt;0}\frac{\Delta_i}{D(\mu_i\|\mu^*)}
\]</div>
<p>where <span class="arithmatex">\(D(p\|q)\)</span> is the Kullback–Leibler divergence between the reward distributions of arms <span class="arithmatex">\(i\)</span> and the optimal arm.</p>
<hr />
<h2 id="contextual-bandits"><strong>Contextual Bandits</strong></h2>
<p>While standard multi-armed bandits assume no additional data or “context” is available when selecting an arm, many real-world applications present extra information—sometimes called <strong>features</strong> or <strong>context</strong>—that can help guide the choice of action. This setting is known as a <strong>contextual bandit</strong> or <strong>bandit with side information</strong>.</p>
<h3 id="motivation-and-setup">Motivation and Setup</h3>
<p>In a <strong>contextual bandit</strong> problem, at each time step <span class="arithmatex">\(t\)</span>:</p>
<ol>
<li>The environment reveals a <strong>context</strong> <span class="arithmatex">\(x_t \in \mathcal{X}\)</span>.  </li>
<li>Based on this context, the agent chooses an action (arm) <span class="arithmatex">\(A_t \in \{1, \dots, K\}\)</span>.  </li>
<li>The chosen action yields a reward <span class="arithmatex">\(R_t\)</span>, drawn from a distribution that can depend on both the action and the context.</li>
</ol>
<p>Formally, we might write:</p>
<div class="arithmatex">\[
R_t \sim \mathcal{R}\bigl(a = A_t, x = x_t\bigr)
\]</div>
<p>Here, <span class="arithmatex">\(\mathcal{X}\)</span> is a (possibly high-dimensional) space of contexts. The agent’s goal remains to maximize cumulative reward (or minimize regret), but now it can exploit the relationship between <strong>(context, action)</strong> and reward.</p>
<h3 id="distinction-from-standard-mab">Distinction from Standard MAB</h3>
<ul>
<li>In <strong>standard MAB</strong>, the same arms are offered in every round, with no side information, and each arm has a single reward distribution.  </li>
<li>In <strong>contextual bandits</strong>, each arm’s reward distribution changes depending on the context <span class="arithmatex">\(\(x\)\)</span>. The agent must learn a <strong>context-to-action</strong> mapping (a policy) that predicts which arm will perform best in each situation.</li>
</ul>
<h3 id="example-use-cases">Example Use Cases</h3>
<ul>
<li><strong>News Article Recommendation</strong>  </li>
<li><strong>Personalized Medicine</strong>  </li>
<li><strong>Targeted Advertising</strong></li>
</ul>
<hr />
<h2 id="linucb-algorithm"><strong>LinUCB Algorithm</strong></h2>
<p>One of the canonical and most influential approaches to contextual bandits is the <strong>LinUCB</strong> algorithm. LinUCB is designed for problems where the reward can be assumed (or approximated) to be a <strong>linear function</strong> of the context. This approach was popularized by <a href="https://dl.acm.org/doi/10.1145/1772690.1772758">Li et al. (2010)</a>, where it was used for news article recommendation; see also the <a href="https://dl.acm.org/doi/10.1023/A:1013689704352">original UCB framework by Auer et al. (2002)</a> for the theoretical underpinnings of confidence-bound methods.</p>
<h3 id="linear-contextual-model">Linear Contextual Model</h3>
<p>Assume the reward from arm <span class="arithmatex">\(i\)</span> when context <span class="arithmatex">\(x_t \in \mathbb{R}^d\)</span> is presented has an <strong>expected value</strong> of the form:</p>
<div class="arithmatex">\[
\mathbb{E}[R_t \mid x_t, A_t = i] = x_t^\top \theta_i,
\]</div>
<p>where <span class="arithmatex">\(\theta_i \in \mathbb{R}^d\)</span> is an unknown weight vector for arm <span class="arithmatex">\(i\)</span>. Each arm <span class="arithmatex">\(i\)</span> thus corresponds to a particular linear relationship between context and reward. In practice, this means if you have a <span class="arithmatex">\(d\)</span>-dimensional feature vector <span class="arithmatex">\(x_t\)</span> representing the context at time <span class="arithmatex">\(t\)</span>, the arm’s expected payoff is captured by the dot product between <span class="arithmatex">\(x_t\)</span> and the parameter vector <span class="arithmatex">\(\theta_i\)</span>.</p>
<ul>
<li><strong>Why linearity?</strong> The assumption of linearity often arises from modeling each component of <span class="arithmatex">\(x_t\)</span> as contributing additively to the reward. While real-world relationships may be more complex, linear approximations can be quite effective in high-dimensional settings, especially when combined with feature engineering.</li>
</ul>
<h3 id="algorithm-structure">Algorithm Structure</h3>
<p>At a high level, LinUCB maintains an <strong>estimate</strong> <span class="arithmatex">\(\hat{\theta}_i\)</span> for each arm <span class="arithmatex">\(i\)</span>. To account for uncertainty in <span class="arithmatex">\(\hat{\theta}_i\)</span>, it constructs an upper confidence bound for the expected reward of each arm, thereby balancing exploration and exploitation (see <a href="https://arxiv.org/abs/1102.2670">Abbasi-Yadkori et al. (2011)</a> for in-depth theoretical analysis of this confidence set approach).</p>
<ol>
<li><strong>Initialization</strong> (for each arm <span class="arithmatex">\(i\)</span>):</li>
<li><span class="arithmatex">\(A_i = I_{d\times d}\)</span> (identity matrix)  </li>
<li><span class="arithmatex">\(b_i = 0\)</span> (zero vector in <span class="arithmatex">\(\mathbb{R}^d\)</span>)</li>
</ol>
<p>Here, <span class="arithmatex">\(A_i\)</span> and <span class="arithmatex">\(b_i\)</span> can be understood in terms of ridge regression: they will accumulate the contextual data and observed rewards for arm <span class="arithmatex">\(i\)</span>, respectively.</p>
<ol>
<li><strong>At time <span class="arithmatex">\(t\)</span></strong>, upon receiving context <span class="arithmatex">\(x_t\)</span>:</li>
<li>For each arm <span class="arithmatex">\(i\)</span>:</li>
</ol>
<div class="arithmatex">\[
\hat{\theta}_i = A_i^{-1} \, b_i
\]</div>
<div class="arithmatex">\[
p_i(t) = x_t^\top \hat{\theta}_i + \alpha \sqrt{x_t^\top A_i^{-1} x_t}
\]</div>
<p>where $ \alpha $ is an exploration parameter controlling how “optimistic” the estimate is, and $ \sqrt{x_t^\top A_i^{-1} x_t} $ measures uncertainty in the linear reward estimate. The larger this term, the less data we have for arm <span class="arithmatex">\(i\)</span> under similar contexts, so the algorithm encourages exploration of that arm.</p>
<ul>
<li><strong>Select</strong> arm <span class="arithmatex">\(A_t = \arg\max_i \; p_i(t)\)</span>.</li>
</ul>
<p>Intuitively, <span class="arithmatex">\(p_i(t)\)</span> combines the <strong>current best guess</strong> (<span class="arithmatex">\(x_t^\top \hat{\theta}_i\)</span>) with a <strong>statistical bonus</strong> (<span class="arithmatex">\(\alpha \sqrt{x_t^\top A_i^{-1} x_t}\)</span>). This reflects the principle of <em>optimism in the face of uncertainty</em>: an action with limited data is given a higher “optimistic” estimate, prompting additional exploration.</p>
<ol>
<li><strong>Observe reward</strong> <span class="arithmatex">\(R_t\)</span>. <strong>Update</strong>:</li>
</ol>
<div class="arithmatex">\[
A_{A_t} \leftarrow A_{A_t} + x_t x_t^\top, 
\quad
b_{A_t} \leftarrow b_{A_t} + R_t \, x_t.
\]</div>
<p>These updates are analogous to incrementally solving a regularized least-squares problem for each arm’s parameters. After enough pulls, <span class="arithmatex">\(A_i\)</span> becomes well-conditioned, shrinking the confidence interval in <span class="arithmatex">\(p_i(t)\)</span> for arm <span class="arithmatex">\(i\)</span>.</p>
<h3 id="usage-of-linucb-in-contextual-bandits">Usage of LinUCB in Contextual Bandits</h3>
<p>LinUCB is particularly effective when the context-reward relationship is (or is close to) linear. It scales well to large time horizons so long as the context dimension <span class="arithmatex">\(d\)</span> is not too large, because the key matrix inverse <span class="arithmatex">\(A_i^{-1}\)</span> is only <span class="arithmatex">\(d \times d\)</span>.</p>
<ul>
<li><strong>Feature Construction</strong>: If the raw context is not linear, one can often use polynomial or kernel feature mappings to approximate non-linear relationships within a higher-dimensional linear model.  </li>
<li><strong>Hyperparameter Tuning</strong>: The exploration parameter $ \alpha $ often requires careful tuning (or theoretically derived values) to ensure a good balance of exploration and exploitation.  </li>
<li><strong>Practical Extensions</strong>: Variants of LinUCB can incorporate regularization parameters, discount old data for nonstationary environments, and use approximate matrix updates for very large <span class="arithmatex">\(d\)</span>.</li>
</ul>
<p>For more details and practical insights, see:
- <a href="https://dl.acm.org/doi/10.1145/1772690.1772758">Li et al. (2010)</a> for the original application to personalized news recommendation.<br />
- <a href="https://proceedings.neurips.cc/paper_files/paper/2011/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf">Chapelle &amp; Li (2011)</a> for empirical comparisons of bandit algorithms (including LinUCB).<br />
- <a href="https://dl.acm.org/doi/10.1023/A:1013689704352">Auer (2002)</a> for the foundational UCB concept.</p>
<h3 id="regret-analysis-of-linucb">Regret Analysis of LinUCB</h3>
<p>Under standard assumptions (linear rewards, bounded noise), LinUCB achieves <strong>sublinear</strong> regret in the order of <span class="arithmatex">\(O(d \sqrt{T} \ln T)\)</span>. As <span class="arithmatex">\(T\)</span> grows, average per-step regret goes to zero, indicating the algorithm efficiently balances exploration and exploitation in a theoretically rigorous manner.</p>
<ul>
<li><strong>High-Level Idea</strong>: The quantity <span class="arithmatex">\(x_t^\top A_i^{-1} x_t\)</span> can be interpreted as capturing how much “new information” the context <span class="arithmatex">\(x_t\)</span> provides about arm <span class="arithmatex">\(i\)</span>. Once <span class="arithmatex">\(A_i\)</span> becomes large and well-inverted, the algorithm is confident in its parameter estimates, reducing the exploration term.  </li>
<li><strong>Practical Interpretation</strong>: In simpler terms, each arm <span class="arithmatex">\(i\)</span> fits a linear predictor <span class="arithmatex">\(\hat{\theta}_i\)</span> by “collecting” relevant <span class="arithmatex">\((x_t, R_t)\)</span> pairs. With enough data, the algorithm zeroes in on the optimal linear function.</li>
</ul>
<p>For a formal derivation of these regret bounds, one can consult:</p>
<ul>
<li><a href="https://arxiv.org/abs/1102.2670">Abbasi-Yadkori et al. (2011)</a> — detailed proofs for linear bandits’ regret bounds.  </li>
<li><a href="https://www.researchgate.net/publication/346700490_Survey_on_Applications_of_Multi-Armed_and_Contextual_Bandits">Bouneffouf et al. (2020)</a> — comprehensive survey on bandits, including contextual and linear settings.</li>
</ul>
<hr />
<h2 id="thompson-sampling-in-contextual-bandits"><strong>Thompson Sampling in Contextual Bandits</strong></h2>
<h3 id="overview">Overview</h3>
<p>Thompson Sampling (TS) can also be extended to <strong>contextual</strong> bandits by placing a prior over each arm’s parameter vector and updating that posterior after each interaction. Similar to standard TS, it selects arms by sampling from this posterior and picking the arm whose sampled parameter suggests the highest reward given the current context.</p>
<h3 id="usage-of-thompson-sampling-algorithm-in-contextual-bandits">Usage of Thompson Sampling Algorithm in Contextual Bandits</h3>
<ol>
<li><strong>Model Specification</strong>: Assume a prior distribution over each arm’s parameter <span class="arithmatex">\(\theta_i\)</span> (e.g., Gaussian for linear models).  </li>
<li><strong>At Each Round <span class="arithmatex">\(t\)</span></strong>:</li>
<li>Observe context <span class="arithmatex">\(x_t\)</span> .  </li>
<li>Sample <span class="arithmatex">\(\tilde{\theta}_i\)</span> from the posterior for each arm <span class="arithmatex">\(i\)</span> .  </li>
<li>Compute <span class="arithmatex">\(\tilde{r}_i(t) = x_t^\top \tilde{\theta}_i\)</span> .  </li>
<li>Select <span class="arithmatex">\(A_t = \arg\max_i \tilde{r}_i(t)\)</span> .  </li>
<li>Observe reward <span class="arithmatex">\(R_t\)</span> .  </li>
<li>Update the posterior of <span class="arithmatex">\(\theta_{A_t}\)</span> .</li>
</ol>
<h3 id="regret-analysis-of-thompson-sampling-in-contextual-bandits">Regret Analysis of Thompson Sampling in Contextual Bandits</h3>
<p>With similar assumptions to LinUCB, contextual Thompson Sampling attains comparable <span class="arithmatex">\(O(\sqrt{T})\)</span> -type regret bounds, often with good empirical results due to its Bayesian “probability matching” mechanism.</p>
<h1 id="conclusion">Conclusion</h1>
<p>The <strong>multi-armed bandit (MAB)</strong> problem encapsulates a fundamental tension inherent to sequential decision-making under uncertainty: the <strong>exploration–exploitation trade-off</strong>. Despite its conceptual simplicity—consisting solely of a set of actions with unknown reward distributions—the MAB framework reveals rich theoretical structures and remains deeply relevant across a wide array of real-world applications.</p>
<p>This chapter began with a formalization of the stochastic bandit setting, introducing key constructs such as <strong>action-value functions</strong>, <strong>sample-average estimation</strong>, and the <strong>non-associativity property</strong>, which distinguishes MABs from general Markov Decision Processes (MDPs) by eliminating the influence of state transitions. The core objective was established as the maximization of cumulative reward, equivalently viewed through the lens of <strong>regret minimization</strong>.</p>
<p>To this end, various algorithmic strategies were introduced for estimating action values and managing the trade-off between exploration and exploitation:</p>
<ul>
<li>
<p><strong>Sample-average and incremental update rules</strong> form the foundation for value estimation in stationary environments, while <strong>constant step-size updates</strong> extend applicability to nonstationary settings through exponential weighting of recent observations.</p>
</li>
<li>
<p>The notion of <strong>regret</strong>, both instantaneous and cumulative, provides a principled metric for evaluating the performance of bandit algorithms. Analytical decompositions reveal that total regret depends critically on the <strong>gap</strong> between suboptimal and optimal actions and the frequency with which suboptimal actions are selected.</p>
</li>
<li>
<p>Baseline strategies such as <strong>ε-greedy</strong> and <strong>optimistic initial values</strong> offer intuitive approaches to exploration, though ε-greedy with a constant exploration rate incurs linear regret. Improvements can be achieved through <strong>decaying exploration schedules</strong> or more principled algorithms.</p>
</li>
<li>
<p><strong>Upper Confidence Bound (UCB)</strong> methods exemplify the "optimism in the face of uncertainty" paradigm by using high-probability confidence intervals to balance learning and exploitation. These methods offer <strong>logarithmic regret bounds</strong>, matching the <strong>Lai–Robbins lower bound</strong> for stochastic settings.</p>
</li>
<li>
<p><strong>Thompson Sampling</strong>, rooted in Bayesian inference and probability matching, introduces a powerful and flexible framework for balancing exploration and exploitation. It often performs competitively with UCB both in theory and practice, and generalizes well across reward models.</p>
</li>
<li>
<p>Extensions to the <strong>contextual bandit setting</strong> further elevate the practical relevance of MABs. By incorporating side information or features, algorithms such as <strong>LinUCB</strong> and <strong>contextual Thompson Sampling</strong> dynamically adapt action choices based on observed context, effectively learning context-to-action policies with provably sublinear regret.</p>
</li>
</ul>
<p>In summary, the MAB framework offers a minimal yet powerful model that lies at the heart of many online learning and reinforcement learning scenarios. The theoretical underpinnings, from regret analysis to optimal exploration policies, provide valuable tools for designing adaptive systems. Simultaneously, the algorithmic developments discussed herein continue to form the basis of modern intelligent agents operating in uncertain, real-time environments.</p>
<h2 id="authors">Author(s)</h2>
<div class="grid cards">
<ul>
<li><img align="left" alt="Instructor Avatar" src="/assets/images/staff/Arshia-Gharooni.jpg" width="150" />
    <span class="description">
        <p><strong>Arshia Gharooni</strong></p>
        <p>Teaching Assistant</p>
        <p><a href="mailto:arshiyagharoony@gmail.com">arshiyagharoony@gmail.com</a></p>
        <p>
        <a href="https://x.com/Arshia_Gharooni" target="_blank"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M357.2 48h70.6L273.6 224.2 455 464H313L201.7 318.6 74.5 464H3.8l164.9-188.5L-5.2 48h145.6l100.5 132.9zm-24.8 373.8h39.1L119.1 88h-42z"/></svg></span></a>
        <a href="https://github.com/SilentDrift" target="_blank"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg></span></a>
        <a href="https://silentdrift.github.io/" target="_blank"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M16.36 14c.08-.66.14-1.32.14-2s-.06-1.34-.14-2h3.38c.16.64.26 1.31.26 2s-.1 1.36-.26 2m-5.15 5.56c.6-1.11 1.06-2.31 1.38-3.56h2.95a8.03 8.03 0 0 1-4.33 3.56M14.34 14H9.66c-.1-.66-.16-1.32-.16-2s.06-1.35.16-2h4.68c.09.65.16 1.32.16 2s-.07 1.34-.16 2M12 19.96c-.83-1.2-1.5-2.53-1.91-3.96h3.82c-.41 1.43-1.08 2.76-1.91 3.96M8 8H5.08A7.92 7.92 0 0 1 9.4 4.44C8.8 5.55 8.35 6.75 8 8m-2.92 8H8c.35 1.25.8 2.45 1.4 3.56A8 8 0 0 1 5.08 16m-.82-2C4.1 13.36 4 12.69 4 12s.1-1.36.26-2h3.38c-.08.66-.14 1.32-.14 2s.06 1.34.14 2M12 4.03c.83 1.2 1.5 2.54 1.91 3.97h-3.82c.41-1.43 1.08-2.77 1.91-3.97M18.92 8h-2.95a15.7 15.7 0 0 0-1.38-3.56c1.84.63 3.37 1.9 4.33 3.56M12 2C6.47 2 2 6.5 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10A10 10 0 0 0 12 2"/></svg></span></a>
        </p>
    </span></li>
<li><img align="left" alt="Instructor Avatar" src="/assets/images/staff/Mohammad-Mohammadi.jpg" width="150" />
    <span class="description">
        <p><strong>Mohammad Mohammadi</strong></p>
        <p>Teaching Assistant</p>
        <p><a href="mailto:mohammadm97i@gmail.com">mohammadm97i@gmail.com</a></p>
        <p>
        <a href="https://x.com/imohammad97" target="_blank"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M357.2 48h70.6L273.6 224.2 455 464H313L201.7 318.6 74.5 464H3.8l164.9-188.5L-5.2 48h145.6l100.5 132.9zm-24.8 373.8h39.1L119.1 88h-42z"/></svg></span></a>
        <a href="https://github.com/iMohammad97" target="_blank"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg></span></a>
        <a href="https://www.linkedin.com/in/mohammadmohammadi97" target="_blank"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M100.3 448H7.4V148.9h92.9zM53.8 108.1C24.1 108.1 0 83.5 0 53.8c0-14.3 5.7-27.9 15.8-38S39.6 0 53.8 0s27.9 5.7 38 15.8 15.8 23.8 15.8 38c0 29.7-24.1 54.3-53.8 54.3M447.9 448h-92.7V302.4c0-34.7-.7-79.2-48.3-79.2-48.3 0-55.7 37.7-55.7 76.7V448h-92.8V148.9h89.1v40.8h1.3c12.4-23.5 42.7-48.3 87.9-48.3 94 0 111.3 61.9 111.3 142.3V448z"/></svg></span></a>
        </p>
    </span></li>
<li><img align="left" alt="Instructor Avatar" src="/assets/images/staff/Hesam-Hosseini.jpg" width="150" />
    <span class="description">
        <p><strong>Hesam Hosseini</strong></p>
        <p>Teaching Assistant</p>
        <p><a href="mailto:hesam138122@gmail.com">hesam138122@gmail.com</a></p>
        <p>
        <a href="https://scholar.google.com/citations?user=ODTtV1gAAAAJ&amp;hl=en" target="_blank"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M390.9 298.5s0 .1.1.1c9.2 19.4 14.4 41.1 14.4 64C405.3 445.1 338.5 512 256 512s-149.3-66.9-149.3-149.3c0-22.9 5.2-44.6 14.4-64 1.7-3.6 3.6-7.2 5.6-10.7q6.6-11.4 15-21.3c27.4-32.6 68.5-53.3 114.4-53.3 33.6 0 64.6 11.1 89.6 29.9 9.1 6.9 17.4 14.7 24.8 23.5 5.6 6.6 10.6 13.8 15 21.3 2 3.4 3.8 7 5.5 10.5zm26.4-18.8c-30.1-58.4-91-98.4-161.3-98.4s-131.2 40-161.3 98.4L0 202.7 256 0l256 202.7-94.7 77.1z"/></svg></span></a>
        <a href="https://github.com/Sam-the-first" target="_blank"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg></span></a>
        <a href="https://www.linkedin.com/in/hesam-hosseini-b57092259" target="_blank"><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M100.3 448H7.4V148.9h92.9zM53.8 108.1C24.1 108.1 0 83.5 0 53.8c0-14.3 5.7-27.9 15.8-38S39.6 0 53.8 0s27.9 5.7 38 15.8 15.8 23.8 15.8 38c0 29.7-24.1 54.3-53.8 54.3M447.9 448h-92.7V302.4c0-34.7-.7-79.2-48.3-79.2-48.3 0-55.7 37.7-55.7 76.7V448h-92.8V148.9h89.1v40.8h1.3c12.4-23.5 42.7-48.3 87.9-48.3 94 0 111.3 61.9 111.3 142.3V448z"/></svg></span></a>
        </p>
    </span></li>
</ul>
</div>
<h1 id="references">References</h1>
<ol>
<li>
<p><a href="https://the-rl-hub.github.io/Pages/2-smab/smab.html">The RL Hub MAB Chapters</a></p>
</li>
<li>
<p><a href="https://lilianweng.github.io/">The Multi-Armed Bandit Problem and Its Solutions</a></p>
</li>
<li>
<p><a href="https://www.chenyi-li.com/posts/sequence_decision/">How to make decisions in a bandit game?</a></p>
</li>
<li>
<p><a href="https://www.stat.berkeley.edu/~bartlett/courses/2014fall-cs294stat260/lectures/bandit-lower-bound-notes.pdf">Lower bounds on regret for multi-armed bandits.</a></p>
</li>
<li>
<p><a href="https://ambiata.com/blog/2020-06-03-intro-contextual-bandits/">Continuous Intelligence with Contextual Bandits</a></p>
</li>
<li>
<p><a href="https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf">Reinforcement Learning (BartoSutton)</a></p>
</li>
</ol>







  
  



  



  <form class="md-feedback" name="feedback" hidden>
    <fieldset>
      <legend class="md-feedback__title">
        Was this page helpful?
      </legend>
      <div class="md-feedback__inner">
        <div class="md-feedback__list">
          
            <button class="md-feedback__icon md-icon" type="submit" title="This page was helpful" data-md-value="1">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m7 0c0 .8-.7 1.5-1.5 1.5S14 10.3 14 9.5 14.7 8 15.5 8s1.5.7 1.5 1.5m-5 7.73c-1.75 0-3.29-.73-4.19-1.81L9.23 14c.45.72 1.52 1.23 2.77 1.23s2.32-.51 2.77-1.23l1.42 1.42c-.9 1.08-2.44 1.81-4.19 1.81"/></svg>
            </button>
          
            <button class="md-feedback__icon md-icon" type="submit" title="This page could be improved" data-md-value="0">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10m-6.5-4c.8 0 1.5.7 1.5 1.5s-.7 1.5-1.5 1.5-1.5-.7-1.5-1.5.7-1.5 1.5-1.5M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m2 4.5c1.75 0 3.29.72 4.19 1.81l-1.42 1.42C14.32 16.5 13.25 16 12 16s-2.32.5-2.77 1.23l-1.42-1.42C8.71 14.72 10.25 14 12 14"/></svg>
            </button>
          
        </div>
        <div class="md-feedback__note">
          
            <div data-md-value="1" hidden>
              
              
                
              
              
              
                
                
              
              Thanks for your feedback!
            </div>
          
            <div data-md-value="0" hidden>
              
              
                
              
              
              
                
                
              
              Thanks for your feedback! Help us improve this page by using our <a href="..." target="_blank" rel="noopener">feedback form</a>.
            </div>
          
        </div>
      </div>
    </fieldset>
  </form>


<script src="https://giscus.app/client.js"
        data-repo="DeepRLCourse/DeepRLCourse.github.io"
        data-repo-id="R_kgDOMPOFbQ"
        data-category="Announcements"
        data-category-id="DIC_kwDOMPOFbc4CmGY6"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="1"
        data-input-position="top"
        data-theme="preferred_color_scheme"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>

                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Made with ❤️ in Robust and Interpretable Machine Learning Lab
    </div>
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["navigation.tabs", "navigation.tabs.sticky", "navigation.top", "toc.integrate"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>