{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome","text":"<p>Welcome to Spring 2026 offering of Deep Reinforcement Learning course at Sharif University of Technology! We are excited to have you join us on this journey into the world of deep reinforcement learning.</p> <p>Previous Course (Spring 2025): DeepRLCourse Spring 2025</p>"},{"location":"#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand the fundamentals of reinforcement learning</li> <li>Apply reinforcement learning to various domains</li> <li>Use deep learning techniques to handle large state spaces in RL</li> <li>Master the concepts and gain practical understanding of RL</li> <li>Gain hands-on experience with important RL problems</li> <li>Equip students with enough theoretical knowledge to understand research papers</li> </ul>"},{"location":"#instructor","title":"Instructor","text":"<ul> <li> <p>Dr. Mohammad Hossein Rohban</p> <p>Instructor</p> <p>rohban@sharif.edu</p> <p> </p> </li> </ul>"},{"location":"#schedule","title":"Schedule","text":"Session # Topic of the Session Date Deadlines / Deliverables Session 1 Introduction to RL 3 \u0627\u0633\u0641\u0646\u062f(Feb 22) - Session 2 Introduction to RL 5 \u0627\u0633\u0641\u0646\u062f(Feb 24) - Session 3 Value Based 10 \u0627\u0633\u0641\u0646\u062f(March 1) - Session 4 Value Based 12 \u0627\u0633\u0641\u0646\u062f(March 3) HW 1TA Session 1 Session 5 Policy Based 17 \u0627\u0633\u0641\u0646\u062f(March 8) Project 1 Release Session 6 Policy Based 19 \u0627\u0633\u0641\u0646\u062f(March 10) HW 2TA Session 2 Session 7 Actor Critic 24 \u0627\u0633\u0641\u0646\u062f(March 15) - Session 8 Actor Critic 26 \u0627\u0633\u0641\u0646\u062f(March 17) TA Session 3 Session 9 Dedicated to Previous Topics 16 \u0641\u0631\u0648\u0631\u062f\u06cc\u0646(April 5) HW 3Midterm 1 Session 10 Model Based 18 \u0641\u0631\u0648\u0631\u062f\u06cc\u0646(April 7) - Session 11 Model Based 23 \u0641\u0631\u0648\u0631\u062f\u06cc\u0646(April 12) - Session 12 Model Based 30 \u0641\u0631\u0648\u0631\u062f\u06cc\u0646(April 19) - Session 13 Model Based 1 \u0627\u0631\u062f\u06cc\u0628\u0647\u0634\u062a(April 21) HW 4Project 2 Release Session 14 Multi-armed Bandits 6 \u0627\u0631\u062f\u06cc\u0628\u0647\u0634\u062a(April 26) - Session 15 Multi-armed Bandits 8 \u0627\u0631\u062f\u06cc\u0628\u0647\u0634\u062a(April 28) HW 5 Session 16 RL Theory 13 \u0627\u0631\u062f\u06cc\u0628\u0647\u0634\u062a(May 3) - Session 17 RL Theory 15 \u0627\u0631\u062f\u06cc\u0628\u0647\u0634\u062a(May 5) Midterm 2 Session 18 Exploration Methods 20 \u0627\u0631\u062f\u06cc\u0628\u0647\u0634\u062a(May 10) - Session 19 Exploration Methods 22 \u0627\u0631\u062f\u06cc\u0628\u0647\u0634\u062a(May 12) HW 6 Session 20 Imitation and Inverse RL 27 \u0627\u0631\u062f\u06cc\u0628\u0647\u0634\u062a(May 17) - Session 21 Imitation and Inverse RL 29 \u0627\u0631\u062f\u06cc\u0628\u0647\u0634\u062a(May 19) - Session 22 Imitation and Inverse RL 3 \u062e\u0631\u062f\u0627\u062f(May 24) - Session 23 Offline Methods 5 \u062e\u0631\u062f\u0627\u062f(May 26) - Session 24 Offline Methods 10 \u062e\u0631\u062f\u0627\u062f(May 31) - Session 25 Offline Methods 12 \u062e\u0631\u062f\u0627\u062f(June 2) HW 7 Session 26 Meta RL 17 \u062e\u0631\u062f\u0627\u062f(June 7) - Session 27 Meta RL 19 \u062e\u0631\u062f\u0627\u062f(June 9) - Session 28 Meta RL 24 \u062e\u0631\u062f\u0627\u062f(June 14) HW8Final Exam"},{"location":"#logistics-policies","title":"Logistics &amp; Policies","text":"<ul> <li> <p>Lectures: Held on Sundays and Tuesdays from 1:30 PM to 3:00 PM in room 102 of the CE department.</p> </li> <li> <p>TA Sessions: Coming soon!</p> </li> </ul>"},{"location":"#late-submission-policy","title":"Late Submission Policy","text":"<ul> <li>Each student has 14 total late days for the course.</li> <li>Up to 3 late days may be used per assignment.</li> </ul>"},{"location":"#grading","title":"Grading","text":"<p>The grading for the Deep Reinforcement Learning course is structured as follows:</p>"},{"location":"#main-components","title":"Main Components","text":"<ul> <li>Homeworks: 8 assignments for continuous assessment</li> <li>Midterms: 2 exams focused on conceptual understanding</li> <li>Final: Comprehensive theoretical exam at the end of the course</li> <li>Projects: 2 practical projects applying course concepts</li> </ul> Component Points Date Details Homeworks 7 - 8 HWs \\(\\times \\approx\\) 0.875 each Midterm 1 3 \u061f(?) @ ??:00 AM Midterm 2 3 \u061f(?) @ ??:00 AM Final 6 14 \u062a\u06cc\u0631(July 5) @ 14:30 PM Project 1 1.5 \u061f(?) - Project 2 2.5 \u061f(?) -"},{"location":"#head-assistants","title":"Head Assistants","text":"<ul> <li> <p>Ali Najar</p> <p>Head TA</p> <p>anajar13750@gmail.com</p> <p> </p> </li> <li> <p>Mohammad Pouya Toroghi</p> <p>Head TA</p> <p>pouyatoroghi@gmail.com</p> </li> </ul>"},{"location":"#teaching-assistants","title":"Teaching Assistants","text":"<ul> <li> <p> <p>Parsa Ghezelbash</p> <p>Teaching Assistant</p> </p> </li> <li> <p> <p>Amir Kooshan Fattah</p> <p>Teaching Assistant</p> </p> </li> <li> <p> <p>Mohammad Amin Abbasfar</p> <p>Teaching Assistant</p> </p> </li> <li> <p> <p>Arian Komaei</p> <p>Teaching Assistant</p> </p> </li> <li> <p> <p>Mazdak Teymourian</p> <p>Teaching Assistant</p> </p> </li> <li> <p> <p>Ramtin Moslemi</p> <p>Teaching Assistant</p> </p> </li> <li> <p> <p>Danial Parnian</p> <p>Teaching Assistant</p> </p> </li> <li> <p> <p>Amirmahdi Meighani</p> <p>Teaching Assistant</p> </p> </li> <li> <p> <p>Mobin Bagherian</p> <p>Teaching Assistant</p> </p> </li> <li> <p> <p>Amir Malekhosseini</p> <p>Teaching Assistant</p> </p> </li> <li> <p> <p>Ali Soltani</p> <p>Teaching Assistant</p> </p> </li> <li> <p> <p>Mahshid Dehghani</p> <p>Teaching Assistant</p> </p> </li> <li> <p> <p>Milad Hosseini</p> <p>Teaching Assistant</p> </p> </li> <li> <p> <p>Alireza Nobakht</p> <p>Teaching Assistant</p> </p> </li> <li> <p> <p>Amir Homayoun Sharifi-zadeh</p> <p>Teaching Assistant</p> </p> </li> <li> <p> <p>Saeed Terik</p> <p>Teaching Assistant</p> </p> </li> <li> <p> <p>Hamidreza Ebrahimpour</p> <p>Teaching Assistant</p> </p> </li> <li> <p> <p>Arshia Izadyari</p> <p>Teaching Assistant</p> </p> </li> <li> <p> <p>SeyedAhmad MousaviAwal</p> <p>Teaching Assistant</p> </p> </li> </ul>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>We would like to express our gratitude to the following individuals for their invaluable contributions to the Spring 2025 and 2024 offerings of this course. Their efforts have been instrumental in the development and success of this course.</p> Spring 2025 Arash Alikhani Soroush VafaeiTabar Amir Mohammad Izadi Abdollah Zohrabi Ahmad Karami SeyyedAli MirGhasemi Alireza Nobakht Amirabbas Afzali Amirhossein Asadi Amirreza Velaei Armin Saghafian Arshia Gharooni Behnia Soleymani Benyamin Naderi Dariush Jamshidian Faezeh Sadeghi Ghazal Hosseini Hamidreza Ebrahimpour Hesam Hosseini Mahyar Afshimehr Masoud Tahmasbi Milad Hosseini Mohammad Mohammadi MohammadHasan Abbasi Naseer Kazemi Nima Shirzadi Ramtin Moslemi Reza GhaderiZadeh Spring 2024 Alireza Ghahremani Alireza Sakhaei Rad Amirhossein Mohammadpour Azari Amirmohammad Izadi Arian Ahadinia Armin Behnamnia Armin Saghefian Behnia Soleimani Hossein Jafariniya Mahdi Ghaznavi Mohammadhassan Alikhani Ramtin Moslemi <p>This offering and all of these changes are thanks to their effort in starting this course.</p>"},{"location":"blog/posts/welcome/","title":"Welcome to Deep RL Course!","text":"<p>Welcome to the Spring 2026 offering of the Deep Reinforcement Learning course at Sharif University of Technology! This week, we aim to introduce you to the exciting world of Reinforcement Learning. We've planned an engaging and informative week filled with lectures, hands-on sessions, and fun activities to get you started on your RL journey!</p>","tags":["welcome","introduction"]},{"location":"blog/posts/welcome/#lectures","title":"Lectures","text":"<p>Get ready for two exciting lectures that will introduce you to the fundamentals of RL.</p> <p>To kick things off, we recommend watching this award-winning documentary before class:</p> <p>And don't miss this talk on the history of reinforcement learning by Professor Andrew Barto himself:</p>","tags":["welcome","introduction"]},{"location":"blog/posts/welcome/#recitation","title":"Recitation","text":"<p>We have a recitation session where we will go through the lecture content with clarifying examples.</p>","tags":["welcome","introduction"]},{"location":"blog/posts/welcome/#workshop","title":"Workshop","text":"<p>Join us for a fun workshop session where we will get hands-on and implement most of the important ideas covered in the lectures and more!</p>","tags":["welcome","introduction"]},{"location":"blog/posts/welcome/#homework","title":"Homework","text":"<p>We've designed a fun homework assignment for you! You'll model some interesting problems in RL and solve them using the StableBaseline framework.</p>","tags":["welcome","introduction"]},{"location":"blog/posts/welcome/#guest-lecture","title":"Guest Lecture","text":"<p>We also have a very exciting and surprising guest for this week that we will announce tomorrow!</p> <p>Wish you all the best for this semester!</p>","tags":["welcome","introduction"]},{"location":"course_notes/","title":"Introduction","text":""},{"location":"course_notes/advanced/","title":"Week 4: Advanced Methods","text":""},{"location":"course_notes/advanced/#introduction","title":"Introduction","text":"<p>Reinforcement Learning (RL) has significantly evolved with the development of actor-critic methods, which combine the benefits of policy-based and value-based approaches. These methods utilize policy gradients for optimizing the agent\u2019s actions while employing a critic network to estimate value functions, leading to improved stability.</p> <p>As we explored last week, key concepts like Reward-to-Go and Advantage Estimation lay the foundation for understanding and enhancing these methods. Building on that conversation, this document revisits these ideas, emphasizing their role in refining policy updates and stabilizing training.</p> <p>In actor-critic methods, we explore key concepts that enhance learning efficiency:</p> <ul> <li>Reward-to-Go: A method for computing future. </li> <li>Advantage Estimation: A technique to quantify how much better an action is compared to the expected return.  </li> <li>Generalized Advantage Estimation (GAE): A framework that balances bias and variance in advantage computation, making training more stable.  </li> </ul> <p>This document covers three widely used actor-critic algorithms:</p> <ul> <li>Proximal Policy Optimization (PPO): A popular on-policy algorithm that stabilizes policy updates through a clipped objective function.  </li> <li>Deep Deterministic Policy Gradient (DDPG): An off-policy algorithm designed for continuous action spaces, incorporating experience replay and target networks.  </li> <li>Soft Actor-Critic (SAC): A state-of-the-art off-policy method that introduces entropy maximization to improve exploration and robustness.  </li> </ul> <p>Each section delves into these algorithms, their theoretical foundations, and their practical advantages in reinforcement learning tasks.</p>"},{"location":"course_notes/advanced/#actor-critic","title":"Actor-Critic","text":"<p>The variance of policy methods can originate from two sources:</p> <ol> <li>high variance in the cumulative reward estimate</li> <li>high variance in the gradient estimate. </li> </ol> <p>For both problems, a solution has been developed: bootstrapping for better reward estimates and baseline subtraction to lower the variance of gradient estimates.</p> <p>In the next seciton we will review the concepts bootstrapping (reward to go), using baseline (Advantage value) and Generalized Advantage Estimation (GAE).</p>"},{"location":"course_notes/advanced/#reward-to-go","title":"Reward to Go:","text":"<p>A cumulative reward from state \\(s_t\\) to the end of the episode by applying policy \\(\\pi_\\theta\\).</p> <p>As mentioned earlier, in the policy gradient method, we update our policy weights with the learning rate \\(\\alpha\\) as follows:</p> \\[ \\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta), \\] <p>where</p> \\[ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum^N_{i=1} \\sum^T_{t=1} \\nabla_\\theta \\log\\pi_\\theta(a_{i,t}|s_{i,t})\\cdot r(s_{i,t},a_{i,t}). \\] <p>In this equation, the term \\(r(s_{i,t}, a_{i,t})\\) is the primary source of variance and noise. We use the causality trick to mitigate this issue by multiplying the policy gradient at state \\(s_t\\) with its future rewards. It is important to note that the policy at state \\(s_t\\) can only affect future rewards, not past ones. The causality trick is represented as follows:</p> \\[ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum^N_{i=1}\\bigg( \\sum^T_{t=1} \\nabla_\\theta \\log\\pi_\\theta(a_{i,t}|s_{i,t})\\bigg) \\bigg( \\sum^T_{t=1}r(s_{i,t},a_{i,t}) \\bigg) \\approx \\frac{1}{N} \\sum^N_{i=1} \\sum^T_{t=1} \\nabla_\\theta \\log\\pi_\\theta(a_{i,t}|s_{i,t})\\bigg( \\sum^T_{t'=t}r(s_{i,t'},a_{i,t'}) \\bigg). \\] <p>The term \\(\\sum^T_{t'=t}r(s_{i,t'},a_{i,t'})\\) is known as reward to go, which is calculated in a Monte Carlo manner. It represents the total expected reward from a given state by applying policy \\(\\pi_\\theta\\), starting from time \\(t\\) to the end of the episode.</p> <p>To further reduce variance, we can approximate the reward to go with the Q-value, which conveys a similar meaning. Thus, we can rewrite \\(\\nabla_\\theta J(\\theta)\\) as:</p> \\[ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum^N_{i=1} \\sum^T_{t=1} \\nabla_\\theta \\log\\pi_\\theta(a_{i,t}|s_{i,t}) Q(s_{i,t},a_{i,t}). \\]"},{"location":"course_notes/advanced/#advantage-value","title":"Advantage Value:","text":"<p>Measures how much an action is better than the average of other actions in a given state.</p>"},{"location":"course_notes/advanced/#why-use-the-advantage-value","title":"Why Use the Advantage Value?","text":"<p>We can further reduce variance by subtracting a baseline from \\(Q(s_{i,t}, a_{i,t})\\) without altering the expectation of \\(\\nabla_\\theta J(\\theta)\\), making it an unbiased estimator:</p> \\[ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum^N_{i=1} \\sum^T_{t=1} \\nabla_\\theta \\log\\pi_\\theta(a_{i,t}|s_{i,t}) \\bigg( Q(s_{i,t},a_{i,t}) - b_t \\bigg). \\] <p>A reasonable choice for the baseline is the expected reward. Although it is not optimal, it significantly reduces variance.</p> <p>We define:</p> \\[ Q(s_{i,t},a_{i,t}) = \\sum_{t'=t}^T E_{\\pi_\\theta}[r(s_{t'}, a_{t'})|s_t,a_t]. \\] <p>To ensure the baseline is independent of the action taken, we compute the expectation of \\(Q(s_{i,t}, a_{i,t})\\) over all actions sampled from the policy:</p> \\[ E_{a_t \\sim \\pi_\\theta(a_{i,t}|s_{i,t})} [Q(s_{i,t},a_{i,t})] = V(s_t) = b_t. \\] <p>Thus, the variance-reduced policy gradient equation becomes:</p> \\[ \\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum^N_{i=1} \\sum^T_{t=1} \\nabla_\\theta \\log\\pi_\\theta(a_{i,t}|s_{i,t}) \\bigg( Q(s_{i,t},a_{i,t}) - V(s_t) \\bigg). \\] <p>We define the advantage function as:</p> \\[ A(s_t,a_t) = Q(s_{i,t},a_{i,t}) - V(s_t). \\] <p>Example of Understanding the Advantage Function</p> <p>Consider a penalty shootout game to illustrate the concept of the advantage function and Q-values in reinforcement learning.</p> <p> </p> <ul> <li>Game Setup:<ol> <li>Goalie Strategy:  A goalie always jumps to the right to block the shot.</li> <li>Kicker Strategy: A kicker can shoot either left or right with equal probability (0.5 each), defining the kicker's policy \\(\\pi_k\\).</li> </ol> </li> </ul> <p>The reward matrix for the game is:</p> Kicker / Goalie Right (jumps right) Left (jumps left) Right (shoots right) 0,1 1,0 Left (shoots left) 1,0 0,1 <ul> <li> <p>Expected Reward:</p> <p>Since the kicker selects left and right with equal probability, the expected reward is:</p> \\[ V^{\\pi_k}(s_t) = 0.5 \\times 1 + 0.5 \\times 0 = 0.5. \\] </li> <li> <p>Q-Value Calculation:     The Q-value is expressed as:</p> \\[ Q^{\\pi_k}(s_{i,t},a_{i,t}) = V^{\\pi_k}(s_t) + A^{\\pi_k}(s_t,a_t). \\] <ul> <li>If the kicker shoots right, the shot is always saved (\\(Q^{\\pi_k}(s_{i,t},r) = 0\\)).</li> <li>If the kicker shoots left, the shot is always successful (\\(Q^{\\pi_k}(s_{i,t},l) = 1\\)).</li> </ul> </li> <li> <p>Advantage Calculation:</p> <p>The advantage function \\(A^{\\pi_k}(s_t,a_t)\\) measures how much better or worse an action is compared to the expected reward.</p> <ul> <li>If the kicker shoots left, he scores (reward = 1), which is 0.5 more than the expected reward \\(V^{\\pi_k}(s_t)\\). Thus, the advantage of shooting left is:</li> </ul> \\[ 1 = 0.5 + A^{\\pi_k}(s_t,l) \\Rightarrow A^{\\pi_k}(s_t,l) = 0.5. \\] <ul> <li>If the kicker shoots right, he fails (reward = 0), which is 0.5 less than the expected reward. Thus, the advantage of shooting right is:</li> </ul> \\[ 0 = 0.5 + A^{\\pi_k}(s_t,r) \\Rightarrow A^{\\pi_k}(s_t,r) = -0.5. \\] </li> </ul>"},{"location":"course_notes/advanced/#estimating-the-advantage-value","title":"Estimating the Advantage Value","text":"<p>Instead of maintaining separate networks for estimating \\(V(s_t)\\) and \\(Q(s_{i,t}, a_{i,t})\\), we approximate \\(Q(s_{i,t}, a_{i,t})\\) using \\(V(s_t)\\):</p> \\[ Q(s_{i,t},a_{i,t}) = r(s_t, a_t) + \\sum_{t'=t+1}^T E_{\\pi_\\theta}[r(s_{t'}, a_{t'})|s_t,a_t] \\approx r(s_t, a_t) + V(s_{t+1}). \\] <p>Thus, we estimate the advantage function as:</p> \\[ A(s_{i,t},a_{i,t}) \\approx r(s_t, a_t) + V(s_{t+1}) - V(s_t). \\] <p>We can also, consider the advantage function with discount factor as:</p> \\[ A(s_{i,t},a_{i,t}) \\approx r(s_t, a_t) + \\gamma V(s_{t+1}) - V(s_t). \\] <p>To train the value estimator, we use Monte Carlo estimation.</p>"},{"location":"course_notes/advanced/#generalized-advantage-estimation-gae","title":"Generalized Advantage Estimation (GAE)","text":"<p>To have a good  balance between variance and bias, we can use the concept of GAE, which is firstly introduced in High-Dimensional Continuous Control Using Generalized Advantage Estimation. </p> <p>At the first, we define \\(\\hat{A}^{(k)}(s_{i,t},a_{i,t})\\) to understand this the GAE concept.</p> \\[ \\hat{A}^{(k)}(s_{i,t},a_{i,t}) = r(s_t, a_t) + \\dots + \\gamma^{k-1}r(s_{t+k-1}, a_{t+k-1}) + \\gamma^k V(s_{t+k})- V(s_t). \\] <p>So, we can write the \\(\\hat{A}^{(k)}(s_{i,t},a_{i,t})\\) for \\(k \\in \\{1, \\infty\\}\\) as:</p> \\[ \\hat{A}^{(1)}(s_{i,t},a_{i,t}) = r(s_t, a_t) + \\gamma V(s_{t+1}) - V(s_t) \\] \\[ \\hat{A}^{(2)}(s_{i,t},a_{i,t}) = r(s_t, a_t) + \\gamma r(s_{t+1}, a_{t+1}) + \\gamma^2 V(s_{t+2}) - V(s_t) \\] \\[ .\\\\ .\\\\ .\\\\ \\] \\[ \\hat{A}^{(\\infty)}(s_{i,t},a_{i,t}) = r(s_t, a_t) + \\gamma r(s_{t+1}, a_{t+1}) + \\gamma^2 r(s_{t+2}, a_{t+2})+ \\dots - V(s_t)\\\\ \\] <p>\\(\\hat{A}^{(1)}(s_{i,t},a_{i,t})\\) is high bias, low variance, whilst \\(\\hat{A}^{(\\infty)}(s_{i,t},a_{i,t})\\) is unbiased, high variance.</p> <p>We take a weighted average of all \\(\\hat{A}^{(k)}(s_{i,t},a_{i,t})\\) for \\(k \\in \\{1, \\infty\\}\\) with weight \\(w_k = \\lambda^{k-1}\\) to balance bias and variance. This is called Generalized Advantage Estimation (GAE). </p> \\[ \\hat{A}^{(GAE)}(s_{i,t},a_{i,t}) = \\frac{\\sum_{k =1}^T  w_k \\hat{A}^{(k)}(s_{i,t},a_{i,t})}{\\sum_k w_k}= \\frac{\\sum_{k =1}^T \\lambda^{k-1} \\hat{A}^{(k)}(s_{i,t},a_{i,t})}{\\sum_k w_k} \\]"},{"location":"course_notes/advanced/#actor-critic-algorihtms","title":"Actor-Critic Algorihtms","text":""},{"location":"course_notes/advanced/#batch-actor-critic-algorithm","title":"Batch actor-critic algorithm","text":"<p>The first algorithm is Actor-Critic with Bootstrapping and Baseline Subtraction. In this algorithm, the simulator runs for an entire episode before updating the policy.</p> <p>Batch actor-critic algorithm:</p> <ol> <li>for each episode do:</li> <li> for each step do:</li> <li>\u2003\u2003Take action \\(a_t \\sim \\pi_{\\theta}(a_t | s_t)\\), get \\((s_t,a_t,s'_t,r_t)\\).</li> <li>\u2003Fit \\(\\hat{V}(s_t)\\) with sampled rewards.</li> <li>\u2003Evaluate the advantage function: \\(A({s_t, a_t})\\)</li> <li>\u2003Compute the policy gradient: \\(\\nabla_{\\theta} J(\\theta) \\approx \\sum_{i} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_i | s_i) A({s_t})\\)</li> <li>\u2003Update the policy parameters:  \\(\\theta \\gets \\theta + \\alpha \\nabla_{\\theta} J(\\theta)\\)</li> </ol> <p>Running full episodes for a single update is inefficient as it requires a significant amount of time. To address this issue, the online actor-critic algorithm is proposed.</p>"},{"location":"course_notes/advanced/#online-actor-critic-algorithm","title":"Online actor-critic algorithm","text":"<p>In this algorithm, we take an action in the environment and immediately apply an update using that action.</p> <p>Online actor-critic algorithm</p> <ol> <li>for each episode do:</li> <li> for each step do:</li> <li>\u2003\u2003Take action \\(a_t \\sim \\pi_{\\theta}(a_t | s_t)\\), get \\((s_t,a_t,s'_t,r_t)\\).</li> <li>\u2003\u2003Fit \\(\\hat{V}(s_t)\\) with the sampled reward.</li> <li>\u2003\u2003Evaluate the advantage function: \\(A({s,a})\\)</li> <li>\u2003\u2003Compute the policy gradient: \\(\\nabla_{\\theta} J(\\theta) \\approx  \\nabla_{\\theta} \\log \\pi_{\\theta}(a | s) A({s,a})\\)</li> <li>\u2003\u2003Update the policy parameters: \\(\\theta \\gets \\theta + \\alpha \\nabla_{\\theta} J(\\theta)\\)</li> </ol> <p>Training neural networks with a batch size of 1 leads to high variance, making the training process unstable.</p> <p>To mitigate this issue, two main solutions are commonly used:</p> <ol> <li>Parallel Actor-Critic (Online)</li> <li>Off-Policy Actor-Critic</li> </ol>"},{"location":"course_notes/advanced/#parallel-actor-critic-online","title":"Parallel Actor-Critic (Online)","text":"<p>Many high-performance implementations are based on the actor critic approach. For large problems, the algorithm is typically parallelized and implemented on a large cluster computer.</p> <p>To reduce variance, multiple actors are used to update the policy. There are two main approaches:</p> <ul> <li>Synchronized Parallel Actor-Critic: All actors run synchronously, and updates are applied simultaneously. However, this introduces synchronization overhead, making it impractical in many cases.</li> <li>Asynchronous Parallel Actor-Critic: Each actor applies its updates independently, reducing synchronization constraints and improving computational efficiency. It also, uses asynchronous (parallel and distributed) gradient descent for optimization of deep neural network controllers.</li> </ul> Resources &amp; Links <p>Asynchronous Methods for Deep Reinforcement Learning</p> <p>Actor-Critic Methods: A3C and A2C</p> <p>The idea behind Actor-Critics and how A2C and A3C improve them</p>"},{"location":"course_notes/advanced/#off-policy-actor-critic-algorithm","title":"Off-Policy Actor-Critic Algorithm","text":"<p>In the off-policy approach, we maintain a replay buffer to store past experiences, allowing us to train the model using previously collected data rather than relying solely on the most recent experience.</p> <p>Off-policy actor-critic algorithm:</p> <ol> <li>for each episode do:</li> <li> for multiple steps do:</li> <li>\u2003\u2003Take action \\(a \\sim \\pi_{\\theta}(a | s)\\), get \\((s,a,s',r)\\), store in \\(\\mathcal{R}\\).</li> <li>\u2003Sample a batch \\(\\{s_i, a_i, r_i, s'_i \\}\\) for buffer \\(\\mathcal{R}\\).</li> <li>\u2003Fit \\(\\hat{Q}^{\\pi}(s_i, a_i)\\) for each \\(s_i, a_i\\).</li> <li>\u2003Compute the policy gradient: \\(\\nabla_{\\theta} J(\\theta) \\approx \\frac{1}{N} \\sum_{i} \\nabla_{\\theta} \\log \\pi_{\\theta}(a^{\\pi}_i | s_i) \\hat{Q}^{\\pi}(s_i, a^{\\pi}_i)\\)</li> <li>\u2003Update the policy parameters: \\(\\theta \\gets \\theta + \\alpha \\nabla_{\\theta} J(\\theta)\\)</li> </ol> <p>To work with off-policy methods, we use the Q-value instead of the V-value in step 3. In step 4, rather than using the advantage function, we directly use \\(\\hat{Q}^{\\pi}(s_i, a^{\\pi}_i)\\), where \\(a^{\\pi}_i\\)  is sampled from the policy \\(\\pi\\). By using the Q-value instead of the advantage function, we do not encounter the high-variance problem typically associated with single-step updates. This is because we sample a batch from the replay buffer, which inherently reduces variance. As a result, there is no need to compute an explicit advantage function for variance reduction.</p>"},{"location":"course_notes/advanced/#issues-with-standard-policy-gradient-methods","title":"Issues with Standard Policy Gradient Methods","text":"<p>Earlier policy gradient methods, such as Vanilla Policy Gradient (VPG) or REINFORCE, suffer from high variance and instability in training. A key problem is that large updates to the policy can lead to drastic performance degradation.</p> <p>To address these issues, Trust Region Policy Optimization (TRPO) was introduced, enforcing a constraint on how much the policy can change in a single update. However, TRPO is computationally expensive because it requires solving a constrained optimization problem. PPO is a simpler and more efficient alternative to TRPO, designed to ensure stable policy updates without requiring complex constraints.</p>"},{"location":"course_notes/advanced/#proximal-policy-optimization-ppo","title":"Proximal Policy Optimization (PPO)","text":""},{"location":"course_notes/advanced/#how-ppo-enhances-on-policy-actor-critic-methods","title":"How PPO Enhances On-Policy Actor-Critic Methods","text":"<p>PPO (Proximal Policy Optimization) is an on-policy actor-critic algorithm. It combines a policy network (the actor) and a value network (the critic) and employs a clipped surrogate objective to restrict excessive policy updates, thereby promoting training stability.</p> <p>PPO addresses several problems inherent in on-policy actor-critic methods in the following ways:</p> <ul> <li> <p>Stabilizing Policy Updates:   PPO introduces a clipping mechanism in its objective function that constrains the policy update by ensuring the new policy doesn't deviate too far from the old policy.This clipping prevents overly large updates, thereby stabilizing the learning process and reducing sensitivity.</p> </li> <li> <p>Improving Sample Efficiency:   Although PPO is an on-policy algorithm, it reuses a fixed batch of data for multiple epochs of mini-batch updates. This means that each set of interactions with the environment can contribute more to learning, mitigating the need for excessive sampling.</p> </li> <li> <p>Reducing Variance in Gradient Estimates:   By incorporating advanced advantage estimation techniques (e.g., Generalized Advantage Estimation), PPO reduces the high variance typically associated with policy gradients, leading to more reliable and stable updates.</p> </li> <li> <p>Implicitly Maintaining a Trust Region:   The clipping mechanism acts like an implicit trust region constraint. It ensures that updates remain within a safe boundary around the current policy, which helps in achieving monotonic improvements and prevents performance collapse.</p> </li> </ul>"},{"location":"course_notes/advanced/#the-intuition-behind-ppo","title":"The intuition behind PPO","text":"<p>The idea with Proximal Policy Optimization (PPO) is that we want to improve the training stability of the policy by limiting the change you make to the policy at each training epoch: we want to avoid having too large policy updates.  why?</p> <ol> <li>We know empirically that smaller policy updates during training are more likely to converge to an optimal solution.</li> <li>If we change the policy too much, we may end up with a bad policy that cannot be improved.</li> </ol> <p>soruce: Unit 8, of the Deep Reinforcement Learning Class with Hugging Face</p> <p>Therefore, in order not to allow the current policy to change much compared to the previous policy, we limit the ratio of these two policies to  \\([1 - \\epsilon, 1 + \\epsilon]\\).</p>"},{"location":"course_notes/advanced/#the-clipped-surrogate-objective","title":"the Clipped Surrogate Objective","text":"\\[ L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip} \\left( r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon \\right) \\hat{A}_t \\right) \\right] \\]"},{"location":"course_notes/advanced/#the-ratio-function","title":"The ratio Function","text":"\\[ r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} \\] <p>\\(r_{\\theta}\\) denotes the probability ratio between the current and old policy. if \\(r_{\\theta} &gt; 1\\), then the probability of doing action \\(a_t\\) at \\(s_t\\) in current policy is higher than the old policy and vice versa.</p> <p>So this probability ratio is an easy way to estimate the divergence between old and current policy.</p>"},{"location":"course_notes/advanced/#the-clipped-part","title":"The clipped part","text":"\\[ \\text{clip} \\left( r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon \\right) \\hat{A}_t \\] <p>If the current policy is updated significantly, such that the new policy parameters \\(\\theta'\\)  diverge greatly from the previous ones, the probability ratio between the new and old policies is clipped to the bounds  \\(1 - \\epsilon\\), \\(1 + \\epsilon\\). At this point, the derivative of the objective function becomes zero, effectively preventing further updates. </p>"},{"location":"course_notes/advanced/#the-unclipped-part","title":"The unclipped part","text":"\\[ r_t(\\theta) \\hat{A}_t \\] <p>In the context of optimization, if the initial starting point is not ideal\u2014i.e., if the probability ratio between the new and old policies is outside the range of \\(1 - \\epsilon\\) and \\(1 + \\epsilon\\)\u2014the ratio is clipped to these bounds. This clipping results in the derivative of the objective function becoming zero, meaning no gradient is available for updates. </p> <p>In this formulation, the optimization is performed with respect to the new policy parameters \\(\\theta'\\), and \\(A\\) represents the advantage function, which indicates how much better or worse the action performed is compared to the average return.</p> <p>Example of PPO objective function</p> <ul> <li>Case 1: Positive Advantage (\\(A &gt; 0\\))</li> </ul> <p>if the Advantage \\(A\\) is positive (indicating that the action taken has a higher return than the expected return), and \\(\\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} &lt; 1-\\epsilon\\) , the unclipped part is less than the clipped part and then it is minimized, so we have gradient to update the policy. This allows the policy to increase the probability of the action, aiming for the ratio to reach \\(1 + \\epsilon\\) without violating the clipping constraint.</p> <ul> <li>Case 2: Negative Advantage (\\(A &lt; 0\\))</li> </ul> <p>On the other hand, if the Advantage \\(A\\) is negative (meaning the action taken is worse than the average return), and \\(\\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} &gt; 1+\\epsilon\\), the unclipped objective is again minimized and the gradient is non-zero, leading to an update. In this case, since the Advantage is negative, the policy is adjusted to reduce the probability of selecting that action, bringing the ratio closer to the boundary (\\(1-\\epsilon\\)), while ensuring that the new policy does not deviate too much from the old one.</p>"},{"location":"course_notes/advanced/#visualize-the-clipped-surrogate-objective","title":"Visualize the Clipped Surrogate Objective","text":"<p>Algorithm: PPO-Clip </p> <ol> <li>Input: initial policy parameters \\(\\theta_0\\), initial value function parameters \\(\\phi_0\\) </li> <li>for \\(k = 0, 1, 2, \\dots\\) do </li> <li>\u2003 Collect set of trajectories \\(\\mathcal{D}_k = \\{\\tau_i\\}\\) by running policy \\(\\pi_k = \\pi(\\theta_k)\\) in the environment.  </li> <li>\u2003 Compute rewards-to-go \\(\\hat{R}_t\\).  </li> <li>\u2003 Compute advantage estimates, \\(\\hat{A}_t\\) (using any method of advantage estimation) based on the current value function \\(V_{\\phi_k}\\).  </li> <li> <p>\u2003 Update the policy by maximizing the PPO-Clip objective:  </p> \\[ \\theta_{k+1} = \\arg \\max_{\\theta} \\frac{1}{|\\mathcal{D}_k| T} \\sum_{\\tau \\in \\mathcal{D}_k} \\sum_{t=0}^{T} \\min \\left( \\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_k}(a_t | s_t)} A^{\\pi_{\\theta_k}}(s_t, a_t), \\, g(\\epsilon, A^{\\pi_{\\theta_k}}(s_t, a_t)) \\right) \\] <p>typically via stochastic gradient ascent with Adam.  </p> </li> <li> <p>\u2003 Fit value function by regression on mean-squared error:  </p> \\[ \\phi_{k+1} = \\arg \\min_{\\phi} \\frac{1}{|\\mathcal{D}_k| T} \\sum_{\\tau \\in \\mathcal{D}_k} \\sum_{t=0}^{T} \\left( V_{\\phi}(s_t) - \\hat{R}_t \\right)^2 \\] </li> <li> <p>end for</p> </li> </ol>"},{"location":"course_notes/advanced/#challenges-of-ppo-algorithm","title":"Challenges of PPO algorithm","text":"<p>PPO requires a significant amount of interactions with the environment to converge. This can be problematic in real-world applications where data is expensive or difficult to collect. In fact it is a sample inefficient algorithm.</p> <ol> <li> <p>Hyperparameter Sensitivity:   PPO requires careful tuning of hyperparameters such as the clipping parameter (epsilon), learning rate, and the number of updates per iteration. Poorly chosen hyperparameters can lead to suboptimal performance or even failure to converge.</p> </li> <li> <p>Sample Efficiency: Although PPO is more sample-efficient than some other policy gradient methods, it still requires a significant amount of data to achieve good performance. This can be problematic in environments where data collection is expensive or time-consuming.</p> </li> <li> <p>Exploration-Exploitation Tradeoff: PPO, like other policy gradient methods, can struggle with balancing exploration and exploitation. It may prematurely converge to suboptimal policies if it fails to explore sufficiently.</p> </li> </ol> Helpful links <p>Unit 8, of the Deep Reinforcement Learning Class with Hugging Face</p> <p>Proximal Policy Optimization (PPO) Explained</p> <p>Proximal Policy Optimization (PPO) - How to train Large Language Models</p> <p>Proximal Policy Optimization</p> <p>Understanding PPO: A Game-Changer in AI Decision-Making Explained for RL Newcomers</p> <p>Proximal Policy Optimization (PPO) - Explained</p>"},{"location":"course_notes/advanced/#deep-deterministic-policy-gradients-ddpg","title":"Deep Deterministic Policy Gradients (DDPG)","text":""},{"location":"course_notes/advanced/#handling-continuous-action-spaces","title":"Handling Continuous Action Spaces","text":""},{"location":"course_notes/advanced/#why-is-this-a-problem","title":"Why is this a problem?","text":"<p>In discrete action spaces, methods like DQN (Deep Q-Networks) can use an action-value function \\(Q(s, a)\\) to select the best action. However, in continuous action spaces, selecting the optimal action requires solving a high-dimensional optimization problem at every step, which is computationally expensive.</p>"},{"location":"course_notes/advanced/#how-does-ddpg-solve-it","title":"How does DDPG solve it?","text":"<p>DDPG uses a deterministic policy network \\(\\pi(s)\\), which directly maps states to actions, eliminating the need for iterative optimization over action values.</p>"},{"location":"course_notes/advanced/#ddpg-architecture","title":"DDPG Architecture","text":"<p>DDPG uses four neural networks:</p> <ul> <li>A Q network </li> <li>A deterministic policy network </li> <li>A target Q network </li> <li>A target policy network </li> </ul> <p>The Q network and policy network are similar to Advantage Actor-Critic (A2C), but in DDPG, the Actor directly maps states to actions (the output of the network directly represents the action) instead of outputting a probability distribution over a discrete action space.</p> <p>The target networks are time-delayed copies of their original networks that slowly track the learned networks. Using these target value networks greatly improves stability in learning.  </p>"},{"location":"course_notes/advanced/#why-use-target-networks","title":"Why Use Target Networks?","text":"<p>In methods without target networks, the update equations of the network depend on the network's own calculated values, making it prone to divergence.  </p> <p>For example, if we update the Q-values directly using the current network, errors can compound, leading to instability. The target networks help mitigate this issue by providing more stable targets for updates. </p> \\[     Q(s_t, a_t) \\leftarrow r_t + \\gamma Q(s_{t+1}, \\arg\\max_{a'} Q(s_{t+1}, a')) \\]"},{"location":"course_notes/advanced/#breakdown-of-ddpg-components","title":"Breakdown of DDPG Components","text":"<ol> <li>Experience Replay </li> <li>Actor &amp; Critic Network Updates </li> <li>Target Network Updates </li> <li>Exploration </li> </ol>"},{"location":"course_notes/advanced/#replay-buffer","title":"Replay Buffer","text":"<p>As used in Deep Q-Learning and other RL algorithms, DDPG also utilizes a replay buffer to store experience tuples:  </p> \\[ (state, action, reward, next\\_state) \\] <p>These tuples are stored in a finite-sized cache (replay buffer). During training, random mini-batches are sampled from this buffer to update the value and policy networks.</p> Why Use Experience Replay? <p>In optimization tasks, we want data to be independently distributed. However, in an on-policy learning process, the collected data is highly correlated.  </p> <p>By storing experience in a replay buffer and sampling random mini-batches for training, we break correlations and improve learning stability.</p>"},{"location":"course_notes/advanced/#actor-policy-critic-value-network-updates","title":"Actor (Policy) &amp; Critic (Value) Network Updates","text":"<p>The value network is updated similarly to Q-learning. The updated Q-value is obtained using the Bellman equation:</p> \\[ y_i = r_i + \\gamma Q' \\left (s_{i+1}, \\mu' (s_{i+1}|\\theta^{\\mu'})|\\theta^{Q'} \\right) \\] <p>However, in DDPG, the next-state Q-values are calculated using the target Q network and target policy network.  </p> <p>Then, we minimize the mean squared error (MSE) loss between the updated Q-value and the original Q-value:</p> \\[ \\mathcal{L} = \\frac{1}{N} \\sum_{i} \\left(y_i -Q(s_i, a_i|\\theta^Q) \\right)^2 \\] <ul> <li>Note: The original Q-value is calculated using the learned Q-network, not the target Q-network.</li> </ul> <p>The policy function aims to maximize the expected return:</p> \\[ J(\\theta) = \\mathbb{E} \\left[ Q(s, a) \\mid s = s_t, a_t = \\mu(s_t) \\right] \\] <p>The policy loss is computed by differentiating the objective function with respect to the policy parameters:</p> \\[ \\nabla_{\\theta^\\mu} J(\\theta) = \\nabla_a Q(s, a) |_{a = \\mu(s)} \\nabla_{\\theta^\\mu} \\mu(s|\\theta^\\mu) \\] <p>But since we are updating the policy in an off-policy way with batches of experience, we take the mean of the sum of gradients calculated from the mini-batch:</p> \\[ \\nabla_{\\theta_\\mu} J(\\theta) \\approx \\frac{1}{N} \\sum_i \\left[  \\left. \\nabla_a Q(s, a \\mid \\theta^Q) \\right|_{s = s_i, a = \\mu(s_i)}  \\nabla_{\\theta_\\mu} \\mu(s \\mid \\theta^\\mu) \\Big|_{s = s_i}  \\right] \\]"},{"location":"course_notes/advanced/#target-network-updates","title":"Target Network Updates","text":"<p>The target networks are updated via soft updates instead of direct copying:</p> \\[ \\theta^{Q'} \\leftarrow \\tau \\theta^{Q} + (1 - \\tau) \\theta^{Q'} \\] \\[ \\theta^{\\mu'} \\leftarrow \\tau \\theta^{\\mu} + (1 - \\tau) \\theta^{\\mu'} \\] <p>where \\(\\tau\\) is a small value , ensuring smooth updates that prevent instability.</p>"},{"location":"course_notes/advanced/#exploration","title":"Exploration","text":"<p>In RL for discrete action spaces, exploration is often done using epsilon-greedy or Boltzmann exploration.  </p> <p>However, in continuous action spaces, exploration is done by adding noise to the action itself.  </p> <ul> <li>Ornstein-Uhlenbeck Process The DDPG paper proposes adding Ornstein-Uhlenbeck (OU) noise to the actions.  </li> </ul> <p>The OU Process generates temporally correlated noise, preventing the noise from canceling out or \"freezing\" the action dynamics.</p> \\[ \\mu^{'}(s_t) = \\mu(s_t|\\theta^\\mu_t) + \\mathcal{N} \\] Diagram Of DDPG Algorithms <p> </p>"},{"location":"course_notes/advanced/#algorithm-ddpg-algorithm","title":"Algorithm: DDPG Algorithm","text":"<p>Randomly initialize critic network \\( Q(s, a | \\theta^Q) \\) and actor \\( \\mu(s | \\theta^\\mu) \\) with weights \\( \\theta^Q \\) and \\( \\theta^\\mu \\). Initialize target networks \\( Q' \\) and \\( \\mu' \\) with weights \\( \\theta^{Q'} \\gets \\theta^Q, \\quad \\theta^{\\mu'} \\gets \\theta^\\mu \\). Initialize replay buffer \\( R \\).  </p> <p>for episode = 1 to \\( M \\) do \u00a0\u00a0Initialize a random process \\( \\mathcal{N} \\) for action exploration. \u00a0\u00a0Receive initial observation state \\( s_1 \\). for \\( t = 1 \\) to \\( T \\) do \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Select action \\( a_t = \\mu(s_t | \\theta^\\mu) + \\mathcal{N}_t \\) according to the current policy and exploration noise. \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Execute action \\( a_t \\) and observe reward \\( r_t \\) and new state \\( s_{t+1} \\). \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Store transition \\( (s_t, a_t, r_t, s_{t+1}) \\) in \\( R \\). \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Sample a random minibatch of \\( N \\) transitions \\( (s_i, a_i, r_i, s_{i+1}) \\) from \\( R \\). \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Set:  </p> \\[   y_i = r_i + \\gamma Q'(s_{i+1}, \\mu'(s_{i+1} | \\theta^{\\mu'}) | \\theta^{Q'})   \\] <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Update critic by minimizing the loss:  </p> \\[   L = \\frac{1}{N} \\sum_i (y_i - Q(s_i, a_i | \\theta^Q))^2   \\] <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Update actor policy using the sampled policy gradient: </p> \\[   \\nabla_{\\theta^\\mu} J \\approx \\frac{1}{N} \\sum_i \\nabla_a Q(s, a | \\theta^Q) |_{s=s_i, a=\\mu(s_i)} \\nabla_{\\theta^\\mu} \\mu(s | \\theta^\\mu) |_{s_i}   \\] <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Update the target networks:  </p> \\[   \\theta^{Q'} \\gets \\tau \\theta^Q + (1 - \\tau) \\theta^{Q'}   \\] \\[   \\theta^{\\mu'} \\gets \\tau \\theta^\\mu + (1 - \\tau) \\theta^{\\mu'}   \\] <p> end for end for </p>"},{"location":"course_notes/advanced/#soft-actor-critic-sac","title":"Soft Actor-Critic (SAC)","text":""},{"location":"course_notes/advanced/#challenges-and-motivation-of-sac","title":"Challenges and motivation of SAC","text":"<ol> <li>Previous Off-policy methods like DDPG often struggle with exploration , leading to suboptimal policies. SAC overcomes this by introducing entropy maximization, which encourages the agent to explore more efficiently.</li> <li>Sample inefficiency is a major issue in on-policy algorithms like Proximal Policy Optimization (PPO), which require a large number of interactions with the environment. SAC, being an off-policy algorithm, reuses past experiences stored in a replay buffer, making it significantly more sample-efficient.</li> <li>Another challenge is instability in learning, as methods like DDPG can suffer from overestimation of Q-values. SAC mitigates this by employing twin Q-functions (similar to TD3) and incorporating entropy regularization, leading to more stable and robust learning.</li> </ol> <p>In essence, SAC seeks to maximize the entropy in policy, in addition to the expected reward from the environment. The entropy in policy can be interpreted as randomness in the policy.</p> what is entropy? <p>We can think of entropy as how unpredictable a random variable is. If a random variable always takes a single value then it has zero entropy because it\u2019s not unpredictable at all. If a random variable can be any Real Number with equal probability then it has very high entropy as it is very unpredictable.  </p> <p>probability distributions with low entropy have a tendency to greedily sample certain values, as the probability mass is distributed relatively unevenly.</p>"},{"location":"course_notes/advanced/#maximum-entropy-reinforcement-learning","title":"Maximum Entropy Reinforcement Learning","text":"<p>In Maximum Entropy RL, the agent tries to optimize the policy to choose the right action that can receive the highest sum of reward and long term sum of entropy. This enables the agent to explore more and avoid converging to local optima.</p> <p>reason</p> <p>We want a high entropy in our policy to encourage the policy to assign equal probabilities to actions that have same or nearly equal Q-values(allow the policy to capture multiple modes of good policies), and also to ensure that it does not collapse into repeatedly selecting a particular action that could exploit some inconsistency in the approximated Q function. Therefore, SAC overcomes the  problem by encouraging the policy network to explore and not assign a very high probability to any one part of the range of actions.</p> <p>The objective function of the Maximum entropy RL is as shown below:</p> \\[ J(\\pi_{\\theta}) = \\mathbb{E}_{\\pi_{\\theta}} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) + \\alpha H(\\pi(\\cdot | s_t)) \\right] \\] <p>and the optimal policy is:</p> \\[ \\pi^* = \\arg\\max_{\\pi_{\\theta}} \\mathbb{E}_{\\pi_{\\theta}} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t) + \\alpha H(\\pi(\\cdot | s_t)) \\right] \\] <p>\\(\\alpha\\) is the temperature parameter that balances between exploration and exploitation.</p>"},{"location":"course_notes/advanced/#overcoming-exploration-bias-in-multimodal-q-functions","title":"Overcoming Exploration Bias in Multimodal Q-Functions","text":"<p>Here we want to explain the concept of a multimodal Q-function in reinforcement learning (RL), where the Q-value function, \\(Q(s, a)\\), represents the expected cumulative reward for taking action \\(a\\) in state \\(s\\). </p> <p>In this context, the robot is in an initial state and has two possible passages to follow, which result in a bimodal Q-function (a function with two peaks). These peaks correspond to two different action choices, each leading to different potential outcomes.</p>"},{"location":"course_notes/advanced/#standard-rl-approach","title":"Standard RL Approach","text":"<p>The grey curve represents the Q-function, which has two peaks, indicating two promising actions.A conventional RL approach typically assumes a unimodal (single-peaked) policy distribution, represented by the red curve.</p> <p>This policy distribution is modeled as a Gaussian \\(\\mathcal{N}(\\mu(s_t), \\Sigma)\\) centered around the highest Q-value peak.</p> <p>This setup results in exploration bias where the agent primarily explores around the highest peak and ignores the lower peak entirely.</p>"},{"location":"course_notes/advanced/#improved-exploration-strategy","title":"Improved Exploration Strategy","text":"<p>Instead of using a unimodal Gaussian policy, a Boltzmann-weighted policy is used. The policy distribution (green shaded area) is proportional to \\(\\exp(Q(s_t, a_t))\\), meaning actions are sampled based on their Q-values. </p> <p>This approach allows the agent to explore multiple high-reward actions and avoids the bias of ignoring one passage. As a result, the agent recognizes both options, increasing the chance of finding the optimal path.</p> <p>This concept is relevant for actor-critic RL methods like Soft Actor-Critic (SAC), which uses entropy to encourage diverse exploration.</p>"},{"location":"course_notes/advanced/#soft-policy","title":"Soft Policy","text":"<ul> <li>Soft policy </li> </ul> \\[ J(\\pi) = \\sum_{t=0}^{T} \\mathbb{E}_{(s_t, a_t)\\sim\\rho_{\\pi}} \\left[ r(s_t, a_t) + \\alpha\\mathcal{H}(\\pi(\\cdot | s_t)) \\right] \\] <p>With new objective function we need to define Value funciton and Q-value funciton again. </p> <ul> <li>Soft Q-value funciton</li> </ul> \\[ Q(s_t, a_t) = r(s_t, a_t) + \\gamma\\mathbb{E}_{s_{t+1}\\sim p}\\left[ V(s_{t+1}) \\right] \\] <ul> <li>Soft Value function</li> </ul> \\[ V(s_t) = \\mathbb{E}_{a_t\\sim \\pi} \\left[Q(s_t, a_t) - \\text{log}\\space\\pi(a_t|s_t)\\right] \\]"},{"location":"course_notes/advanced/#soft-policy-iteration","title":"Soft Policy Iteration","text":"<p>Soft Policy Iteration is an entropy-regularized version of classical policy iteration, which consists of:</p> <ol> <li>Soft Policy Evaluation: Estimating the soft Q-value function under the current policy.</li> <li>Soft Policy Improvement:  Updating the policy to maximize the soft Q-value function,incorporating entropy regularization.</li> </ol> <p>This process iteratively improves the policy while balancing exploration and exploitation.</p>"},{"location":"course_notes/advanced/#soft-policy-evaluation-critic-update","title":"Soft Policy Evaluation (Critic Update)","text":"<p>The goal of soft policy evaluation is to compute the expected return of a given policy \\(\\pi\\) under the maximum entropy objective, which modifies the standard Bellman equation by adding an entropy term. (SAC explicitly learns the Q-function for the current policy)</p> <p>The soft Q-value function for a policy \\(\\pi\\) is updated using a modified Bellman operator \\(T^{\\pi}\\):</p> \\[ T^\\pi Q(s_t, a_t) = r(s_t, a_t) + \\gamma \\mathbb{E}_{s_{t+1} \\sim p} [V(s_{t+1})] \\] <p>with substitution of \\(V\\) we have :</p> \\[ T^\\pi Q(s_t, a_t) = r(s_t, a_t) + \\gamma \\mathbb{E}_{\\substack{s_{t+1} \\sim p \\\\ a_{t+1} \\sim \\pi}} [Q(s_{t+1,}, a_{t+1}) - \\text{log}\\space\\pi(a_{t+1}|s_{t+1})] \\] <p>Key Result: Soft Bellman Backup Convergence </p> <p>Theorem: By repeatedly applying the operator \\(T^\\pi\\), the Q-value function converges to the true soft Q-value function for policy \\(\\pi\\):  </p> \\[ Q_k \\to Q^\\pi \\text{ as } k \\to \\infty \\] <p>Thus, we can estimate \\(Q^\\pi\\) iteratively.</p>"},{"location":"course_notes/advanced/#soft-policy-improvement-actor-update","title":"Soft Policy Improvement (Actor Update)","text":"<p>Once the Q-function is learned, we need to improve the policy using a gradient-based update. This means:</p> <ul> <li>Instead of directly maximizing Q-values, the policy is updated to optimize a modified objective that balances reward maximization and exploration.</li> <li>The update is off-policy, meaning the policy can be trained using past experiences stored in a replay buffer, rather than requiring fresh samples like on-policy methods (e.g., PPO).</li> </ul> <p>The update is based on an exponential function of the Q-values:</p> \\[ \\pi^*(a | s) \\propto \\exp (Q^\\pi(s, a)) \\] <p>which means the optimal policy is obtained by normalizing \\(\\exp (Q^\\pi(s, a))\\) over all actions:</p> \\[ \\pi^*(a | s) = \\frac{\\exp (Q^\\pi(s, a))}{Z^\\pi(s)} \\] <p>where \\(Z^\\pi(s)\\) is the partition function that ensures the distribution sums to 1.</p> <p>For the policy improvement step, we update the policy distribution towards the softmax distribution for the current Q function.</p> \\[ \\pi_{\\text{new}} = \\arg \\min_{\\pi' \\in \\Pi} D_{\\text{KL}} \\left( \\pi'(\\cdot | s) \\, \\bigg|\\bigg| \\, \\frac{\\exp (Q^\\pi(s, \\cdot))}{Z^\\pi(s)} \\right) \\] <p>Key Result: Soft Policy Improvement Theorem The new policy \\(\\pi_{\\text{new}}\\) obtained via this update improves the expected soft return:</p> \\[ Q^{\\pi_{\\text{new}}}(s, a) \\geq Q^{\\pi_{\\text{old}}}(s, a) \\quad \\forall (s, a) \\] <p>Thus, iterating this process leads to a better policy.</p>"},{"location":"course_notes/advanced/#convergence-of-soft-policy-iteration","title":"Convergence of Soft Policy Iteration","text":"<p>By alternating between soft policy evaluation and soft policy improvement, soft policy iteration converges to an optimal maximum entropy policy within the policy class \\(\\Pi\\):</p> \\[ \\pi^* = \\arg \\max_{\\pi \\in \\Pi} \\sum_t \\mathbb{E}[r_t + \\alpha H(\\pi(\\cdot | s_t))] \\] <p>However, this exact method is only feasible in the tabular setting. For continuous control, we approximate it using function approximators.</p>"},{"location":"course_notes/advanced/#soft-actor-critic","title":"Soft Actor-Critic","text":"<p>For complex learning domains with high-dimensional and/or continuous state-action spaces, it is mostly impossible to find exact solutions for the MDP. Thus, we must leverage function approximation (i.e. neural networks) to find a practical approximation to soft policy iteration. then we use stochastic gradient descent (SGD) to update parameters of these networks.</p> <p>we model the value functions as expressive neural networks, and the policy as a Gaussian distribution over the action space with the mean and covariance given as neural network outputs with the current state as input.</p>"},{"location":"course_notes/advanced/#soft-value-function-v_psis","title":"Soft Value function (\\(V_{\\psi}(s)\\))","text":"<p>A separate soft value function which helps in stabilising the training process. The soft value function approximator minimizes the squared residual error as follows:</p> \\[ J_V(\\psi) = \\mathbb{E}_{s_{t} \\sim \\mathcal{D}} \\left[ \\frac{1}{2} \\left( V_{\\psi}(s_t) - \\mathbb{E}_{a \\sim \\pi_{\\phi}} [Q_{\\theta}(s_t, a_t) - \\log \\pi_{\\phi}(a_t | s_t)] \\right)^2 \\right] \\] <p>It means the learning of the state-value function \\(V\\), is done by minimizing the squared difference between the prediction of the value network and expected prediction of Q-function with the entropy of the policy, \\(\\pi\\).</p> <ul> <li>\\(D\\) is the distribution of previously sampled states and actions, or a replay buffer.</li> </ul> <p>Gradient Update for \\(V_{\\psi}(s)\\)</p> \\[ \\hat{\\nabla}_{\\psi} J_V(\\psi) = \\nabla_{\\psi} V_{\\psi}(s_t) \\left( V_{\\psi}(s_t) - Q_{\\theta}(s_t, a_t) + \\log \\pi_{\\phi}(a_t | s_t) \\right) \\] <p>where the actions are sampled according to the current policy, instead of the replay buffer.</p>"},{"location":"course_notes/advanced/#soft-q-funciton-q_thetas-a","title":"Soft Q-funciton (\\(Q_{\\theta}(s, a)\\))","text":"<p>We minimize the soft Q-function parameters by using the soft Bellman residual provided here:</p> \\[ J_Q(\\theta) = \\mathbb{E}_{(s_{t}, a_t) \\sim \\mathcal{D}} \\left[ \\frac{1}{2} \\left( Q_{\\theta}(s_t, a_t) - \\hat{Q}(s_t, a_t)\\right)^2 \\right] \\] <p>with : </p> \\[ \\hat{Q}(s_t, a_t) = r(s_t, a_t) + \\gamma \\space \\mathbb{E}_{s_{t+1} \\sim p} [V_{\\bar{\\psi}}(s_{t+1})] \\] <p>Gradient Update for \\(Q_{\\theta}\\):</p> \\[ \\hat{\\nabla}_\\theta J_Q(\\theta) = \\nabla_{\\theta} Q_{\\theta}(s_t, a_t) \\left( Q_{\\theta}(s_t, a_t) - r(s_t, a_t) - \\gamma V_{\\bar{\\psi}}(s_{t+1}) \\right) \\] <p>A target value function \\(V_{\\bar{\\psi}}\\) (exponentially moving average of \\(V_{\\psi}\\)) is used to stabilize training.</p> More Explanation About Target Networks <p>The use of target networks is motivated by a problem in training \\(V\\) network. If you go back to the objective functions in the Theory section, you will find that the target for the \\(Q\\) network training depends on the \\(V\\) Network and the target for the \\(V\\) Network depends on the \\(Q\\) network (this makes sense because we are trying to enforce Bellman Consistency between the two functions). Because of this, the \\(V\\) network has a target that\u2019s indirectly dependent on itself which means that the \\(V\\) network\u2019s target depends on the same parameters we are trying to train. This makes training very unstable.</p> <p>The solution is to use a set of parameters which comes close to the parameters of the main \\(V\\) network, but with a time delay. Thus we create a second network which lags the main network called the target network. There are two ways to go about this.</p> <ol> <li> <p>Periodic Hard Update</p> <p>This method involves completely overwriting the target network\u2019s parameters (\\(\\theta^{-}\\)) with the main network\u2019s parameters (\\(\\theta\\)) at regular intervals (after a fixed number of steps).</p> <ul> <li> <p>Purpose: The periodic hard update ensures the target network aligns closely with the main network, preventing significant divergence between them.</p> </li> <li> <p>Key Characteristics:  </p> <ul> <li>Sudden updates: The target network parameters are replaced entirely at specific intervals.</li> <li>Simplicity: Easy to implement without requiring complex calculations.</li> <li>Stability: Reduces computational overhead during training.</li> </ul> </li> <li> <p>Equation: </p> </li> </ul> \\[ \\theta^{-} \\leftarrow \\theta \\] <ul> <li>Drawback: The abrupt change can lead to instability in learning if the main network's parameters shift significantly during training. It may result in fluctuations in performance for some environments.</li> </ul> </li> <li> <p>Soft Update </p> <p>Soft updates use Polyak averaging (a kind of moving averaging), a method where the target network\u2019s parameters (\\(\\theta^{-}\\)) are updated gradually based on a weighted combination of the current main network\u2019s parameters (\\(\\theta\\)) and the existing target network\u2019s parameters.</p> <ul> <li> <p>Purpose: This smooth transition avoids abrupt shifts and allows the target network to slowly converge towards the main network\u2019s parameters, promoting stability in learning.</p> </li> <li> <p>Key Characteristics:  </p> <ul> <li>Incremental updates: Parameters are updated gradually at each training step.</li> <li>Flexibility: The weighting factor \\(\\tau\\) (a small constant, e.g., 0.001) controls the speed of convergence.(\\(\\tau \\ll 1\\))</li> <li>Stability: Ensures smooth transitions and minimizes sudden changes.</li> </ul> </li> <li> <p>Equation: </p> </li> </ul> \\[ \\theta^{-} \\leftarrow \\tau \\theta + (1-\\tau) \\theta^{-} \\] <ul> <li> <p>Advantages:  </p> <ul> <li>Gradual updates reduce instability in learning.</li> <li>Ideal for environments requiring smooth and stable convergence.</li> </ul> </li> <li> <p>Trade-off: Slower adaptation to the main network\u2019s parameters compared to hard updates, but the added stability usually outweighs this drawback.</p> </li> </ul> </li> </ol> <p> </p> <p>source: concept target network in category reinforcement learning</p> <p>other links:</p> <p>Deep Q-Network -- Tips, Tricks, and Implementation</p> <p>How and when should we update the Q-target in deep Q-learning?</p>"},{"location":"course_notes/advanced/#policy-network-pi_phias","title":"Policy network (\\(\\pi_{\\phi}(a|s)\\))","text":"<p>The policy \\(\\pi_{\\phi}(a | s)\\) is updated using the soft policy improvement step, minimizing the KL-divergence:</p> \\[ J_{\\pi}(\\phi) = \\mathbb{E}_{s_t \\sim \\mathcal{D}} \\left[ D_{\\text{KL}} \\left( \\pi_{\\phi}(\\cdot | s_t) \\bigg\\| \\frac{\\exp(Q_{\\theta}(s_t, \\cdot))}{Z_{\\theta}(s_t)} \\right) \\right] \\] <p>Instead of solving this directly, SAC reparameterizes the policy using:</p> \\[ a_t = f_{\\phi}(\\epsilon_t; s_t) \\] <p>This trick is used to make sure that sampling from the policy is a differentiable process so that there are no problems in backpropagating the errors.  \\(\\epsilon_t\\) is random noise vector sampled from fixed distribution (e.g., Spherical Gaussian).</p> Why Reparameterization is Needed? <p>In reinforcement learning, the policy \\(\\pi(a | s)\\) often outputs a probability distribution over actions rather than deterministic actions. The standard way to sample an action is:</p> \\[ a_t \\sim \\pi_{\\phi}(a_t | s_t) \\] <p>However, this sampling operation blocks gradient flow during backpropagation, preventing efficient training using stochastic gradient descent (SGD).</p> <p>Instead of directly sampling \\(a_t\\) from \\(\\pi_{\\phi}(a_t | s_t)\\), we transform a simple noise variable into an action:</p> \\[ a_t = f_{\\phi}(\\epsilon_t, s_t) \\] <ul> <li>\\(\\epsilon_t \\sim \\mathcal{N}(0, I)\\) is sampled from a fixed noise distribution (e.g., a Gaussian).</li> <li>\\(f_{\\phi}(\\epsilon_t, s_t)\\) is a differentiable function (often a neural network) that maps noise to an action.</li> </ul> <p>For a Gaussian policy in SAC, the action is computed as:</p> \\[ a_t = \\mu_{\\phi}(s_t) + \\sigma_{\\phi}(s_t) \\cdot \\epsilon_t \\] <p>So instead of sampling from \\(\\mathcal{N}(\\mu, \\sigma^2)\\) directly, we sample from a fixed standard normal and transform it using a differentiable function.This makes the policy differentiable, allowing gradients to flow through \\(\\mu_{\\phi}\\) and \\(\\sigma_{\\phi}\\).</p> <p> </p> <ul> <li>Continuous Action Generation</li> </ul> <p>In a continuous action space soft actor-critic agent, the neural network in the actor takes the current observation and generates two outputs, one for the mean and the other for the standard deviation. To select an action, the actor randomly selects an unbounded action from this Gaussian distribution. If the soft actor-critic agent needs to generate bounded actions, the actor applies tanh and scaling operations to the action sampled from the Gaussian distribution.</p> <p>During training, the agent uses the unbounded Gaussian distribution to calculate the entropy of the policy for the given observation.  </p> <ul> <li>Discrete Action Generation</li> </ul> <p>In a discrete action space soft actor-critic agent, the actor takes the current observation and generates a categorical distribution, in which each possible action is associated with a probability. Since each action that belongs to the finite set is already assumed feasible, no bounding is needed.</p> <p>During training, the agent uses the categorical distribution to calculate the entropy of the policy for the given observation.</p> <p>if we rewrite the equation we have:</p> \\[ J_{\\pi}(\\phi) = \\mathbb{E}_{s_t \\sim \\mathcal{D}, \\epsilon_t \\sim \\mathcal{N}} \\left[ \\text{log}\\space\\pi_{\\phi} \\left(f_{\\phi}(\\epsilon_t; s_t) | s_t \\right) - Q_{\\theta}(s_t,f_{\\phi}(\\epsilon_t; s_t) \\right] \\] <p>where \\(\\pi_{\\phi}\\) is defined implicitly in terms of \\(f_{\\phi}\\), and we have noted that the partition function is independent of \\(\\phi\\) and can thus be omitted.</p> <p>Policy Gradient Update</p> \\[ \\hat{\\nabla}_{\\phi} J_{\\pi}(\\phi) = \\nabla_{\\phi} \\log \\pi_{\\phi}(a_t | s_t) + \\left( \\nabla_{a_t} \\log \\pi_{\\phi}(a_t | s_t)- \\nabla_{a_t} Q_{\\theta}(s_t, a_t) \\right) \\nabla_{\\phi} f_{\\phi}(\\epsilon_t; s_t) \\] <p>SAC main steps</p> <ol> <li> <p>Q-function Update:</p> \\[ Q(s, a) \\leftarrow r(s, a) + \\mathbb{E}_{s' \\sim p, a' \\sim \\pi} \\left[ Q(s', a') - \\log \\pi(a' | s') \\right]. \\] <p>This update converges to \\(Q^\\pi\\), the Q-function under the current policy \\(\\pi\\).</p> </li> <li> <p>Policy Update:</p> \\[ \\pi_{\\text{new}} = \\arg\\min_{\\pi'} D_{KL} \\left( \\pi'(\\cdot | s) \\Bigg\\| \\frac{1}{Z} \\exp Q^{\\pi_{\\text{old}}}(s, \\cdot) \\right). \\] <p>In practice, only one gradient step is taken on this objective to ensure stability.</p> </li> <li> <p>Interaction with the Environment:     Collect more data by interacting with the environment using the updated policy.</p> </li> </ol> <p>Algorithm: Soft Actor-Critic</p> <p>Initialize parameter vectors \\(\\psi, \\bar{\\psi}, \\theta, \\phi\\)</p> <p>for each iteration do for each environment step do \\(a_t \\sim \\pi_{\\phi}(a_t | s_t)\\) \\(s_{t+1} \\sim p(s_{t+1} | s_t, a_t)\\) \\(\\mathcal{D} \\gets \\mathcal{D} \\cup \\{(s_t, a_t, r(s_t, a_t), s_{t+1})\\}\\) end for </p> <p> for each gradient step do \\(\\psi \\gets \\psi - \\lambda \\hat{\\nabla}_{\\psi} J_V(\\psi)\\) \\(\\theta_i \\gets \\theta_i - \\lambda_Q \\hat{\\nabla}_{\\theta_i} J_Q(\\theta_i) \\quad \\text{for } i \\in \\{1,2\\}\\) \\(\\phi \\gets \\phi - \\lambda_{\\pi} \\hat{\\nabla}_{\\phi} J_{\\pi}(\\phi)\\) \\(\\bar{\\psi} \\gets \\tau \\psi + (1 - \\tau) \\bar{\\psi}\\) end for end for</p>"},{"location":"course_notes/advanced/#conclusion","title":"Conclusion","text":""},{"location":"course_notes/advanced/#reward-to-go_1","title":"Reward-to-Go","text":"<p>Reward-to-Go computes the sum of future rewards starting from a specific timestep, ensuring that the agent optimizes actions based on expected future returns rather than full trajectory rewards.</p> \\[ R_t = \\sum_{k=t}^{T} \\gamma^{k-t} r_k \\] <p>where \\(\\gamma\\) is the discount factor.</p> <ul> <li> <p>Advantages: </p> <ul> <li>More efficient than using complete trajectory returns.  </li> <li>Enhances learning by assigning appropriate importance to past actions based on their future impact.</li> </ul> </li> <li> <p>Disadvantages: </p> <ul> <li>Still introduces variance in training.  </li> <li>Can be unstable if the reward structure is sparse or highly delayed.  </li> </ul> </li> </ul>"},{"location":"course_notes/advanced/#advantage-estimation","title":"Advantage Estimation","text":"<p>The advantage function quantifies how much better a specific action is compared to the expected return under the current policy. It is defined as:  </p> \\[ A(s_t, a_t) = Q(s_t, a_t) - V(s_t) \\] <ul> <li> <p>Advantages: </p> <ul> <li>Reduces variance in policy gradient updates compared to directly using return estimates.  </li> <li>Helps improve stability in training by distinguishing between good and bad actions.  </li> </ul> </li> <li> <p>Disadvantages: </p> <ul> <li>Requires an accurate value function estimation.  </li> <li>Can still introduce bias if the value function is not well-trained.  </li> </ul> </li> </ul>"},{"location":"course_notes/advanced/#generalized-advantage-estimation-gae_1","title":"Generalized Advantage Estimation (GAE)","text":"<p>GAE improves advantage estimation by introducing a trade-off between bias and variance, using an exponentially-weighted sum of temporal-difference (TD) residuals:</p> \\[ A_t^{GAE(\\lambda)} = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l} \\] <p>where \\(\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)\\). The parameter \\(\\lambda\\) determines the trade-off:  </p> <ul> <li>Low \\(\\lambda\\) (close to 0) \u2192 More bias, less variance (TD-learning).  </li> <li> <p>High \\(\\lambda\\) (close to 1) \u2192 Less bias, more variance (Monte Carlo estimation).  </p> </li> <li> <p>Advantages: </p> <ul> <li>Provides a tunable balance between bias and variance.  </li> <li>Improves sample efficiency by reducing variance in advantage estimates.  </li> </ul> </li> <li> <p>Disadvantages: </p> <ul> <li>Requires careful tuning of \\(\\lambda\\) for optimal performance.  </li> <li>Slightly increases computational overhead due to extra calculations.  </li> </ul> </li> </ul>"},{"location":"course_notes/advanced/#comparison-of-reward-to-go-advantage-estimation-and-gae","title":"Comparison of Reward-to-Go, Advantage Estimation, and GAE","text":"Method Variance Reduction Bias Stability Computational Cost Reward-to-Go Moderate No bias Moderate Low Advantage Estimation High Some bias High Moderate GAE Adjustable (via \\(\\lambda\\)) Adjustable High Higher"},{"location":"course_notes/advanced/#proximal-policy-optimization-ppo_1","title":"Proximal Policy Optimization (PPO)","text":"<p>PPO is an on-policy actor-critic algorithm that improves stability by constraining policy updates with a clipped objective function. It builds upon Trust Region Policy Optimization (TRPO) while being simpler to implement.  </p> <ul> <li> <p>Advantages: </p> <ul> <li>Ensures stable updates by limiting drastic policy changes.  </li> <li>Works well in large-scale reinforcement learning problems.  </li> <li>Simple to implement compared to TRPO.  </li> <li>Effective for environments with discrete and continuous action spaces.  </li> </ul> </li> <li> <p>Disadvantages: </p> <ul> <li>As an on-policy method, it requires more samples for training.  </li> <li>Less sample-efficient than off-policy methods like DDPG and SAC.  </li> <li>Clipping can sometimes slow down convergence if not tuned properly.  </li> </ul> </li> </ul>"},{"location":"course_notes/advanced/#deep-deterministic-policy-gradient-ddpg","title":"Deep Deterministic Policy Gradient (DDPG)","text":"<p>DDPG is an off-policy, model-free algorithm designed for continuous action spaces. It extends Deterministic Policy Gradient (DPG) by incorporating experience replay and target networks, inspired by Deep Q-Networks (DQN).  </p> <ul> <li> <p>Advantages: </p> <ul> <li>More sample-efficient than on-policy methods like PPO.  </li> <li>Can handle high-dimensional continuous action spaces effectively.  </li> <li>Uses replay buffers to break correlation in training data.  </li> </ul> </li> <li> <p>Disadvantages: </p> <ul> <li>Highly sensitive to hyperparameters like learning rates and noise scaling.  </li> <li>Poor exploration due to its deterministic policy, requiring techniques like Ornstein-Uhlenbeck (OU) noise.  </li> <li>Prone to instability due to function approximation errors in Q-learning.  </li> </ul> </li> </ul>"},{"location":"course_notes/advanced/#soft-actor-critic-sac_1","title":"Soft Actor-Critic (SAC)","text":"<p>SAC is an off-policy algorithm that improves upon DDPG by introducing entropy regularization, which encourages exploration and robustness. It uses two Q-networks (double Q-learning) to mitigate overestimation bias and a stochastic policy for improved exploration.  </p> <ul> <li> <p>Advantages: </p> <ul> <li>Better exploration due to entropy regularization.  </li> <li>More stable than DDPG because of double Q-learning.  </li> <li>Handles high-dimensional continuous control tasks efficiently.  </li> <li>More sample-efficient than PPO.  </li> </ul> </li> <li> <p>Disadvantages: </p> <ul> <li>Higher computational cost due to multiple Q-function updates.  </li> <li>Requires careful tuning of entropy coefficient \\(\\alpha\\).  </li> <li>Slower than DDPG in deterministic settings where exploration is not an issue.  </li> </ul> </li> </ul>"},{"location":"course_notes/advanced/#comparison-table-ppo-vs-ddpg-vs-sac","title":"Comparison Table: PPO vs. DDPG vs. SAC","text":"Algorithm Type Sample Efficiency Stability Exploration PPO On-policy Low High Moderate DDPG Off-policy High Moderate Weak (deterministic) SAC Off-policy High High Strong (entropy regularization)"},{"location":"course_notes/advanced/#final-thoughts","title":"Final Thoughts","text":"<p>Actor-critic methods such as PPO, DDPG, and SAC have significantly improved reinforcement learning, making it more scalable and sample-efficient.  </p> <ul> <li>PPO is widely used for its stability and ease of implementation but is less sample-efficient.  </li> <li>DDPG works well in continuous control but suffers from poor exploration and instability.  </li> <li>SAC improves upon DDPG by adding entropy regularization, leading to better exploration and stability.  </li> </ul>"},{"location":"course_notes/advanced/#authors","title":"Author(s)","text":"<ul> <li> <p>Ahmad Karami</p> <p>Teaching Assistant</p> <p>ahmad.karami77@yahoo.com</p> <p> </p> </li> <li> <p>Hamidreza Ebrahimpour</p> <p>Teaching Assistant</p> <p>ebrahimpour.7879@gmail.com</p> <p> </p> </li> <li> <p>Hesam Hosseini</p> <p>Teaching Assistant</p> <p>hesam138122@gmail.com</p> <p> </p> </li> </ul>"},{"location":"course_notes/advanced/#references","title":"References","text":"<ol> <li> <p>Reinforcement Learning Explained</p> </li> <li> <p>An Introduction to Deep Reinforcement Learning</p> </li> <li> <p>Deep Reinforcement Learning Processor Design for Mobile Applications</p> </li> <li> <p>Policy Gradient Algorithms</p> </li> <li> <p>Deep Reinforcement Learning</p> </li> <li> <p>Reinforcement Learning (BartoSutton)</p> </li> <li> <p>Spinning Up in Deep RL!</p> </li> <li> <p>CleanRL Algorithms</p> </li> </ol>"},{"location":"course_notes/bandits/","title":"Week 6: Multi-Armed Bandits","text":""},{"location":"course_notes/bandits/#introduction","title":"Introduction","text":"<p>The multi-armed bandit (MAB) problem represents one of the simplest yet profoundly insightful frameworks for analyzing the fundamental dilemma known as the exploration-exploitation tradeoff in decision-making under uncertainty. This tradeoff arises naturally whenever an agent faces multiple choices whose outcomes are uncertain, requiring it to continually balance between exploring unknown actions to discover their potential rewards and exploiting known actions to maximize immediate returns. The elegance and simplicity of the MAB setup enable rigorous theoretical analysis while maintaining relevance to numerous practical scenarios.</p>"},{"location":"course_notes/bandits/#formal-problem-statement","title":"Formal Problem Statement","text":"<p>Formally, a multi-armed bandit problem can be modeled as a simplified form of a Markov Decision Process (MDP) characterized solely by an action set and reward functions, without state dynamics. Specifically, the bandit setup is represented by the tuple \\((\\mathcal{A}, \\mathcal{R})\\), where:</p> <ul> <li> <p>Action Set: \\(\\mathcal{A}\\) is a finite set of discrete actions, often referred to as \"bandit arms,\" indexed by \\(a = 1, 2, \\dots, k\\). Here, \\(k\\) denotes the total number of available actions.</p> </li> <li> <p>Reward Distributions: Each action \\(a \\in \\mathcal{A}\\) is associated with a distinct probability distribution over rewards, denoted by \\(\\mathcal{R}^a\\). Formally, the reward obtained from action \\(a\\) at time step \\(t\\), represented as \\(R_t\\), is sampled from this distribution:</p> </li> </ul> \\[ R_t \\sim \\mathcal{R}^{A_t}, \\quad \\text{where } A_t \\in \\mathcal{A} \\] <p>This means the reward for choosing action \\(a\\) is a random variable with a specific but unknown probability distribution.</p> <ul> <li>Objective: The goal of the agent in this setting is explicitly to maximize the cumulative reward collected over a finite horizon of \\(T\\) steps:</li> </ul> \\[ G_T = \\sum_{t=1}^{T} R_t \\]"},{"location":"course_notes/bandits/#action-value-functions-q-values","title":"Action-Value Functions (Q-values)","text":"<p>To formally analyze and optimize decisions in the multi-armed bandit problem, we define an essential concept known as the action-value or Q-value of an action. The Q-value of an action represents its expected or average reward:</p> \\[ q(a) = \\mathbb{E}[R \\mid A = a] = \\int_{-\\infty}^{\\infty} r \\cdot \\mathcal{R}^{a}(r) \\,dr \\] <p>In simpler terms, the action-value \\(q(a)\\) captures the average reward the agent can expect if it repeatedly selects action \\(a\\). Estimating these action-values accurately is central to solving bandit problems, as optimal actions will naturally correspond to those with higher Q-values.</p>"},{"location":"course_notes/bandits/#optimal-action-and-optimal-value","title":"Optimal Action and Optimal Value","text":"<p>Within the multi-armed bandit framework, there always exists at least one optimal action, denoted by \\(a^\\star\\), that maximizes the expected reward. The corresponding maximum Q-value, known as the optimal value, is defined as:</p> \\[ v_\\star = q(a^\\star) = \\max_{a \\in \\mathcal{A}} q(a) \\] <p>Identifying the optimal action is the primary challenge, as the agent initially lacks knowledge about the reward distributions and must learn through interaction.</p>"},{"location":"course_notes/bandits/#exploration-vs-exploitation-core-difficulty","title":"Exploration vs. Exploitation: Core Difficulty","text":"<p>The fundamental difficulty faced by agents in the MAB scenario arises precisely from the lack of initial knowledge about the underlying reward distributions. The agent must simultaneously accomplish two conflicting tasks:</p> <ul> <li> <p>Exploration: By choosing less-understood or infrequently selected arms, the agent gathers crucial information about their reward structures. Exploration can yield long-term benefits by identifying potentially superior actions.</p> </li> <li> <p>Exploitation: By selecting the actions known to yield high rewards, the agent maximizes immediate returns. Excessive exploitation, however, risks prematurely converging to suboptimal actions due to inadequate exploration.</p> </li> </ul> <p>Balancing these two aspects to maximize cumulative reward over time forms the crux of solving any bandit problem effectively.</p>"},{"location":"course_notes/bandits/#non-associativity-property","title":"Non-Associativity Property","text":"<p>One unique characteristic of the multi-armed bandit setting, which significantly simplifies its theoretical analysis compared to general MDPs, is the property of non-associativity. Formally:</p> <ul> <li> <p>Non-associativity means the optimal action does not depend on any notion of \"state\" or previous actions. In other words, the bandit problem does not include state transitions\u2014each action choice is independent of any past or future decision.</p> </li> <li> <p>Therefore, the optimal action \\(a^\\star\\) remains constant for all time steps, unaffected by previously selected actions. Mathematically, no state-based transition probabilities or value functions conditioned on states are necessary, making the bandit problem a purely action-oriented optimization scenario.</p> </li> </ul> <p>This non-associativity greatly simplifies both theoretical and practical treatment, allowing researchers to isolate the core exploration-exploitation dynamics from more complex temporal or state-dependent phenomena.</p>"},{"location":"course_notes/bandits/#real-world-applications","title":"Real-World Applications","text":"<p>Despite its apparent simplicity, the multi-armed bandit framework finds extensive applications across diverse fields, where efficient decision-making under uncertainty directly influences outcomes. Some key areas include:</p> <ul> <li> <p>Medical Trials: Clinical research often faces the challenge of testing multiple treatments while minimizing patient risk. MAB strategies help researchers adaptively assign treatments, effectively balancing learning (exploring treatment efficacy) and optimizing patient outcomes (exploiting effective treatments).</p> </li> <li> <p>Online Advertising: Digital platforms utilize MAB algorithms to dynamically select advertisements that maximize user engagement and revenue. By continuously balancing exploration of new ads and exploitation of proven performers, businesses optimize long-term profits.</p> </li> <li> <p>Recommendation Systems: Platforms like streaming services or e-commerce websites employ MAB methods to personalize content delivery. Adaptive recommendation algorithms efficiently learn user preferences by experimenting with various content while maintaining user satisfaction.</p> </li> <li> <p>Financial Investment: Asset allocation and portfolio management tasks naturally map onto bandit problems, where investment decisions must balance immediate financial returns against uncertainty about future asset performance. Using MAB-based decision frameworks, investors systematically explore financial instruments to identify strategies that yield superior long-term returns.</p> </li> </ul> <p>In all these applications, the fundamental logic of balancing exploration and exploitation captured by the multi-armed bandit problem remains central to achieving optimal performance under uncertainty.</p>"},{"location":"course_notes/bandits/#action-value-methods-and-types","title":"Action-Value Methods and Types","text":"<p>To effectively approach and solve the Multi-Armed Bandit (MAB) problem, we require a method for accurately estimating the value associated with each action. This value, commonly referred to as the action-value function, denoted by \\(Q_t(a)\\), represents the estimated expected reward of choosing a particular action \\(a\\) at time step \\(t\\). Formally, the goal is for \\(Q_t(a)\\) to approximate the true expected reward \\(q_*(a)\\), as closely as possible:</p> \\[ Q_t(a) \\approx q_*(a). \\] <p>In practice, the exact values \\(q_*(a)\\) are unknown and must be estimated through experience.</p>"},{"location":"course_notes/bandits/#sample-average-estimation","title":"Sample-Average Estimation","text":"<p>A straightforward approach for estimating the action-value is known as sample-average estimation. Under this method, the value of an action \\(a\\) is estimated by averaging all the observed rewards obtained from selecting action \\(a\\) up to time step \\(t\\). The sample-average estimator is formally defined as:</p> \\[ Q_t(a) = \\frac{1}{N_t(a)} \\sum_{i=1}^{N_t(a)} R_i, \\] <p>where: - \\(N_t(a)\\) is the total number of times action \\(a\\) has been selected up to time step \\(t\\).</p> <ul> <li>\\(R_i\\) is the reward received at the \\(i^{th}\\) time action \\(a\\) was selected.</li> </ul> Intuition <p>This method relies on the Law of Large Numbers, where averaging a large number of observations converges to the true expected reward. Initially, the estimates are inaccurate due to limited observations, but as the action is repeatedly selected, the estimate    \\(Q_t(a)\\) increasingly stabilizes and converges towards the true mean reward \\(q_*(a)\\).</p>"},{"location":"course_notes/bandits/#incremental-update-rule-for-efficient-computation","title":"Incremental Update Rule for Efficient Computation","text":"<p>While computing the action-value through sample-average estimation, it would be computationally inefficient and memory-intensive to store and sum all previous rewards each time a new reward is obtained. Instead, an efficient, incremental update rule can be derived, allowing the estimate \\(Q_t(a)\\) to be updated using only the previously calculated estimate and the most recent reward.</p> <p>This incremental rule is given by:</p> \\[ Q_{t+1}(a) = Q_t(a) + \\frac{1}{N_t(a)} \\left(R_t - Q_t(a)\\right). \\] Derivation of the Incremental Update Rule <p>Starting from the definition of the sample-average estimate at the next time step \\(t+1\\), we have:</p> \\[ Q_{t+1}(a) = \\frac{1}{N_{t+1}(a)} \\sum_{i=1}^{N_{t+1}(a)} R_i. \\] <p>Breaking this down into the previous \\(N_t(a)\\) rewards plus the most recent reward \\(R_t\\), we have:</p> \\[ Q_{t+1}(a) = \\frac{1}{N_{t+1}(a)} \\left(\\sum_{i=1}^{N_t(a)} R_i + R_t\\right). \\] <p>We already have from the previous step:</p> \\[ Q_t(a) = \\frac{1}{N_t(a)} \\sum_{i=1}^{N_t(a)} R_i \\quad \\Rightarrow \\quad \\sum_{i=1}^{N_t(a)} R_i = N_t(a)Q_t(a). \\] <p>Substituting this into the equation above gives:</p> \\[ Q_{t+1}(a) = \\frac{1}{N_{t+1}(a)} \\left(N_t(a)Q_t(a) + R_t\\right). \\] <p>Recognizing that \\(N_{t+1}(a) = N_t(a) + 1\\), we can rewrite this as:</p> \\[ Q_{t+1}(a) = Q_t(a) + \\frac{1}{N_t(a) + 1}\\left(R_t - Q_t(a)\\right), \\] <p>which is precisely the incremental update rule. This formulation clearly demonstrates that updating action-value estimates does not require retaining all historical  rewards\u2014only the current estimate, \\(Q_t(a)\\), and the most recent observation, \\(R_t\\), are needed.</p>"},{"location":"course_notes/bandits/#constant-step-size-update-for-nonstationary-problems","title":"Constant Step-Size Update for Nonstationary Problems","text":"<p>The previously discussed sample-average estimation assumes the reward distributions are stationary (constant over time). However, many practical problems involve nonstationary environments, where the true action values can change over time. To handle such scenarios, we introduce a modified update rule that uses a constant step-size parameter \\(\\alpha\\) instead of the diminishing factor \\(\\frac{1}{N_t(a)}\\):</p> \\[ Q_{t+1}(a) = Q_t(a) + \\alpha(R_t - Q_t(a)), \\] <p>where \\(0 &lt; \\alpha \\leq 1\\) determines how much emphasis is placed on recent rewards.</p> <ul> <li>If \\(\\alpha = \\frac{1}{N_t(a)}\\), this formulation reverts back to the sample-average method.</li> <li>If \\(\\alpha\\) is constant and fixed, recent rewards have greater influence, making the estimates more responsive to changes in the environment.</li> </ul> Exponential Weighted Averaging <p>When employing a constant step-size, the estimate effectively becomes an exponentially weighted average of past rewards, giving exponentially decreasing weights to older observations. This becomes clear by expanding the incremental update recursively:</p> \\[ Q_{t+1}(a) = (1 - \\alpha)Q_t(a) + \\alpha R_t \\] <p>Continuing recursively for additional steps, we have:</p> \\[ Q_{t+2}(a) = (1 - \\alpha)^2 Q_t(a) + \\alpha(1 - \\alpha) R_t + \\alpha R_{t+1}. \\] <p>Generalizing this recursive expansion, the influence of the initial estimate \\(Q_0(a)\\) decreases exponentially, and we have the general form:</p> \\[ Q_t(a) = (1 - \\alpha)^t Q_0(a) + \\sum_{i=0}^{t-1} \\alpha(1 - \\alpha)^i R_{t-i}. \\] <p>This explicitly illustrates the exponential weighting mechanism: recent rewards (closer to the current time \\(t\\)) exert a higher influence on the current estimate, while older rewards have their influence gradually diminished by a factor of \\((1 - \\alpha)\\) per time step.</p> <p>This exponential weighting characteristic makes the constant step-size update particularly well-suited for dynamic, nonstationary environments, where quickly adapting to changes in action-value distributions is critical.</p>"},{"location":"course_notes/bandits/#regret-measuring-suboptimality","title":"Regret: Measuring Suboptimality","text":""},{"location":"course_notes/bandits/#concept-of-regret","title":"Concept of Regret","text":"<p>In sequential decision-making, especially within reinforcement learning and multi-armed bandit frameworks, a central concept is the regret. Regret quantifies the notion of lost opportunity incurred by choosing suboptimal actions over optimal ones. Intuitively, regret measures how much better the agent could have performed had it always selected the best possible action available, denoted by \\(a^\\star\\). Formally, we define the instantaneous regret at iteration \\(t\\) as the expected difference between the reward from the optimal action and the reward received from the chosen action \\(A_t\\):</p> \\[ I_t = \\mathbb{E}[v_\\star - q(A_t)], \\] <p>where \\(v_\\star\\) represents the expected reward of the optimal action \\(a^\\star\\), and \\(q(A_t)\\) represents the expected reward from the action actually taken at step \\(t\\).</p>"},{"location":"course_notes/bandits/#total-regret","title":"Total Regret","text":"<p>To evaluate the performance of an agent over a sequence of decisions, we typically consider the cumulative effect of these instantaneous regrets. The total regret over a horizon of \\(t\\) steps is thus:</p> \\[ L_t = \\mathbb{E}\\left[\\sum_{\\tau=1}^{t} (v_\\star - q(A_\\tau))\\right]. \\] <p>Minimizing total regret is directly equivalent to maximizing cumulative reward, making regret a natural performance metric for learning algorithms in reinforcement learning contexts.</p>"},{"location":"course_notes/bandits/#3-regret-gap-and-action-counts","title":"3. Regret, Gap, and Action Counts","text":"<p>To analyze regret in greater detail, we introduce two important concepts:</p> <ul> <li>The action-count \\(N_t(a)\\), which denotes the expected number of times an action \\(a\\) has been selected up to iteration \\(t\\).</li> <li>The gap \\(\\Delta_a\\), defined as the difference between the optimal action's expected value and the expected value of action \\(a\\):</li> </ul> \\[ \\Delta_a = v_\\star - q(a). \\] <p>Given these definitions, the total regret \\(L_t\\) can also be expressed in terms of the gaps and action counts. Specifically, by decomposing the regret according to how often each suboptimal action is chosen, we have:</p> \\[ \\begin{aligned} L_t &amp;= \\mathbb{E}\\left[\\sum_{\\tau=1}^{t} (v_\\star - q(A_\\tau))\\right] \\\\ &amp;= \\sum_{a \\in \\mathcal{A}} \\mathbb{E}[N_t(a)](v_\\star - q(a)) \\\\ &amp;= \\sum_{a \\in \\mathcal{A}} \\mathbb{E}[N_t(a)] \\Delta_a. \\end{aligned} \\] <p>Thus, the problem of regret minimization reduces to minimizing the expected count of suboptimal actions chosen, particularly those with large gaps.</p>"},{"location":"course_notes/bandits/#regret-dynamics-and-algorithmic-insights","title":"Regret Dynamics and Algorithmic Insights","text":"<p>An important insight about regret is how it grows as a function of time \\(t\\) under various strategies. For instance, a purely greedy algorithm\u2014one that selects actions solely based on current value estimates\u2014will exhibit linear regret. This linear growth occurs because the algorithm might prematurely \"lock in\" on a suboptimal action indefinitely, accruing constant regret at each step.</p> <p>One powerful mitigation strategy is known as optimistic initialization, where we deliberately overestimate initial action values. Formally, the action-value estimates \\(Q(a)\\) are updated using an averaging process:</p> \\[ Q(a) = \\frac{1}{N_t(a)} \\sum_{\\tau=1}^{t} \\mathbf{1}(A_\\tau = a) R_\\tau. \\] <p>This optimistic approach incentivizes initial exploration, reducing the chance of permanently settling on a suboptimal action, thereby improving long-term regret performance.</p>"},{"location":"course_notes/bandits/#lower-bound-on-regret-lai-robbins-bound-this-topic-is-beyond-the-scope-of-this-course","title":"Lower Bound on Regret (Lai-Robbins Bound) (This topic is beyond the scope of this course.)","text":"<p>An essential theoretical result by Lai and Robbins (1985) provides a fundamental lower bound on achievable regret growth for any \"consistent\" algorithm\u2014that is, any algorithm whose regret grows sublinearly for all problem instances. Formally, the Lai-Robbins bound is stated as:</p> \\[ \\liminf_{t \\to \\infty} \\frac{L_t}{\\ln t} \\geq \\sum_{a \\mid \\Delta_a &gt; 0} \\frac{\\Delta_a}{D_{\\text{KL}}(\\mathcal{R}^a \\|\\| \\mathcal{R}^{a^\\star})}, \\] <p>where \\(D_{\\text{KL}}(\\mathcal{R}^a || \\mathcal{R}^{a^\\star})\\) is the Kullback\u2013Leibler (KL) divergence between the reward distributions of a suboptimal arm \\(a\\) and the optimal arm \\(a^\\star\\). Intuitively, this bound indicates that arms with smaller gaps (\\(\\Delta_a\\) close to zero) or similar reward distributions to the optimal arm (small KL divergence) inherently require more exploration, resulting in greater regret.</p> liminf <p>In mathematics, the limit inferior (or liminf) of a sequence \\(\\{a_n\\}\\) is defined as:</p> \\[ \\liminf_{n \\to \\infty} a_n = \\lim_{n \\to \\infty} \\left( \\inf \\{a_k : k \\geq n\\} \\right) \\] <p>This expression represents the greatest lower bound of the tail of the sequence, effectively capturing the \"largest eventual minimum\" of the sequence.   </p>"},{"location":"course_notes/bandits/#bernoulli-bandit-case","title":"Bernoulli Bandit Case","text":"<p>In practical scenarios such as Bernoulli bandits, where each action's reward distribution is Bernoulli(\\(\\mu_a\\)), the KL divergence has a closed-form expression:</p> \\[ D_{\\text{KL}}(\\text{Bern}(\\mu_a) \\|\\| \\text{Bern}(\\mu^\\star)) = \\mu^\\star \\ln \\frac{\\mu^\\star}{\\mu_a} + (1 - \\mu^\\star) \\ln \\frac{1 - \\mu^\\star}{1 - \\mu_a}. \\] <p>For large \\(t\\), the Lai-Robbins bound simplifies approximately to:</p> \\[ L_t \\gtrsim \\sum_{a \\mid \\mu_a &lt; \\mu^\\star} \\frac{\\ln t}{\\mu^\\star - \\mu_a}, \\] <p>clearly demonstrating the logarithmic lower bound on regret growth. Thus, no algorithm can improve beyond a logarithmic rate of regret growth for these problem instances.</p>"},{"location":"course_notes/bandits/#problem-dependent-versus-minimax-regret","title":"Problem-Dependent versus Minimax Regret","text":"<p>The regret bounds discussed so far are problem-dependent, reflecting intrinsic characteristics of specific problem instances (such as gaps between arms). Another view is the minimax regret, which considers the worst-case regret across all possible problem instances. For stochastic bandits with fixed reward distributions, the problem-dependent bound (\\(\\Theta(\\ln t)\\)) is generally more informative and achievable compared to the minimax bound, which typically scales as \\(\\Theta(\\sqrt{Kt})\\) in adversarial settings.</p> <p>Several algorithms, including Upper Confidence Bound (UCB) and Thompson Sampling, have been shown to achieve regret growth matching the logarithmic Lai-Robbins lower bound, both asymptotically and in some cases even in constant factors. This optimal performance contrasts starkly with naive strategies such as fixed \\(\\varepsilon\\)-greedy methods, which incur linear regret due to continual exploration.</p>"},{"location":"course_notes/bandits/#the-explorationexploitation-trade-off","title":"The Exploration\u2013Exploitation Trade-off","text":"<p>In sequential decision-making tasks, particularly in the multi-armed bandit problem, maintaining accurate estimates of the value or expected reward of each available action (often called an \"arm\") is essential. However, accurate estimation alone is insufficient. A fundamental and challenging issue emerges naturally from this setting: the exploration\u2013exploitation trade-off. This trade-off encapsulates a strategic decision every agent must confront repeatedly: should it exploit its current knowledge by choosing actions known (or estimated) to yield high rewards, or should it explore uncertain options to gather more information and potentially discover even more rewarding choices?</p>"},{"location":"course_notes/bandits/#formal-definition-and-intuition","title":"Formal Definition and Intuition","text":"<p>Formally, the exploration\u2013exploitation trade-off can be characterized as follows. Consider a bandit problem with a set of arms \\(\\mathcal{A} = \\{1, 2, \\dots, K\\}\\), each arm \\(i\\) associated with a fixed but unknown reward distribution characterized by a mean reward \\(\\mu_i\\). At any time step \\(t\\), the agent selects an arm \\(A_t \\in \\mathcal{A}\\), observes a reward \\(R_t \\sim \\text{distribution}(\\mu_{A_t})\\), and updates its value estimates accordingly. If we denote the agent's estimate of the expected reward of arm \\(i\\) at time \\(t\\) by \\(\\hat{Q}_t(i)\\), the decision about which arm to pull next becomes critical.</p> <p>The key tension arises because of incomplete knowledge: the agent does not initially know the true mean rewards \\(\\mu_i\\). Thus, it must decide between:</p> <ul> <li>Exploitation: Selecting the arm with the highest current estimated value \\(\\hat{Q}_t(i)\\) (greedy choice) to maximize immediate reward.</li> <li>Exploration: Selecting a less certain arm to refine value estimates and possibly discover a superior arm for future exploitation.</li> </ul> <p>Intuitively, excessive exploitation risks converging prematurely to a suboptimal arm due to misleading early observations. On the other hand, excessive exploration continuously incurs opportunity costs by potentially sacrificing immediate rewards. Hence, effective strategies must delicately balance these competing objectives to achieve minimal long-term regret.</p>"},{"location":"course_notes/bandits/#risks-of-pure-exploitation-and-exploration-strategies","title":"Risks of Pure Exploitation and Exploration Strategies","text":"<p>Consider first a purely exploitative approach\u2014commonly referred to as the greedy strategy. Under this policy, at every step after an initial brief exploration period, the agent always selects the arm that currently has the highest empirical mean reward:</p> \\[ A_t = \\arg\\max_{i \\in \\mathcal{A}} \\hat{Q}_t(i) \\] <p>At first glance, this might seem optimal, as the agent is consistently choosing the \"best\" known option. However, such a strategy is vulnerable to early randomness. For example, if the true best arm \\(i^*\\) initially yields a low reward due to chance, while an inferior arm \\(j\\) provides unusually high early rewards, the greedy policy will mistakenly favor the suboptimal arm \\(j\\) indefinitely. Consequently, the agent fails to discover the superior reward potential of arm \\(i^*\\), causing substantial long-term regret. Formally, it can be rigorously proven that the purely greedy strategy incurs linear regret:</p> \\[ R(T) = \\Theta(T), \\quad \\text{as } T \\rightarrow \\infty \\] <p>In contrast, a purely exploratory strategy, which chooses arms uniformly at random or with constant probability regardless of their past performance, guarantees discovery of each arm\u2019s true expected value but at an excessive cost. Because exploration is indiscriminate, the agent continues to select suboptimal arms frequently, incurring unnecessary losses. This continuous exploration also leads to linear regret in expectation:</p> \\[ R(T) = \\Theta(T) \\quad \\text{(pure exploration strategy)} \\] <p>Therefore, neither extreme\u2014pure exploitation nor pure exploration\u2014is desirable. A systematic, controlled approach is required to reduce regret from linear to sublinear growth.</p>"},{"location":"course_notes/bandits/#regret-minimization-and-the-concept-of-optimism","title":"Regret Minimization and the Concept of Optimism","text":"<p>Regret, denoted \\(R(T)\\), measures the cumulative loss of reward compared to always choosing the best possible arm \\(i^*\\) with mean reward \\(\\mu^* = \\max_i \\mu_i\\). Mathematically, regret after \\(T\\) steps is defined as:</p> \\[ R(T) = T \\mu^* - \\sum_{t=1}^{T} \\mu_{A_t} \\] <p>Strategies addressing the exploration\u2013exploitation trade-off aim for sublinear regret growth, typically logarithmic or polynomial, ensuring the average regret per step diminishes as time progresses. This goal motivates the concept of optimism in the face of uncertainty, a foundational principle guiding many effective algorithms. Optimism assumes uncertain actions potentially hold better rewards than currently estimated, encouraging exploration of less well-known arms. As the uncertainty around an arm's estimated value decreases (through repeated selection and reward observation), the optimism naturally decreases, favoring exploitation once the uncertainty sufficiently narrows.</p>"},{"location":"course_notes/bandits/#common-approaches-to-balancing-exploration-and-exploitation","title":"Common Approaches to Balancing Exploration and Exploitation","text":"<p>Several classic strategies systematically manage exploration and exploitation, each embodying optimism in a different way. We'll now skim through them briefly and then dive deeper into each one.</p>"},{"location":"course_notes/bandits/#1-epsilon-greedy-strategy","title":"1. \\(\\epsilon\\)-Greedy Strategy","text":"<p>The \\(\\epsilon\\)-greedy algorithm selects the greedy action with probability \\(1-\\epsilon\\), and explores randomly chosen arms uniformly with probability \\(\\epsilon\\). This approach guarantees continuous exploration but at a simple and fixed rate, allowing eventual convergence toward the optimal arm. The key limitation is the fixed exploration rate, which may remain unnecessarily high as uncertainty decreases, leading to avoidable regret.</p>"},{"location":"course_notes/bandits/#2-optimistic-initial-values","title":"2. Optimistic Initial Values","text":"<p>This approach deliberately initializes all arms\u2019 value estimates \\(\\hat{Q}_0(i)\\) optimistically high. The optimism encourages initial exploration since arms must be repeatedly tested to reduce inflated estimates toward their true values. Eventually, as real performance emerges clearly, exploitation naturally takes over. While effective initially, this method relies heavily on appropriate initial values and may lack flexibility at later stages.</p>"},{"location":"course_notes/bandits/#3-upper-confidence-bound-ucb-algorithms","title":"3. Upper Confidence Bound (UCB) Algorithms","text":"<p>UCB methods use statistical confidence intervals around the estimated values of arms. The algorithm selects actions by:</p> \\[ A_t = \\arg\\max_{i} \\left[ \\hat{Q}_t(i) + \\sqrt{\\frac{2\\ln t}{N_t(i)}} \\right] \\] <p>where \\(N_t(i)\\) denotes the number of times arm \\(i\\) has been chosen by time \\(t\\). The term added to the estimate is larger when arm \\(i\\) is less explored (small \\(N_t(i)\\)), creating optimism toward uncertain arms. This systematic exploration results in a provably logarithmic regret bound, making UCB highly appealing from a theoretical perspective.</p>"},{"location":"course_notes/bandits/#4-thompson-sampling-bayesian-probability-matching","title":"4. Thompson Sampling (Bayesian Probability Matching)","text":"<p>Thompson Sampling employs a Bayesian framework. At each step, the agent draws random samples from posterior distributions representing its belief about arm values and chooses the arm associated with the highest sampled value. This probabilistic matching naturally balances exploration and exploitation, with uncertainty directly encoded in the posterior distributions. Thompson sampling frequently demonstrates excellent empirical and theoretical performance, often achieving state-of-the-art regret bounds.</p>"},{"location":"course_notes/bandits/#exploration-strategies-for-multi-armed-bandits","title":"Exploration Strategies for Multi-Armed Bandits","text":"<p>Multi-armed bandit (MAB) problems embody the fundamental challenge of balancing exploration (gathering information about the uncertain environment) and exploitation (leveraging existing knowledge to maximize rewards). Several exploration strategies have emerged, each employing distinct mechanisms to navigate this critical trade-off. Below, we elaborate on two common strategies\u2014the \u03b5-Greedy algorithm and Optimistic Initial Values\u2014examining their theoretical underpinnings, implementation specifics, and intuitive rationale.</p>"},{"location":"course_notes/bandits/#the-epsilon-greedy-algorithm","title":"The \\(\\epsilon\\)-Greedy Algorithm","text":""},{"location":"course_notes/bandits/#overview-and-motivation","title":"Overview and Motivation","text":"<p>The \\(\\epsilon\\)-greedy algorithm is one of the most fundamental and widely used strategies for balancing the exploration-exploitation trade-off in sequential decision-making problems, particularly in the context of the stochastic multi-armed bandit problem. Its appeal lies in its simplicity and intuitive structure: the agent typically selects what appears to be the best action according to its current knowledge (exploitation), but occasionally takes a random action to gather more information about alternatives (exploration).</p> <p>Formally, consider a \\(K\\)-armed bandit problem, where each arm \\(i \\in \\{1, \\dots, K\\}\\) provides i.i.d. rewards drawn from an unknown distribution with mean \\(\\mu_i\\). The goal is to maximize the cumulative reward over time, or equivalently, minimize the regret with respect to always playing the optimal arm \\(i^* = \\arg\\max_i \\mu_i\\).</p> <p>The \\(\\epsilon\\)-greedy algorithm addresses this by injecting randomness into the action selection process. At each time step \\(t\\), it behaves as follows:</p> \\[ A_t =  \\begin{cases} \\text{random arm from } \\{1,\\dots,K\\}, &amp; \\text{with probability } \\epsilon, \\\\ \\arg\\max_i \\hat{Q}_{t-1}(i), &amp; \\text{with probability } 1 - \\epsilon. \\end{cases} \\] <p>Here, \\(\\hat{Q}_{t-1}(i)\\) is the estimated mean reward of arm \\(i\\) based on observations up to time \\(t-1\\).</p>"},{"location":"course_notes/bandits/#formal-definition","title":"Formal Definition","text":"<p>Let us define the following notation:</p> <ul> <li>Let \\(K\\) be the number of arms.</li> <li>Let \\(X_{i, s}\\) be the \\(s\\)-th observed reward from arm \\(i\\).</li> <li>Let \\(N_t(i)\\) denote the number of times arm \\(i\\) has been selected up to time \\(t\\).</li> <li>Let \\(\\hat{Q}_t(i) = \\frac{1}{N_t(i)} \\sum_{s=1}^{N_t(i)} X_{i,s}\\) denote the empirical mean reward for arm \\(i\\) at time \\(t\\).</li> </ul> <p>At time \\(t+1\\), the \\(\\epsilon\\)-greedy algorithm proceeds as:</p> \\[ A_{t+1} = \\begin{cases} \\text{randomly select an arm from } \\{1, \\dots, K\\}, &amp; \\text{with probability } \\epsilon, \\\\ \\arg\\max_i \\hat{Q}_t(i), &amp; \\text{with probability } 1 - \\epsilon. \\end{cases} \\] <p>The value of \\(\\epsilon \\in [0,1]\\) is typically a small constant (e.g., \\(\\epsilon = 0.1\\)), ensuring occasional exploration while primarily exploiting the current knowledge.</p> Intuition Behind the Algorithm <p>The core idea of \\(\\epsilon\\)-greedy is to ensure that all arms are explored with non-zero probability. This addresses the fundamental problem of uncertainty in estimating the rewards of each arm. Initially, all estimates \\(\\hat{Q}_t(i)\\) are inaccurate due to limited samples. If the algorithm only exploits the current maximum, it risks becoming overconfident in suboptimal arms and permanently ignoring better alternatives.</p> <p>Exploration allows the algorithm to collect more data about all arms, improving the estimates and preventing premature convergence to a suboptimal policy. Exploitation ensures that we are using the best known option most of the time, thus maximizing the expected reward in the short term.</p> <p>The balance is governed by \\(\\epsilon\\): high \\(\\epsilon\\) means more exploration (potentially higher short-term regret), while low \\(\\epsilon\\) means more exploitation (potentially poor long-term performance if the optimal arm is missed early).</p>"},{"location":"course_notes/bandits/#regret-analysis-with-constant-epsilon","title":"Regret Analysis with Constant \\(\\epsilon\\)","text":"<p>The regret of a bandit algorithm at time \\(T\\) is defined as:</p> \\[ R(T) = T \\mu^* - \\mathbb{E} \\left[ \\sum_{t=1}^T \\mu_{A_t} \\right], \\] <p>where \\(\\mu^* = \\max_i \\mu_i\\) is the mean reward of the optimal arm.</p> <p>For constant \\(\\epsilon\\), the agent explores with probability \\(\\epsilon\\) in each round. During exploration, it selects a random arm uniformly from the \\(K\\) options. Hence, even as time progresses and the estimate \\(\\hat{Q}_t\\) of the optimal arm improves, the algorithm will still spend \\(\\epsilon T\\) steps (in expectation) exploring randomly.</p> <p>Let \\(\\Delta_i = \\mu^* - \\mu_i\\) denote the expected reward gap between arm \\(i\\) and the optimal arm. Then, the expected regret due to exploration is roughly:</p> \\[ R_{\\text{explore}}(T) \\approx \\epsilon T \\cdot \\Delta_{\\text{avg}}, \\] <p>where \\(\\Delta_{\\text{avg}} = \\frac{1}{K} \\sum_{i=1}^K \\Delta_i\\) is the average regret per random pull.</p> <p>The expected regret due to exploitation is smaller. With enough exploration, the agent will learn to identify the optimal arm, and during the \\((1 - \\epsilon)T\\) exploitation steps, it will mostly choose the correct arm. Hence, the dominant contribution to regret comes from exploration.</p> <p>Thus, for constant \\(\\epsilon\\), we have:</p> \\[ R(T) = \\Theta(T), \\quad \\text{(linear regret)} \\] <p>and the average regret \\(\\frac{R(T)}{T} \\to \\epsilon \\Delta_{\\text{avg}}\\) as \\(T \\to \\infty\\). Therefore, constant-\\(\\epsilon\\) greedy is not asymptotically optimal.</p>"},{"location":"course_notes/bandits/#convergence-of-hatq_t-estimates","title":"Convergence of \\(\\hat{Q}_t\\) Estimates","text":"<p>Despite its linear regret, constant-\\(\\epsilon\\) greedy does guarantee convergence of estimates. Since there is a fixed, non-zero probability of selecting each arm at every time step, the number of times any given arm \\(i\\) is selected satisfies:</p> \\[ \\mathbb{E}[N_T(i)] \\geq \\epsilon \\cdot \\frac{T}{K}. \\] <p>By the Law of Large Numbers, this ensures:</p> \\[ \\hat{Q}_T(i) \\xrightarrow{a.s.} \\mu_i \\quad \\text{as } T \\to \\infty, \\] <p>for all \\(i\\). Thus, in the limit, the algorithm identifies the optimal arm correctly, but continues to explore forever at a constant rate \u2014 causing the regret to grow linearly over time.</p>"},{"location":"course_notes/bandits/#decaying-epsilon_t-and-sublinear-regret","title":"Decaying \\(\\epsilon_t\\) and Sublinear Regret","text":"<p>To improve performance, one can use a time-dependent exploration rate \\(\\epsilon_t\\) that decreases with \\(t\\). The motivation is that early on, when little is known, exploration should be frequent. As estimates improve, less exploration is needed, and exploitation becomes safer.</p> <p>Common choices for decaying schedules include:</p> <ul> <li>Inverse time decay: \\(\\epsilon_t = \\frac{1}{t}\\)</li> <li>Logarithmic decay: \\(\\epsilon_t = \\frac{c \\ln t}{t}\\), for some constant \\(c &gt; 0\\)</li> <li>Gap-aware decay: \\(\\epsilon_t = \\min\\left\\{1, \\frac{K}{t \\Delta^2}\\right\\}\\) (requires knowledge of gap \\(\\Delta\\))</li> </ul> <p>Under such schedules, we can show that:</p> \\[ R(T) = O(\\ln T), \\] <p>i.e., the regret grows logarithmically, which is the best one can hope for in the stochastic setting (matching the lower bound of Lai and Robbins for the asymptotic regret of consistent algorithms).</p> <p>Sketch of proof (informal intuition): With \\(\\epsilon_t = \\frac{c \\ln t}{t}\\), the total number of exploration steps up to time \\(T\\) is approximately:</p> \\[ \\sum_{t=1}^T \\epsilon_t = \\sum_{t=1}^T \\frac{c \\ln t}{t} \\leq c \\ln^2 T. \\] <p>This means that suboptimal arms are chosen much less frequently over time, and the cumulative regret remains sublinear.</p>"},{"location":"course_notes/bandits/#optimistic-initial-values","title":"Optimistic Initial Values","text":"<p>In the context of multi-armed bandit problems, where an agent must choose among several actions (or \"arms\") to maximize cumulative reward, one of the core challenges is managing the exploration-exploitation trade-off. That is, the agent must balance the need to explore lesser-known actions to gather information with the desire to exploit currently believed-to-be optimal actions to maximize rewards.</p> <p>While a common strategy like \\(\\epsilon\\)-greedy addresses this by injecting randomness into action selection, another elegant and deterministic alternative is optimistic initialization, or optimistic initial values. This approach leverages prior optimism to naturally induce exploration, without relying on explicit stochasticity.</p>"},{"location":"course_notes/bandits/#motivating-intuition","title":"Motivating Intuition","text":"<p>The intuition behind optimistic initial values stems from a simple psychological metaphor: the agent begins with overly optimistic beliefs about the potential payoff of every action. It \u201cbelieves\u201d every arm is excellent \u2014 better than it realistically could be \u2014 and therefore is compelled to try each arm at least once to confirm or refute this belief. Upon playing an arm and observing actual rewards (which are, on average, lower than the initial belief), the agent adjusts its estimate downward. Thus, exploration arises not from randomness, but from disappointment in inflated expectations.</p> <p>This method of optimistic bias is especially powerful in stationary environments, where the underlying reward distributions do not change over time. In such settings, an intense burst of early exploration can suffice, after which the agent can greedily exploit the best-known option based on refined value estimates.</p>"},{"location":"course_notes/bandits/#formal-definition-and-mathematical-formulation","title":"Formal Definition and Mathematical Formulation","text":"<p>Let us consider the standard \\(k\\)-armed bandit problem. Each arm \\(i \\in \\{1, 2, \\dots, k\\}\\) yields stochastic rewards drawn from an unknown and stationary distribution with true mean \\(\\mu_i\\).</p> <p>The agent maintains an estimate \\(\\hat{Q}_t(i)\\) of the mean reward for each arm \\(i\\) at time \\(t\\), which is updated incrementally as rewards are observed. The standard sample average update rule is:</p> \\[ \\hat{Q}_{t+1}(i) = \\hat{Q}_t(i) + \\alpha_t(i) \\left( R_t(i) - \\hat{Q}_t(i) \\right), \\] <p>where:</p> <ul> <li>\\(R_t(i)\\) is the reward observed after playing arm \\(i\\) at time \\(t\\),</li> <li>\\(\\alpha_t(i)\\) is the step size, typically set to \\(1/N_t(i)\\) where \\(N_t(i)\\) is the number of times arm \\(i\\) has been selected by time \\(t\\).</li> </ul> <p>In optimistic initialization, we initialize the estimates as follows:</p> \\[ \\hat{Q}_0(i) = Q^+ \\quad \\text{for all } i, \\] <p>where \\(Q^+\\) is a constant such that \\(Q^+ &gt; \\max_i \\mu_i\\), i.e., it exceeds all plausible true reward means. For example, if rewards are bounded in the interval \\([0, 1]\\), a typical choice is \\(Q^+ = 1\\) or even slightly higher.</p> <p>At each time step \\(t\\), the agent selects the arm with the highest estimated value:</p> \\[ A_t = \\arg\\max_i \\hat{Q}_t(i), \\] <p>which is a purely greedy policy.</p>"},{"location":"course_notes/bandits/#behavioral-dynamics","title":"Behavioral Dynamics","text":"<p>Initially, all estimates \\(\\hat{Q}_0(i) = Q^+\\) are equal and maximal, so the agent arbitrarily picks one. Upon selecting an arm, the estimate is updated based on the reward received. Because actual rewards are typically lower than \\(Q^+\\), the new estimate will decrease. Since all other arms still retain their high initial estimates, the agent is then drawn to try those next. This cyclic effect continues until all arms have been sampled and their estimates revised downward, in proportion to their observed performance.</p> <p>Once each arm has been sampled sufficiently to provide reliable estimates of their true means, the arm with the highest empirical average is selected going forward. At this point, the agent effectively switches from exploration to exploitation \u2014 but crucially, without any explicit exploration parameter.</p>"},{"location":"course_notes/bandits/#comparison-to-epsilon-greedy","title":"Comparison to \\(\\epsilon\\)-Greedy","text":"<p>In contrast to \\(\\epsilon\\)-greedy \u2014 where the agent continues to explore with fixed probability \\(\\epsilon\\) indefinitely \u2014 optimistic initialization focuses exploration into the early stage of learning. Once the overly optimistic estimates are corrected, the algorithm becomes purely greedy. This concentrated exploration phase often leads to faster convergence to optimal behavior, especially when the environment is stationary.</p> <p>Furthermore, optimistic initialization avoids persistent exploration of obviously suboptimal arms, a common downside of \\(\\epsilon\\)-greedy. This leads to reduced long-term regret in many practical scenarios.</p>"},{"location":"course_notes/bandits/#regret-analysis","title":"Regret Analysis","text":"<p>Let us now examine the regret of optimistic initialization from a theoretical standpoint. Define the regret at time \\(T\\) as:</p> \\[ \\text{Regret}(T) = T\\mu^* - \\sum_{t=1}^T \\mathbb{E}[\\mu_{A_t}], \\] <p>where \\(\\mu^* = \\max_i \\mu_i\\) is the mean reward of the optimal arm.</p> <p>Even though the policy is greedy after a short initial phase, optimistic initialization ensures that every arm is sampled sufficiently many times to detect suboptimality. Suppose that \\(Q^+\\) is set such that each suboptimal arm \\(i\\) is pulled at most \\(O\\left(\\frac{1}{\\Delta_i^2} \\log T\\right)\\) times, where \\(\\Delta_i = \\mu^* - \\mu_i\\) is the suboptimality gap. This yields:</p> \\[ \\text{Regret}(T) = O\\left( \\sum_{i: \\Delta_i &gt; 0} \\frac{\\log T}{\\Delta_i} \\right), \\] <p>which matches the asymptotic regret bound of more sophisticated algorithms like Upper Confidence Bound (UCB). Hence, optimistic initialization, despite its simplicity, can achieve logarithmic regret.</p> <p>However, if \\(Q^+\\) is set too optimistically, the agent may spend unnecessary time validating even poor arms. Conversely, if it is set insufficiently optimistically, some arms may not be explored at all. Therefore, the choice of \\(Q^+\\) must be made carefully, ideally based on prior knowledge of the reward bounds.</p>"},{"location":"course_notes/bandits/#upper-confidence-bound-ucb-algorithms-a-detailed-exploration","title":"Upper Confidence Bound (UCB) Algorithms: A Detailed Exploration","text":""},{"location":"course_notes/bandits/#introduction-and-motivation","title":"Introduction and Motivation","text":"<p>In the study of the exploration-exploitation dilemma in stochastic multi-armed bandit (MAB) problems, one of the most elegant and foundational strategies is the Upper Confidence Bound (UCB) algorithm. UCB embodies a principle of decision-making known as optimism in the face of uncertainty. This heuristic encourages an agent to behave optimistically about less-explored actions by constructing upper confidence bounds for their expected rewards and then selecting actions as if these bounds were true estimates of the actual value.</p> <p>The UCB framework is based on a rigorous statistical foundation: if we can form a high-probability upper bound on the true reward of each arm, then choosing the arm with the largest such bound encourages both exploitation of arms that are empirically promising and exploration of arms about which we remain uncertain. </p> <p>To put it simply: the algorithm behaves as if the true reward of each arm is the most optimistic plausible value consistent with the observed data. This naturally balances the dual needs of exploration (gathering information about uncertain arms) and exploitation (using the current best knowledge to make good decisions).</p>"},{"location":"course_notes/bandits/#problem-setup","title":"Problem Setup","text":"<p>We formalize the stochastic K-armed bandit setting as follows:</p> <ul> <li>Let \\(\\mathcal{A} = \\{1, 2, \\dots, K\\}\\) denote the set of \\(K\\) actions or arms.</li> <li>At each time step \\(t = 1, 2, \\dots, T\\), the agent selects an arm \\(A_t \\in \\mathcal{A}\\) and receives a reward \\(R_t \\in [0,1]\\) drawn from a fixed, unknown distribution associated with that arm.</li> <li>Let \\(\\mu_i = \\mathbb{E}[R_t \\mid A_t = i]\\) denote the true expected reward of arm \\(i\\).</li> <li>Let \\(\\mu^* = \\max_i \\mu_i\\) denote the value of the optimal arm.</li> <li>The goal is to minimize regret, defined as the expected difference between the reward accumulated by always playing the optimal arm and the reward collected by the algorithm:</li> </ul> \\[ R(T) = T\\mu^* - \\mathbb{E}\\left[\\sum_{t=1}^T R_t\\right] = \\sum_{i=1}^K \\Delta_i \\mathbb{E}[N_i(T)], \\] <p>where \\(\\Delta_i = \\mu^* - \\mu_i\\) and \\(N_i(T)\\) is the number of times arm \\(i\\) was selected up to time \\(T\\).</p>"},{"location":"course_notes/bandits/#optimism-in-the-face-of-uncertainty","title":"Optimism in the Face of Uncertainty","text":"<p>At the heart of UCB algorithms lies the idea of confidence intervals. Suppose for each arm \\(i\\), we maintain an estimate \\(\\hat{Q}_{t-1}(i)\\) of its true mean \\(\\mu_i\\), based on observed rewards. Alongside this estimate, we compute an upper confidence term \\(U_{t-1}(i)\\), such that with high probability:</p> \\[ \\mu_i \\leq \\hat{Q}_{t-1}(i) + U_{t-1}(i). \\] <p>Rather than just selecting the arm with the highest empirical mean, we select the arm with the highest upper bound, i.e.,</p> \\[ A_t = \\arg\\max_{i \\in \\mathcal{A}} \\hat{Q}_{t-1}(i) + U_{t-1}(i). \\] <p>This choice reflects optimism: we act as if each arm is as good as it could plausibly be, given the data so far. The key insight is that this encourages exploration of less-frequently pulled arms because their confidence intervals are wider (i.e., larger \\(U_{t-1}(i)\\)), and this naturally decreases as more data accumulates.</p>"},{"location":"course_notes/bandits/#derivation-via-hoeffdings-inequality","title":"Derivation via Hoeffding's Inequality","text":"<p>To construct the upper confidence term \\(U_{t-1}(i)\\), we rely on Hoeffding's inequality, a concentration bound for bounded random variables.</p>"},{"location":"course_notes/bandits/#hoeffdings-inequality","title":"Hoeffding\u2019s Inequality:","text":"<p>Let \\(X_1, ..., X_n\\) be i.i.d. random variables with values in \\([0, 1]\\), and let \\(\\bar{X}_n = \\frac{1}{n} \\sum_{j=1}^n X_j\\) be their sample mean. Then for any \\(u &gt; 0\\),</p> \\[ \\Pr\\left\\{ \\mathbb{E}[X] &gt; \\bar{X}_n + u \\right\\} \\leq e^{-2nu^2}. \\] <p>This inequality bounds the probability that the true mean exceeds the empirical mean by more than \\(u\\). Rearranging it gives us a confidence interval: with probability at least \\(1 - \\delta\\),</p> \\[ \\mathbb{E}[X] \\leq \\bar{X}_n + \\sqrt{\\frac{\\ln(1/\\delta)}{2n}}. \\] <p>We apply this inequality to each arm \\(i\\), where \\(X_j\\) is the reward from the \\(j\\)-th time we pulled arm \\(i\\), \\(\\bar{X}_n = \\hat{Q}_{t-1}(i)\\), and \\(n = N_{t-1}(i)\\) is the number of times we\u2019ve pulled arm \\(i\\).</p> <p>This leads us to define:</p> \\[ U_{t-1}(i) = \\sqrt{\\frac{\\ln(1/\\delta_{t-1})}{2N_{t-1}(i)}}, \\] <p>so that with high probability,</p> \\[ \\mu_i \\leq \\hat{Q}_{t-1}(i) + U_{t-1}(i). \\]"},{"location":"course_notes/bandits/#the-ucb1-algorithm","title":"The UCB1 Algorithm","text":"<p>To ensure the confidence holds for all time steps (so that the overall regret is bounded), we define a schedule for \\(\\delta_t\\) that decays with \\(t\\), e.g., \\(\\delta_t = 1/t^2\\) or \\(\\delta_t = 1/t^4\\). Plugging this into our formula gives the well-known UCB1 index:</p> \\[ \\text{UCB}_t(i) = \\hat{Q}_{t-1}(i) + \\sqrt{\\frac{2 \\ln t}{N_{t-1}(i)}}. \\] <p>Then the action selection rule is:</p> \\[ A_t = \\arg\\max_{i \\in \\mathcal{A}} \\left[ \\hat{Q}_{t-1}(i) + \\sqrt{\\frac{2 \\ln t}{N_{t-1}(i)}} \\right]. \\] <p>This algorithm guarantees that each arm is explored enough to maintain confidence, while also converging to exploiting the optimal arm.</p>"},{"location":"course_notes/bandits/#interpretation-and-intuition","title":"Interpretation and Intuition","text":"<p>This formula can be interpreted in two parts:</p> <ul> <li>Exploitation: \\(\\hat{Q}_{t-1}(i)\\) is the empirical mean of rewards \u2014 it represents our current best estimate.</li> <li>Exploration Bonus: \\(\\sqrt{\\frac{2 \\ln t}{N_{t-1}(i)}}\\) is large when the arm has been pulled only a few times (small \\(N\\)) or early in time (small \\(t\\)), and shrinks as \\(N\\) grows.</li> </ul> <p>Crucially, \\(\\ln t\\) grows slowly (logarithmically), so even at large time steps, the algorithm still occasionally re-explores arms with low \\(N_i\\). This ensures that no arm is neglected forever, and yet the frequency of exploration diminishes as the confidence in \\(\\hat{Q}\\) grows.</p>"},{"location":"course_notes/bandits/#regret-analysis-of-ucb1","title":"Regret Analysis of UCB1","text":"<p>The strength of UCB1 lies in its theoretical guarantees. Let \\(\\Delta_i = \\mu^* - \\mu_i\\) denote the suboptimality gap of arm \\(i\\).</p> <p>Auer et al. (2002) proved that:</p> \\[ \\mathbb{E}[N_i(T)] \\leq \\frac{8 \\ln T}{\\Delta_i^2} + O(1), \\] <p>meaning the number of times a suboptimal arm is pulled grows logarithmically with \\(T\\). This leads to the following bound on expected cumulative regret:</p> \\[ \\mathbb{E}[R(T)] = \\sum_{i: \\Delta_i &gt; 0} \\Delta_i \\mathbb{E}[N_i(T)] \\leq \\sum_{i: \\Delta_i &gt; 0} \\left( \\frac{8 \\ln T}{\\Delta_i} + O(\\Delta_i) \\right), \\] <p>which simplifies to:</p> \\[ \\mathbb{E}[R(T)] = O\\left( \\sum_{i: \\Delta_i &gt; 0} \\frac{\\ln T}{\\Delta_i} \\right) = O(\\ln T). \\] <p>This is order-optimal, matching the lower bound for regret in stochastic bandits up to constant factors. Importantly, this bound is problem dependent: larger suboptimality gaps \\(\\Delta_i\\) lead to fewer required explorations, and hence lower regret.</p>"},{"location":"course_notes/bandits/#thompson-sampling-bayesian-probability-matching","title":"Thompson Sampling (Bayesian Probability Matching)","text":"<p>Thompson Sampling (TS), originally proposed by William R. Thompson in 1933, is a foundational algorithm in the domain of sequential decision-making under uncertainty, particularly within the multi-armed bandit (MAB) framework. It embodies a Bayesian philosophy, maintaining a probabilistic belief about the reward-generating distribution of each arm and using this belief to guide arm selection.</p> <p>At its core, Thompson Sampling follows the principle of probability matching: it selects actions (i.e., arms) in proportion to the probability that each is the optimal choice, conditioned on observed data. This leads to a dynamic and adaptive strategy that naturally balances exploration (gathering information about uncertain arms) and exploitation (leveraging the current best guess to maximize reward).</p>"},{"location":"course_notes/bandits/#bayesian-framework","title":"Bayesian Framework","text":"<p>Suppose we have a stochastic K-armed bandit problem with arms indexed by \\(i=1,2,\\dots,K\\). Each arm \\(i\\) is associated with an unknown reward distribution parameterized by \\(\\theta_i\\), and our goal is to maximize cumulative reward over \\(T\\) rounds. At each time \\(t\\), the learner chooses an arm \\(A_t\\), observes a reward \\(R_t\\sim\\mathcal{D}_{A_t}\\), and updates their belief about the corresponding \\(\\theta_{A_t}\\).</p> <p>From a Bayesian standpoint, we place a prior distribution \\(p(\\theta_i)\\) over each \\(\\theta_i\\), and after observing data \\(\\mathcal{H}_t\\) (history up to time \\(t\\)), we update the posterior \\(p(\\theta_i|\\mathcal{H}_t)\\) via Bayes' theorem:</p> \\[ p(\\theta_i|\\mathcal{H}_t)=\\frac{p(\\mathcal{H}_t|\\theta_i)p(\\theta_i)}{p(\\mathcal{H}_t)} \\] <p>Thompson Sampling samples \\(\\tilde{\\theta}_i^{(t)}\\sim p(\\theta_i|\\mathcal{H}_t)\\) for each arm and chooses the arm with the highest sampled expected reward.</p> <p>Formally:</p> \\[ A_t=\\arg\\max_{i\\in\\{1,\\dots,K\\}}\\mathbb{E}[R_t|\\tilde{\\theta}_i^{(t)}] \\]"},{"location":"course_notes/bandits/#bernoulli-bandits-beta-bernoulli-model","title":"Bernoulli Bandits: Beta-Bernoulli Model","text":"<p>To make these ideas concrete, consider the special case where each arm yields Bernoulli rewards:</p> \\[ R_t\\in\\{0,1\\} \\quad \\text{with} \\quad R_t\\sim\\text{Bernoulli}(\\theta_i) \\] <p>We assume that the success probability \\(\\theta_i\\in[0,1]\\) is unknown. The natural conjugate prior for the Bernoulli distribution is the Beta distribution, defined as:</p> \\[ \\theta_i\\sim\\text{Beta}(\\alpha_i,\\beta_i), \\quad \\text{with density:} \\quad p(\\theta_i)=\\frac{\\theta_i^{\\alpha_i-1}(1-\\theta_i)^{\\beta_i-1}}{B(\\alpha_i,\\beta_i)} \\] <p>where \\(B(\\alpha,\\beta)\\) is the beta function (a normalization constant). The Beta distribution is flexible and allows us to express varying degrees of prior belief. For instance, the uninformative uniform prior is \\(\\text{Beta}(1,1)\\).</p>"},{"location":"course_notes/bandits/#algorithm-steps","title":"Algorithm Steps","text":"<p>At each round \\(t\\), the Thompson Sampling algorithm proceeds as follows:</p> <ol> <li>Posterior Sampling:    For each arm \\(i=1,\\dots,K\\), draw:</li> </ol> \\[ \\tilde{\\theta}_i^{(t)}\\sim\\text{Beta}(\\alpha_i,\\beta_i) \\] <ol> <li>Action Selection:    Choose the arm with the highest sampled value:</li> </ol> \\[ A_t=\\arg\\max_{i}\\tilde{\\theta}_i^{(t)} \\] <ol> <li> <p>Reward Observation:    Pull arm \\(A_t\\), observe reward \\(R_t\\in\\{0,1\\}\\)</p> </li> <li> <p>Posterior Update:    Update the Beta parameters:</p> </li> </ol> \\[ \\alpha_{A_t}\\leftarrow\\alpha_{A_t}+R_t, \\quad \\beta_{A_t}\\leftarrow\\beta_{A_t}+(1-R_t) \\] <p>The rest of the arms' parameters remain unchanged.</p> Intuition Behind Exploration and Exploitation <p>This process allows the algorithm to explore uncertain arms and exploit promising ones in a naturally balanced way. Consider an arm \\(i\\) with a high mean estimate but low certainty (wide posterior). There's a non-negligible chance that its sampled \\(\\tilde{\\theta}_i\\) will be large, leading to selection. Conversely, an arm with high empirical reward but tight posterior still occasionally gets out-sampled by a more uncertain one.</p> <p>This phenomenon is called randomized optimism: sometimes, by chance, an uncertain arm is sampled optimistically, leading to its exploration. The more we pull an arm, the narrower its posterior becomes, reducing unnecessary exploration over time.</p>"},{"location":"course_notes/bandits/#extension-beyond-bernoulli-rewards","title":"Extension Beyond Bernoulli Rewards","text":"<p>While the Beta-Bernoulli setup is particularly elegant due to its conjugacy (posterior is analytically tractable), Thompson Sampling extends naturally to other reward models:</p> <ul> <li>Gaussian rewards with unknown mean (known variance): Use a normal prior \\(\\theta_i\\sim\\mathcal{N}(\\mu_i,\\sigma_i^2)\\) </li> <li>Poisson rewards: Use a Gamma prior on the rate parameter \\(\\lambda_i\\) </li> <li>General likelihoods: Use approximate inference (e.g., Monte Carlo methods, variational inference)</li> </ul> <p>In non-conjugate or complex settings, one often resorts to sampling-based approximations of the posterior, such as particle filters or MCMC methods.</p>"},{"location":"course_notes/bandits/#regret-analysis_1","title":"Regret Analysis","text":"<p>Thompson Sampling was initially justified from a Bayesian perspective \u2014 minimizing expected regret under a prior over reward distributions. However, rigorous analysis has shown that TS also achieves strong frequentist guarantees.</p>"},{"location":"course_notes/bandits/#regret-definition","title":"Regret Definition","text":"<p>Let \\(\\mu_i=\\mathbb{E}[R_t|A_t=i]\\) be the expected reward of arm \\(i\\), and let \\(\\mu^*=\\max_i\\mu_i\\) be the optimal reward. Then the cumulative regret over \\(T\\) rounds is:</p> \\[ R(T)=T\\mu^*-\\sum_{t=1}^T\\mathbb{E}[\\mu_{A_t}] \\] <p>For Bernoulli bandits, let \\(\\Delta_i=\\mu^*-\\mu_i\\). Then under mild conditions, the expected regret of Thompson Sampling satisfies:</p> \\[ \\mathbb{E}[R(T)]=O\\left(\\sum_{i:\\Delta_i&gt;0}\\frac{\\ln T}{\\Delta_i^2}\\right) \\] <p>This bound is only slightly looser than the regret of UCB algorithms, which have regret scaling as \\(\\sum_i\\frac{\\ln T}{\\Delta_i}\\). Despite this, Thompson Sampling often outperforms UCB in practice due to better constant factors and more flexible adaptation.</p> <p>In fact, for many distributions, it has been shown that TS asymptotically matches the Lai\u2013Robbins lower bound:</p> \\[ \\liminf_{T\\to\\infty}\\frac{\\mathbb{E}[R(T)]}{\\ln T}\\geq\\sum_{i:\\Delta_i&gt;0}\\frac{\\Delta_i}{D(\\mu_i\\|\\mu^*)} \\] <p>where \\(D(p\\|q)\\) is the Kullback\u2013Leibler divergence between the reward distributions of arms \\(i\\) and the optimal arm.</p>"},{"location":"course_notes/bandits/#contextual-bandits","title":"Contextual Bandits","text":"<p>While standard multi-armed bandits assume no additional data or \u201ccontext\u201d is available when selecting an arm, many real-world applications present extra information\u2014sometimes called features or context\u2014that can help guide the choice of action. This setting is known as a contextual bandit or bandit with side information.</p>"},{"location":"course_notes/bandits/#motivation-and-setup","title":"Motivation and Setup","text":"<p>In a contextual bandit problem, at each time step \\(t\\):</p> <ol> <li>The environment reveals a context \\(x_t \\in \\mathcal{X}\\).  </li> <li>Based on this context, the agent chooses an action (arm) \\(A_t \\in \\{1, \\dots, K\\}\\).  </li> <li>The chosen action yields a reward \\(R_t\\), drawn from a distribution that can depend on both the action and the context.</li> </ol> <p>Formally, we might write:</p> \\[ R_t \\sim \\mathcal{R}\\bigl(a = A_t, x = x_t\\bigr) \\] <p>Here, \\(\\mathcal{X}\\) is a (possibly high-dimensional) space of contexts. The agent\u2019s goal remains to maximize cumulative reward (or minimize regret), but now it can exploit the relationship between (context, action) and reward.</p>"},{"location":"course_notes/bandits/#distinction-from-standard-mab","title":"Distinction from Standard MAB","text":"<ul> <li>In standard MAB, the same arms are offered in every round, with no side information, and each arm has a single reward distribution.  </li> <li>In contextual bandits, each arm\u2019s reward distribution changes depending on the context \\(\\(x\\)\\). The agent must learn a context-to-action mapping (a policy) that predicts which arm will perform best in each situation.</li> </ul>"},{"location":"course_notes/bandits/#example-use-cases","title":"Example Use Cases","text":"<ul> <li>News Article Recommendation </li> <li>Personalized Medicine </li> <li>Targeted Advertising</li> </ul>"},{"location":"course_notes/bandits/#linucb-algorithm","title":"LinUCB Algorithm","text":"<p>One of the canonical and most influential approaches to contextual bandits is the LinUCB algorithm. LinUCB is designed for problems where the reward can be assumed (or approximated) to be a linear function of the context. This approach was popularized by Li et al. (2010), where it was used for news article recommendation; see also the original UCB framework by Auer et al. (2002) for the theoretical underpinnings of confidence-bound methods.</p>"},{"location":"course_notes/bandits/#linear-contextual-model","title":"Linear Contextual Model","text":"<p>Assume the reward from arm \\(i\\) when context \\(x_t \\in \\mathbb{R}^d\\) is presented has an expected value of the form:</p> \\[ \\mathbb{E}[R_t \\mid x_t, A_t = i] = x_t^\\top \\theta_i, \\] <p>where \\(\\theta_i \\in \\mathbb{R}^d\\) is an unknown weight vector for arm \\(i\\). Each arm \\(i\\) thus corresponds to a particular linear relationship between context and reward. In practice, this means if you have a \\(d\\)-dimensional feature vector \\(x_t\\) representing the context at time \\(t\\), the arm\u2019s expected payoff is captured by the dot product between \\(x_t\\) and the parameter vector \\(\\theta_i\\).</p> <ul> <li>Why linearity? The assumption of linearity often arises from modeling each component of \\(x_t\\) as contributing additively to the reward. While real-world relationships may be more complex, linear approximations can be quite effective in high-dimensional settings, especially when combined with feature engineering.</li> </ul>"},{"location":"course_notes/bandits/#algorithm-structure","title":"Algorithm Structure","text":"<p>At a high level, LinUCB maintains an estimate \\(\\hat{\\theta}_i\\) for each arm \\(i\\). To account for uncertainty in \\(\\hat{\\theta}_i\\), it constructs an upper confidence bound for the expected reward of each arm, thereby balancing exploration and exploitation (see Abbasi-Yadkori et al. (2011) for in-depth theoretical analysis of this confidence set approach).</p> <ol> <li>Initialization (for each arm \\(i\\)):</li> <li>\\(A_i = I_{d\\times d}\\) (identity matrix)  </li> <li>\\(b_i = 0\\) (zero vector in \\(\\mathbb{R}^d\\))</li> </ol> <p>Here, \\(A_i\\) and \\(b_i\\) can be understood in terms of ridge regression: they will accumulate the contextual data and observed rewards for arm \\(i\\), respectively.</p> <ol> <li>At time \\(t\\), upon receiving context \\(x_t\\):</li> <li>For each arm \\(i\\):</li> </ol> \\[ \\hat{\\theta}_i = A_i^{-1} \\, b_i \\] \\[ p_i(t) = x_t^\\top \\hat{\\theta}_i + \\alpha \\sqrt{x_t^\\top A_i^{-1} x_t} \\] <p>where $ \\alpha $ is an exploration parameter controlling how \u201coptimistic\u201d the estimate is, and $ \\sqrt{x_t^\\top A_i^{-1} x_t} $ measures uncertainty in the linear reward estimate. The larger this term, the less data we have for arm \\(i\\) under similar contexts, so the algorithm encourages exploration of that arm.</p> <ul> <li>Select arm \\(A_t = \\arg\\max_i \\; p_i(t)\\).</li> </ul> <p>Intuitively, \\(p_i(t)\\) combines the current best guess (\\(x_t^\\top \\hat{\\theta}_i\\)) with a statistical bonus (\\(\\alpha \\sqrt{x_t^\\top A_i^{-1} x_t}\\)). This reflects the principle of optimism in the face of uncertainty: an action with limited data is given a higher \u201coptimistic\u201d estimate, prompting additional exploration.</p> <ol> <li>Observe reward \\(R_t\\). Update:</li> </ol> \\[ A_{A_t} \\leftarrow A_{A_t} + x_t x_t^\\top,  \\quad b_{A_t} \\leftarrow b_{A_t} + R_t \\, x_t. \\] <p>These updates are analogous to incrementally solving a regularized least-squares problem for each arm\u2019s parameters. After enough pulls, \\(A_i\\) becomes well-conditioned, shrinking the confidence interval in \\(p_i(t)\\) for arm \\(i\\).</p>"},{"location":"course_notes/bandits/#usage-of-linucb-in-contextual-bandits","title":"Usage of LinUCB in Contextual Bandits","text":"<p>LinUCB is particularly effective when the context-reward relationship is (or is close to) linear. It scales well to large time horizons so long as the context dimension \\(d\\) is not too large, because the key matrix inverse \\(A_i^{-1}\\) is only \\(d \\times d\\).</p> <ul> <li>Feature Construction: If the raw context is not linear, one can often use polynomial or kernel feature mappings to approximate non-linear relationships within a higher-dimensional linear model.  </li> <li>Hyperparameter Tuning: The exploration parameter $ \\alpha $ often requires careful tuning (or theoretically derived values) to ensure a good balance of exploration and exploitation.  </li> <li>Practical Extensions: Variants of LinUCB can incorporate regularization parameters, discount old data for nonstationary environments, and use approximate matrix updates for very large \\(d\\).</li> </ul> <p>For more details and practical insights, see: - Li et al. (2010) for the original application to personalized news recommendation. - Chapelle &amp; Li (2011) for empirical comparisons of bandit algorithms (including LinUCB). - Auer (2002) for the foundational UCB concept.</p>"},{"location":"course_notes/bandits/#regret-analysis-of-linucb","title":"Regret Analysis of LinUCB","text":"<p>Under standard assumptions (linear rewards, bounded noise), LinUCB achieves sublinear regret in the order of \\(O(d \\sqrt{T} \\ln T)\\). As \\(T\\) grows, average per-step regret goes to zero, indicating the algorithm efficiently balances exploration and exploitation in a theoretically rigorous manner.</p> <ul> <li>High-Level Idea: The quantity \\(x_t^\\top A_i^{-1} x_t\\) can be interpreted as capturing how much \u201cnew information\u201d the context \\(x_t\\) provides about arm \\(i\\). Once \\(A_i\\) becomes large and well-inverted, the algorithm is confident in its parameter estimates, reducing the exploration term.  </li> <li>Practical Interpretation: In simpler terms, each arm \\(i\\) fits a linear predictor \\(\\hat{\\theta}_i\\) by \u201ccollecting\u201d relevant \\((x_t, R_t)\\) pairs. With enough data, the algorithm zeroes in on the optimal linear function.</li> </ul> <p>For a formal derivation of these regret bounds, one can consult:</p> <ul> <li>Abbasi-Yadkori et al. (2011) \u2014 detailed proofs for linear bandits\u2019 regret bounds.  </li> <li>Bouneffouf et al. (2020) \u2014 comprehensive survey on bandits, including contextual and linear settings.</li> </ul>"},{"location":"course_notes/bandits/#thompson-sampling-in-contextual-bandits","title":"Thompson Sampling in Contextual Bandits","text":""},{"location":"course_notes/bandits/#overview","title":"Overview","text":"<p>Thompson Sampling (TS) can also be extended to contextual bandits by placing a prior over each arm\u2019s parameter vector and updating that posterior after each interaction. Similar to standard TS, it selects arms by sampling from this posterior and picking the arm whose sampled parameter suggests the highest reward given the current context.</p>"},{"location":"course_notes/bandits/#usage-of-thompson-sampling-algorithm-in-contextual-bandits","title":"Usage of Thompson Sampling Algorithm in Contextual Bandits","text":"<ol> <li>Model Specification: Assume a prior distribution over each arm\u2019s parameter \\(\\theta_i\\) (e.g., Gaussian for linear models).  </li> <li>At Each Round \\(t\\):</li> <li>Observe context \\(x_t\\) .  </li> <li>Sample \\(\\tilde{\\theta}_i\\) from the posterior for each arm \\(i\\) .  </li> <li>Compute \\(\\tilde{r}_i(t) = x_t^\\top \\tilde{\\theta}_i\\) .  </li> <li>Select \\(A_t = \\arg\\max_i \\tilde{r}_i(t)\\) .  </li> <li>Observe reward \\(R_t\\) .  </li> <li>Update the posterior of \\(\\theta_{A_t}\\) .</li> </ol>"},{"location":"course_notes/bandits/#regret-analysis-of-thompson-sampling-in-contextual-bandits","title":"Regret Analysis of Thompson Sampling in Contextual Bandits","text":"<p>With similar assumptions to LinUCB, contextual Thompson Sampling attains comparable \\(O(\\sqrt{T})\\) -type regret bounds, often with good empirical results due to its Bayesian \u201cprobability matching\u201d mechanism.</p>"},{"location":"course_notes/bandits/#conclusion","title":"Conclusion","text":"<p>The multi-armed bandit (MAB) problem encapsulates a fundamental tension inherent to sequential decision-making under uncertainty: the exploration\u2013exploitation trade-off. Despite its conceptual simplicity\u2014consisting solely of a set of actions with unknown reward distributions\u2014the MAB framework reveals rich theoretical structures and remains deeply relevant across a wide array of real-world applications.</p> <p>This chapter began with a formalization of the stochastic bandit setting, introducing key constructs such as action-value functions, sample-average estimation, and the non-associativity property, which distinguishes MABs from general Markov Decision Processes (MDPs) by eliminating the influence of state transitions. The core objective was established as the maximization of cumulative reward, equivalently viewed through the lens of regret minimization.</p> <p>To this end, various algorithmic strategies were introduced for estimating action values and managing the trade-off between exploration and exploitation:</p> <ul> <li> <p>Sample-average and incremental update rules form the foundation for value estimation in stationary environments, while constant step-size updates extend applicability to nonstationary settings through exponential weighting of recent observations.</p> </li> <li> <p>The notion of regret, both instantaneous and cumulative, provides a principled metric for evaluating the performance of bandit algorithms. Analytical decompositions reveal that total regret depends critically on the gap between suboptimal and optimal actions and the frequency with which suboptimal actions are selected.</p> </li> <li> <p>Baseline strategies such as \u03b5-greedy and optimistic initial values offer intuitive approaches to exploration, though \u03b5-greedy with a constant exploration rate incurs linear regret. Improvements can be achieved through decaying exploration schedules or more principled algorithms.</p> </li> <li> <p>Upper Confidence Bound (UCB) methods exemplify the \"optimism in the face of uncertainty\" paradigm by using high-probability confidence intervals to balance learning and exploitation. These methods offer logarithmic regret bounds, matching the Lai\u2013Robbins lower bound for stochastic settings.</p> </li> <li> <p>Thompson Sampling, rooted in Bayesian inference and probability matching, introduces a powerful and flexible framework for balancing exploration and exploitation. It often performs competitively with UCB both in theory and practice, and generalizes well across reward models.</p> </li> <li> <p>Extensions to the contextual bandit setting further elevate the practical relevance of MABs. By incorporating side information or features, algorithms such as LinUCB and contextual Thompson Sampling dynamically adapt action choices based on observed context, effectively learning context-to-action policies with provably sublinear regret.</p> </li> </ul> <p>In summary, the MAB framework offers a minimal yet powerful model that lies at the heart of many online learning and reinforcement learning scenarios. The theoretical underpinnings, from regret analysis to optimal exploration policies, provide valuable tools for designing adaptive systems. Simultaneously, the algorithmic developments discussed herein continue to form the basis of modern intelligent agents operating in uncertain, real-time environments.</p>"},{"location":"course_notes/bandits/#authors","title":"Author(s)","text":"<ul> <li> <p>Arshia Gharooni</p> <p>Teaching Assistant</p> <p>arshiyagharoony@gmail.com</p> <p> </p> </li> <li> <p>Mohammad Mohammadi</p> <p>Teaching Assistant</p> <p>mohammadm97i@gmail.com</p> <p> </p> </li> <li> <p>Hesam Hosseini</p> <p>Teaching Assistant</p> <p>hesam138122@gmail.com</p> <p> </p> </li> </ul>"},{"location":"course_notes/bandits/#references","title":"References","text":"<ol> <li> <p>The RL Hub MAB Chapters</p> </li> <li> <p>The Multi-Armed Bandit Problem and Its Solutions</p> </li> <li> <p>How to make decisions in a bandit game?</p> </li> <li> <p>Lower bounds on regret for multi-armed bandits.</p> </li> <li> <p>Continuous Intelligence with Contextual Bandits</p> </li> <li> <p>Reinforcement Learning (BartoSutton)</p> </li> </ol>"},{"location":"course_notes/exploration/","title":"Exploration Methods","text":""},{"location":"course_notes/hierarchical/","title":"Hierarchical RL","text":""},{"location":"course_notes/imitation/","title":"Imitation Learning","text":""},{"location":"course_notes/intro-to-phase2/","title":"Introduction to RL in Depth","text":""},{"location":"course_notes/intro-to-phase2/#goal","title":"goal","text":"<p>get deeper into the concepts. learn new math that are usfull for RL </p>"},{"location":"course_notes/intro-to-phase2/#overview","title":"overview","text":""},{"location":"course_notes/intro-to-phase2/#value-iteration","title":"value iteration","text":""},{"location":"course_notes/intro-to-phase2/#policy-iterration","title":"policy iterration","text":""},{"location":"course_notes/intro-to-phase2/#liptishtness","title":"liptishtness","text":""},{"location":"course_notes/intro-to-phase2/#contraction-mapping","title":"contraction mapping","text":""},{"location":"course_notes/intro-to-phase2/#ddpg","title":"DDPG","text":""},{"location":"course_notes/intro-to-phase2/#natural-gradient","title":"natural gradient","text":""},{"location":"course_notes/intro-to-phase2/#policy-gradient","title":"policy gradient","text":""},{"location":"course_notes/intro-to-phase2/#kl","title":"kl","text":""},{"location":"course_notes/intro-to-phase2/#trpo-theory","title":"TRPO theory","text":""},{"location":"course_notes/intro-to-phase2/#sac-theory","title":"SAC theory","text":""},{"location":"course_notes/intro-to-phase2/#conceteration-bound","title":"conceteration bound","text":""},{"location":"course_notes/intro-to-phase2/#hofdding","title":"hofdding","text":""},{"location":"course_notes/intro-to-phase2/#regret-bound","title":"regret bound","text":""},{"location":"course_notes/intro-to-phase2/#ucb","title":"ucb","text":""},{"location":"course_notes/intro-to-phase2/#notaion","title":"notaion","text":"<p>here is a the notation we will use thgoht this part:</p>"},{"location":"course_notes/intro-to-phase2/#prequestions","title":"prequestions","text":"<p>you may need to know the follwing concepts two better undestad this part:</p>"},{"location":"course_notes/intro-to-rl/","title":"Week 1: Introduction to RL","text":""},{"location":"course_notes/intro-to-rl/#why-do-we-need-reinforcement-learning","title":"Why Do We Need Reinforcement Learning?","text":"<p>Reinforcement Learning (RL) is a subfield of artificial intelligence (AI) that focuses on training agents to make sequences of decisions by interacting with an environment. Unlike other AI methods, RL is particularly well-suited for problems where the agent must learn optimal behavior through trial and error, often in dynamic and uncertain environments.</p>"},{"location":"course_notes/intro-to-rl/#limitations-of-other-ai-methods","title":"Limitations of Other AI Methods","text":"<ol> <li>Supervised Learning: </li> <li>What it does: Supervised learning requires a labeled dataset where the correct output (label) is provided for each input. The model learns to map inputs to outputs based on this data.</li> <li> <p>Limitation: In many real-world scenarios, obtaining a labeled dataset is impractical or impossible. For example, consider training a robot to walk. It's not feasible to provide a labeled dataset of all possible states and actions the robot might encounter.</p> </li> <li> <p>Unsupervised Learning:</p> </li> <li>What it does: Unsupervised learning deals with unlabeled data and tries to find hidden patterns or intrinsic structures within the data.</li> <li> <p>Limitation: While unsupervised learning can identify patterns, it doesn't provide a way to make decisions or take actions based on those patterns. For instance, clustering similar states in a game doesn't tell the agent how to win the game.</p> </li> <li> <p>Traditional Control Methods:</p> </li> <li>What it does: Traditional control methods are designed to maintain a system's state within a desired range using predefined rules.</li> <li>Limitation: These methods require a precise model of the environment and are not adaptable to complex, changing environments.</li> </ol>"},{"location":"course_notes/intro-to-rl/#where-reinforcement-learning-shines","title":"Where Reinforcement Learning Shines","text":"<p>Reinforcement Learning excels in scenarios where:</p> <ul> <li> <p>No Labeled Data is Available: RL agents learn by interacting with the environment and receiving feedback in the form of rewards or penalties. This eliminates the need for a pre-labeled dataset.</p> </li> <li> <p>Sequential Decision-Making is Required: RL is designed to handle problems where decisions are made in a sequence, and each decision affects the future state of the environment. For example, in a game like Go or Chess, each move affects the board's state and influences future moves.</p> </li> <li> <p>The Environment is Dynamic and Uncertain: RL agents can adapt to changing environments and learn optimal policies even when the environment is not fully known or is stochastic. For instance, an RL agent can learn to navigate a maze even if the maze's layout changes over time.</p> </li> <li> <p>Non-i.i.d. Data: RL is capable of handling non-independent and identically distributed (non-i.i.d.) data. In many real-world scenarios, data points are not independent (e.g., the state of the environment at one time step depends on previous states) and may not be identically distributed (e.g., the distribution of states changes over time). A notable example is robotic control, where an autonomous robot learns to walk. Each movement directly affects the next state, and the terrain may change dynamically, requiring the RL agent to adapt its policy based on sequential dependencies. RL agents can learn from such data by considering the temporal dependencies and adapting to the evolving data distribution.</p> </li> <li> <p>Lack of Input Data: In supervised learning, the model is provided with input-output pairs \\((x, y)\\), where \\(x\\) is the input data and \\(y\\) is the corresponding label. In RL, not only are the labels (correct actions) not provided, but the \"\\(x\\)\"s (input states) are also not explicitly given to the model. The agent must actively interact with the environment to observe and collect these states.</p> </li> <li> <p>Example: Consider training an RL agent to play a video game. In supervised learning, you would need a dataset of game states (\\(x\\)) and the corresponding optimal actions (\\(y\\)). In RL, the agent starts with no prior knowledge of the game states or actions. It must explore the game environment, observe the states, and learn which actions lead to higher rewards. This process of discovering and learning from the environment is what makes RL uniquely powerful for tasks where the input data is not readily available.</p> </li> </ul>"},{"location":"course_notes/intro-to-rl/#key-difference-between-rl-and-supervised-learning","title":"Key Difference Between RL and Supervised Learning","text":"<p>The primary distinction between RL and supervised learning is that, while feedback is provided in RL, the exact correct answer or action is not explicitly given. Let's delve deeper into this concept and explore the importance of exploration in RL, along with its challenges.</p>"},{"location":"course_notes/intro-to-rl/#supervised-learning","title":"Supervised Learning:","text":"<ul> <li>In supervised learning, you are presented with a bunch of data and told exactly what the answer for each data point is. Your model adjusts its parameters to get the prediction right for the data you trained on, and the goal is to generalize to unseen data, but is doesn't try to do better than the data.</li> <li> <p>Example: In a image classification task, the model is given images along with their correct labels (e.g., \"cat\" or \"dog\"). The model learns to predict the label for new, unseen images based on this labeled data.</p> </li> <li> <p>Imitation Learning: A specialized form of supervised learning used in decision-making problems is Imitation Learning (IL). In IL, the model learns by mimicking expert demonstrations. The training data consists of state-action pairs provided by an expert, and the model learns to replicate the expert's behavior. Unlike traditional supervised learning, IL is applied to sequential decision-making tasks, making it a bridge between supervised learning and RL.</p> </li> </ul>"},{"location":"course_notes/intro-to-rl/#reinforcement-learning","title":"Reinforcement Learning","text":"<ul> <li> <p>Agent-Environment Interaction: In RL, the agent interacts with the environment, takes actions, receives rewards, and adapts its policy based on these rewards. However, unlike supervised learning, the agent is never told which action was the right one for a given state or what the correct policy is for a given task. In other words, there are no labels! The agent must learn the optimal actions using the learning signals provided by the reward.</p> </li> <li> <p>Exploration: Exploration is a critical component of RL. Since the agent is not provided with labeled data or explicit instructions, it must explore the environment to discover which actions yield the highest rewards. This exploration allows the agent to learn from its experiences and improve its policy over time. Without exploration, the agent might get stuck in suboptimal behaviors, never discovering better strategies.</p> </li> <li> <p>Drawbacks and Difficulties of Exploration: While exploration is essential, it comes with its own set of challenges. One major difficulty is that wrong exploration can lead to poor learning outcomes. If the agent explores suboptimal or harmful actions excessively, it may reinforce bad behaviors, leading to a failure in learning the optimal policy. This is often summarized by the phrase \"garbage in, garbage out\"\u2014if the agent explores poorly and collects low-quality data, the resulting policy will also be of low quality.</p> </li> <li> <p>Example: Consider an RL agent learning to navigate a maze. If the agent spends too much time exploring dead ends or repeatedly taking wrong turns, it may fail to discover the correct path to the goal. The agent's policy will be based on these poor explorations, resulting in a suboptimal or even failing strategy.</p> </li> <li> <p>Balancing Exploration and Exploitation: A key challenge in RL is balancing exploration (trying new actions to discover their effects) and exploitation (using known actions that yield high rewards). Too much exploration can lead to inefficiency, while too much exploitation can cause the agent to miss out on better strategies. Techniques like epsilon-greedy policies and Thompson sampling are often used to strike this balance.</p> </li> <li> <p>Outperforming Human Intelligence: Despite the challenges, when exploration is done effectively, the agent can discover novel strategies and solutions that humans might not consider. Since the data is collected by the agent itself through exploration, an RL agent can even outperform human intelligence and execute impressive actions that no one has thought of before.</p> </li> <li> <p>Example: In a game of chess, the RL agent might receive a reward for winning the game but won't be told which specific move led to the victory. It must figure out the sequence of optimal moves through trial and error. By exploring different moves and learning from the outcomes, the agent can develop a strategy that maximizes its chances of winning. However, if the agent explores ineffective moves too often, it may fail to learn a winning strategy.</p> </li> </ul>"},{"location":"course_notes/intro-to-rl/#example-autonomous-driving","title":"Example: Autonomous Driving","text":"<p>Consider the task of autonomous driving. </p> <ul> <li>Supervised Learning Approach: You would need a massive labeled dataset of all possible driving scenarios, including rare events like a child running into the street. This is impractical.</li> <li> <p>Imitation Learning: In autonomous driving, IL can be used to train a model by observing human drivers. The model is provided with data on how human drivers react in various driving scenarios (e.g., steering, braking, accelerating). The model learns to mimic these actions, effectively reducing the problem to a supervised learning task. However, the model's performance is limited by the quality of the expert demonstrations and may not discover strategies that outperform the expert.</p> </li> <li> <p>Reinforcement Learning Approach: An RL agent can learn to drive by interacting with a simulated environment. It receives rewards for safe driving and penalties for accidents or traffic violations. Over time, the agent learns an optimal policy for driving without needing a labeled dataset.</p> </li> </ul>"},{"location":"course_notes/intro-to-rl/#key-concepts-in-reinforcement-learning","title":"Key Concepts in Reinforcement Learning","text":"<p>To understand RL, it's essential to grasp some fundamental concepts: state, action, reward, and policy. Let's explore each of these concepts in detail, using an example to illustrate their roles.</p>"},{"location":"course_notes/intro-to-rl/#1-state-mathbfs_t","title":"1. State (\\(\\mathbf{s}_t\\))","text":"<ul> <li>Definition: A state represents the current situation or configuration of the environment at a given time. It encapsulates all the relevant information that the agent needs to make a decision.</li> <li>Example: Consider a self-driving car navigating through a city. The state could include the car's current position, speed, the positions of other vehicles, traffic lights, and pedestrians. All these factors together define the state of the environment at any moment.</li> </ul>"},{"location":"course_notes/intro-to-rl/#2-action-mathbfa_t","title":"2. Action (\\(\\mathbf{a}_t\\))","text":"<ul> <li>Definition: An action is a decision or move made by the agent that affects the environment. The set of all possible actions that an agent can take is called the action space.</li> <li>Example: In the self-driving car scenario, possible actions might include accelerating, braking, turning left, turning right, or maintaining the current speed. Each action changes the state of the environment, such as the car's position or speed.</li> </ul>"},{"location":"course_notes/intro-to-rl/#3-reward-r_t","title":"3. Reward (\\(r_t\\))","text":"<ul> <li> <p>Definition: A reward is a feedback signal that the agent receives from the environment after taking an action. The reward indicates the immediate benefit or cost of the action taken. The goal of the agent is to maximize the cumulative reward over time. The reward can be provided by an expert or learned from demonstrations. This can be achieved by directly copying observed behavior or inferring rewards from observed behavior (Inverse RL) .</p> </li> <li> <p>Example: For the self-driving car, rewards could be assigned as follows:</p> </li> <li>Positive Reward: Reaching the destination safely (+100), obeying traffic rules (+10).</li> <li> <p>Negative Reward: Colliding with another vehicle (-100), running a red light (-50).</p> </li> <li> <p>Sparse Reward Environments</p> </li> <li> <p>Definition: In sparse reward environments, the agent receives rewards very infrequently. Instead of getting feedback after every action, the agent might only receive a reward after completing a long sequence of actions or achieving a significant milestone. For example, in a game, the agent might only receive a reward upon winning or losing, with no feedback provided during the game.</p> </li> <li> <p>Challenges:</p> <ul> <li>Difficulty in Learning: Sparse rewards make it challenging for the agent to learn which actions lead to positive outcomes. Since the agent receives little to no feedback during most of its interactions, it struggles to associate specific actions with rewards. This can lead to slow or ineffective learning.</li> <li>Exploration: In sparse reward environments, the agent must explore extensively to discover actions that yield rewards. Without frequent feedback, the agent may take a long time to stumble upon the correct sequence of actions, making the learning process inefficient.</li> <li>Credit Assignment Problem: Determining which actions in a sequence contributed to a reward is difficult in sparse reward settings. The agent may not be able to accurately attribute the reward to the correct actions, leading to suboptimal policies.</li> </ul> </li> <li> <p>Example: Consider an RL agent learning to play a complex strategy game where the only reward is given at the end of the game (e.g., +1 for winning and -1 for losing). The agent must explore countless moves and strategies without any intermediate feedback, making it challenging to learn effective strategies. The agent might take a very long time to discover the sequence of actions that leads to a win.</p> </li> </ul>"},{"location":"course_notes/intro-to-rl/#4-policy-pi_theta","title":"4. Policy (\\(\\pi_{\\theta}\\))","text":"<ul> <li>Definition: A policy is a strategy or set of rules that the agent follows to decide which action to take in a given state. It maps states to actions and can be deterministic (always choosing a specific action in a state) or stochastic (choosing actions based on a probability distribution).</li> <li>Example: In the self-driving car example, a policy might dictate that the car should slow down when it detects a pedestrian crossing the street or stop when it encounters a red traffic light. The policy is what the agent learns and optimizes to maximize cumulative rewards.</li> </ul>        Fig1. RL Framework"},{"location":"course_notes/intro-to-rl/#the-anatomy-of-reinforcement-learning","title":"The Anatomy of Reinforcement Learning","text":"<p>An RL agent's training is done through an interactive process between the agent and the environment. This process involves several key steps that allow the agent to learn and improve its decision-making capabilities. </p>"},{"location":"course_notes/intro-to-rl/#1-evaluate-the-action-how-well-was-our-choice-of-x-y-z-execute-gripping","title":"1. Evaluate the Action: \"How well was our choice of (x, y, z); execute gripping!\"","text":"<ul> <li>This step involves assessing the effectiveness of the action taken by the agent. The agent performs an action, such as gripping an object at a specific location (x, y, z), and then evaluates how well this action was executed.</li> </ul>"},{"location":"course_notes/intro-to-rl/#2-fit-a-model","title":"2. Fit a Model","text":"<ul> <li>Fitting a model refers to creating a representation of the environment or the task based on the data collected from interactions. This model helps the agent predict the outcomes of future actions and understand the dynamics of the environment.</li> </ul>"},{"location":"course_notes/intro-to-rl/#3-estimate-the-return-gripping-objects-estimate-the-return","title":"3. Estimate the Return: \"Gripping objects estimate the return\"","text":"<ul> <li>Estimating the return involves calculating the expected cumulative reward from a given state or action. The return is a key concept in RL, as the agent aims to maximize this cumulative reward over time.</li> </ul>"},{"location":"course_notes/intro-to-rl/#4-understand-physical-consequences-or-when-i-grip-some-object-on-a-specific-location-what-happens-physically-ie-how-its-x-y-zs-change","title":"4. Understand Physical Consequences: \"Or ... when I grip some object on a specific location, what happens physically; i.e. how its (x, y, z)'s change?\"","text":"<ul> <li>This step emphasizes the importance of understanding the physical consequences of actions. The agent needs to know how its actions (like gripping an object at a specific location) affect the environment, particularly the object's position (x, y, z).</li> </ul>"},{"location":"course_notes/intro-to-rl/#5-improve-the-policy-improve-the-policy-best-guess","title":"5. Improve the Policy: \"Improve the policy 'best guess'\"","text":"<ul> <li>Improving the policy involves refining the agent's strategy for choosing actions based on the feedback received from the environment. The \"best guess\" refers to the agent's current understanding of the optimal actions, which is continuously updated as the agent learns.</li> </ul>        Fig2. The Anatomy of RL"},{"location":"course_notes/intro-to-rl/#comparing-reinforcement-learning-with-other-learning-methods","title":"Comparing Reinforcement Learning with Other Learning Methods","text":"<p>To better understand the unique characteristics of Reinforcement Learning, it's helpful to compare it with other learning methods. The table below provides a comparison of RL with other approaches such as Supervised Learning (SL), Unsupervised Learning (UL), and Imitation Learning (IL). Let's break down the table and discuss the key differences and similarities.</p>"},{"location":"course_notes/intro-to-rl/#comparison-table","title":"Comparison Table","text":"AI Planning SL UL RL IL Optimization X X X Learns from experience X X X X Generalization X X X X X Delayed Consequences X X X Exploration X"},{"location":"course_notes/intro-to-rl/#key-concepts","title":"Key Concepts","text":"<ol> <li> <p>Optimization</p> <ul> <li>AI Planning: Involves optimizing a sequence of actions to achieve a goal.</li> <li>RL: Focuses on optimizing policies to maximize cumulative rewards.</li> <li>IL: Also involves optimization, often by learning from demonstrations.</li> </ul> </li> <li> <p>Learns from Experience</p> <ul> <li>SL: Learns from labeled data, where each input has a corresponding output.</li> <li>UL: Learns from unlabeled data by identifying patterns.</li> <li>RL: Learns by interacting with the environment and receiving feedback.</li> <li>IL: Learns from demonstrations of good policies.</li> </ul> </li> <li> <p>Generalization</p> <ul> <li>All methods (AI Planning, SL, UL, RL, IL) aim to generalize from training data to new, unseen situations.</li> </ul> </li> <li> <p>Delayed Consequences</p> <ul> <li>AI Planning: Considers the long-term effects of actions.</li> <li>RL: Takes into account the delayed consequences of actions to maximize future rewards.</li> <li>IL: Can also consider delayed consequences if the demonstrations include long-term strategies.</li> </ul> </li> <li> <p>Exploration</p> <ul> <li>RL: Requires exploration of the environment to discover optimal policies.</li> <li>Other methods (SL, UL, IL) typically do not involve exploration in the same way.</li> <li>In particular, IL is limited to the data and experiences provided by the expert model. Since IL relies on demonstrations, it cannot leverage the benefits of exploration to discover better strategies beyond what the expert has demonstrated. In contrast, RL has the capability to explore and learn from trial and error, allowing it to outperform expert demonstrations in some cases by discovering novel, more efficient policies.</li> </ul> </li> </ol>"},{"location":"course_notes/intro-to-rl/#planning-vs-learning","title":"Planning vs Learning","text":"<p>Two fundamental problems in sequential decision making:</p> <ol> <li> <p>Reinforcement learning:</p> <ul> <li>The environment is initially unknown</li> <li>The agent interacts with the environment</li> <li>The agent improves its policy</li> </ul> </li> <li> <p>Planning:</p> <ul> <li>A model of the environment is known</li> <li>The agent performs computations with its model (without any external interaction)</li> <li>The agent improves its policy, a.k.a. deliberation, reasoning, introspection, pondering, thought, search </li> </ul> </li> </ol>"},{"location":"course_notes/intro-to-rl/#introduction-to-markov-decision-processes-mdps","title":"Introduction to Markov Decision Processes (MDPs)","text":"<p>Markov Decision Processes (MDPs) are a fundamental framework used in Reinforcement Learning to model decision-making problems. MDPs provide a mathematical foundation for describing an environment in which an agent interacts, takes actions, and receives rewards. Let's break down the components of an MDP.</p>"},{"location":"course_notes/intro-to-rl/#components-of-an-mdp","title":"Components of an MDP","text":"<p>An MDP is defined by the following components:</p> <ol> <li> <p>Set of States (\\(S\\)): </p> <ul> <li>The set of all possible states that the environment can be in. A state \\(s \\in S\\) represents a specific configuration or situation of the environment at a given time.</li> </ul> </li> <li> <p>Set of Actions (\\(A\\)): </p> <ul> <li>The set of all possible actions that the agent can take. An action \\(a \\in A\\) is a decision made by the agent that affects the environment.</li> </ul> </li> <li> <p>Transition Function (\\(P(s' | s, a)\\)):</p> <ul> <li>The transition function defines the probability of transitioning from state \\(s\\) to state \\(s'\\) when action \\(a\\) is taken. This function captures the dynamics of the environment.</li> <li>Markov Property: The transition function satisfies the Markov property, which states that the future state \\(s'\\) depends only on the current state \\(s\\) and action \\(a\\), and not on the sequence of states and actions that preceded it. This is why the process is called \"Markovian.\"</li> </ul> </li> <li> <p>Reward Function (\\(R(s, a, s')\\)):</p> <ul> <li>The reward function specifies the immediate reward received by the agent after transitioning from state \\(s\\) to state \\(s'\\) by taking action \\(a\\). The reward is a scalar value that indicates the benefit or cost of the action.</li> </ul> </li> <li> <p>Start State (\\(s_0\\)):</p> <ul> <li>The initial state from which the agent starts its interaction with the environment.</li> </ul> </li> <li> <p>Discount Factor (\\(\\gamma\\)):</p> <ul> <li>The discount factor \\(\\gamma\\) (where \\(0 \\leq \\gamma \\leq 1\\)) determines the present value of future rewards. A discount factor close to 1 makes the agent prioritize long-term rewards, while a discount factor close to 0 makes the agent focus on immediate rewards.</li> </ul> </li> <li> <p>Horizon (\\(H\\)):</p> <ul> <li>The horizon \\(H\\) represents the time horizon over which the agent interacts with the environment. It can be finite (fixed number of steps) or infinite (continuous interaction).</li> </ul> </li> </ol>"},{"location":"course_notes/intro-to-rl/#episodes-and-environment-types","title":"Episodes and Environment Types","text":"<ul> <li>An episode is a sequence of interactions that starts from an initial state and ends when a terminal condition is met.  </li> <li>Episodic Environments: The interaction consists of episodes, meaning the agent's experience is divided into separate episodes with a clear start and end (e.g., playing a game with levels or a robot completing a task like picking up an object).  </li> <li>Non-Episodic (Continuous) Environments: There is no clear termination, and the agent continuously interacts with the environment without resetting (e.g., stock market trading, autonomous vehicle navigation).  </li> </ul>"},{"location":"course_notes/intro-to-rl/#difference-between-horizon-and-episode","title":"Difference Between Horizon and Episode","text":"<ul> <li>The episode refers to a full sequence of interactions that has a clear beginning and an end.  </li> <li>The horizon (\\(H\\)) defines the length of time the agent considers while making decisions, which can be within a single episode (in episodic environments) or over an ongoing interaction (in non-episodic environments).  </li> <li>In finite-horizon episodic tasks, the episode length is usually equal to the horizon. However, in infinite-horizon tasks, the agent keeps interacting with the environment indefinitely.</li> </ul>"},{"location":"course_notes/intro-to-rl/#the-goal-of-reinforcement-learning","title":"The Goal of Reinforcement Learning","text":"<p>The goal of RL in the context of an MDP is to find a policy \\(\\pi\\) that maximizes the expected cumulative reward over time. The policy \\(\\pi\\) is a function that maps states to actions, and it can be deterministic or stochastic.</p>"},{"location":"course_notes/intro-to-rl/#objective","title":"Objective:","text":"\\[ \\max_{\\pi} \\mathbb{E}\\left[\\sum_{t=0}^{H} \\gamma^{t} R(S_t, A_t, S_{t+1}) \\mid \\pi\\right] \\] <ul> <li> <p>Expected Value: The use of the expected value \\(\\mathbb{E}\\) is necessary because the transition function \\(P(s' | s, a)\\) is probabilistic. The agent does not know exactly which state \\(s'\\) it will end up in after taking action \\(a\\) in state \\(s\\). Therefore, the agent must consider the expected reward over all possible future states, weighted by their probabilities.</p> </li> <li> <p>Cumulative Reward: The agent aims to maximize the sum of discounted rewards over time. The discount factor \\(\\gamma\\) ensures that the agent balances immediate rewards with future rewards. A higher \\(\\gamma\\) makes the agent more farsighted, while a lower \\(\\gamma\\) makes it more shortsighted.</p> </li> </ul>"},{"location":"course_notes/intro-to-rl/#example-grid-world","title":"Example: Grid World","text":"<p>Consider a simple grid world where the agent must navigate from a start state to a goal state while avoiding obstacles. The states \\(S\\) are the grid cells, the actions \\(A\\) are movements (up, down, left, right), and the reward function \\(R(s, a, s')\\) provides positive rewards for reaching the goal and negative rewards for hitting obstacles. The transition function \\(P(s' | s, a)\\) defines the probability of actually moving to a neighboring cell when an action is taken.</p>       Fig3. Grid World Game"},{"location":"course_notes/intro-to-rl/#an-example-of-gridworld","title":"An Example of Gridworld","text":"<p>In the provided Gridworld example, the agent starts from the yellow square and has to navigate to a goal while avoiding the cliff. The rewards are: - +1 for reaching the close exit - +10 for reaching the distant exit - -10 penalty for stepping into the cliff (red squares)</p>       Fig4. Grid World Example  <p>The agent's choice depends on: - The discount factor (\\(\\gamma\\)), which determines whether it prioritizes short-term or long-term rewards. - The noise level, which introduces randomness into actions.</p> <p>Depending on the values of \\(\\gamma\\) and noise, the agent's behavior varies: 1. \\(\\gamma\\) = 0.1, noise = 0.5:    - The agent prefers the close exit (+1) but takes the risk of stepping into the cliff (-10). 2. \\(\\gamma\\) = 0.99, noise = 0:    - The agent prefers the distant exit (+10) while avoiding the cliff (-10). 3. \\(\\gamma\\) = 0.99, noise = 0.5:    - The agent still prefers the distant exit (+10), but due to noise, it risks the cliff (-10). 4. \\(\\gamma\\) = 0.1, noise = 0:    - The agent chooses the close exit (+1) while avoiding the cliff. </p>"},{"location":"course_notes/intro-to-rl/#stochastic-policy","title":"Stochastic Policy","text":"<p>Another source of randomness in MDPs comes from stochastic policies. Unlike the transition function \\(P(s' | s, a)\\), which describes the environment\u2019s inherent randomness in executing actions, a stochastic policy \\(\\pi(a | s)\\) defines the probability of selecting an action \\(a\\) when in state \\(s\\). This means that even if the environment were fully deterministic, the agent itself may act probabilistically.</p>"},{"location":"course_notes/intro-to-rl/#example-gridworld-with-a-stochastic-policy","title":"Example: Gridworld with a Stochastic Policy","text":"<p>Consider a modified version of the previous Gridworld example. Instead of always choosing the action with the highest expected return, the agent follows a stochastic policy where it selects each possible action with a certain probability:</p> <ul> <li>With 99% probability, the agent follows its optimal policy.</li> <li>With 1% probability, it selects a random action.</li> </ul> <p>Transition Probability \\(P(s'|s, a)\\)  is the probability that taking action \\(a\\) in state \\(s\\) results in transitioning to state \\(s'\\). This is determined by the environment. If the environment is slippery, moving \"right\" from \\((2,2)\\) may lead to \\((2,3)\\) with 80% probability, but also to \\((3,2)\\) with 20%. Stochastic Policy \\(\\pi(a | s)\\) determines the probability that the agent chooses action \\(a\\) when in state \\(s\\). This is determined by the agent's strategy.</p> <ul> <li>If the policy \\(\\pi(a | s)\\) is deterministic, the agent always selects the same action in a given state.</li> <li>If the policy \\(\\pi(a | s)\\) is stochastic, the agent introduces randomness in its decision-making process, which can be beneficial in exploration and avoiding local optima.</li> </ul>"},{"location":"course_notes/intro-to-rl/#graphical-model-of-mdps","title":"Graphical Model of MDPs","text":"<p>MDPs can be represented graphically as a sequence of states (\\(\\mathbf{s}\\)), actions (\\(\\mathbf{a}\\)), and transitions (\\(p\\)):</p> <ul> <li>The agent starts at state \\(\\mathbf{s}_1\\).</li> <li>It selects an action \\(\\mathbf{a}_1\\), which moves it to \\(\\mathbf{s}_2\\) based on the probability \\(p(\\mathbf{s}_2 | \\mathbf{s}_1, \\mathbf{a}_1)\\).</li> <li>The process continues, forming a decision-making chain where each action influences future states and rewards.</li> </ul>       Fig5. Graphical Model of MDPs"},{"location":"course_notes/intro-to-rl/#partially-observable-mdps-pomdps","title":"Partially Observable MDPs (POMDPs)","text":"<p>In real-world scenarios, the agent may not have full visibility of the environment, leading to Partially Observable MDPs (POMDPs). - Hidden states: The true state \\(\\mathbf{s}_t\\) is not fully known to the agent. - Observations (\\(O\\)): Instead of directly knowing \\(\\mathbf{s}_t\\), the agent receives a noisy or incomplete observation \\(\\mathbf{o}_t\\). - Decision-making challenge: The agent must infer the state from past observations and actions.</p>       Fig6. Partially Observable MDPs (POMDPs)"},{"location":"course_notes/intro-to-rl/#policy-as-a-function-of-state-mathbfs_t-or-observation-mathbfo_t","title":"Policy as a Function of State (\\(\\mathbf{s}_t\\)) or Observation (\\(\\mathbf{o}_t\\))","text":"<ul> <li>Fully Observed Policy: When the agent has access to the full state \\(\\mathbf{s}_t\\), the policy is denoted as \\(\\pi_{\\theta}(\\mathbf{a}_t | \\mathbf{s}_t)\\). This means the action \\(\\mathbf{a}_t\\) is chosen based on the current state \\(\\mathbf{s}_t\\).</li> <li> <p>Partially Observed Policy: When the agent only has access to observations \\(\\mathbf{o}_t\\), the policy is denoted as \\(\\pi_{\\theta}(\\mathbf{a}_t | \\mathbf{o}_t)\\). This means the action \\(\\mathbf{a}_t\\) is chosen based on the current observation \\(\\mathbf{o}_t\\).</p> <ul> <li> <p>Observation: At each time step, the agent receives an observation \\(\\mathbf{o}_t\\) that provides partial information about the current state \\(\\mathbf{s}_t\\). For example, \\(\\mathbf{o}_1\\) is the observation corresponding to state \\(\\mathbf{s}_1\\).</p> </li> <li> <p>Policy: The policy \\(\\pi_{\\theta}\\) maps observations to actions. For instance, \\(\\pi_{\\theta}(\\mathbf{a}_1 | \\mathbf{o}_1)\\) determines the action \\(\\mathbf{a}_1\\) based on the observation \\(\\mathbf{o}_1\\).</p> </li> </ul> </li> </ul>       Fig7. POMDP Policy"},{"location":"course_notes/intro-to-rl/#utility-function-in-reinforcement-learning","title":"Utility Function in Reinforcement Learning","text":"<p>In Reinforcement Learning, the utility function plays a central role in evaluating the long-term desirability of states or state-action pairs. The utility function quantifies the expected cumulative reward that an agent can achieve from a given state or state-action pair, following a specific policy. Let's explore the concept of the utility function and its mathematical formulation.</p>"},{"location":"course_notes/intro-to-rl/#definition-of-utility-function","title":"Definition of Utility Function","text":"<p>The utility function measures the expected cumulative reward that an agent can accumulate over time, starting from a particular state or state-action pair, and following a given policy. There are two main types of utility functions:</p> <ol> <li> <p>State Value Function (\\(V^{\\pi}(s)\\)):</p> <ul> <li>The state value function \\(V^{\\pi}(s)\\) represents the expected cumulative reward when starting from state \\(s\\) and following policy \\(\\pi\\) thereafter.</li> <li>Mathematically, it is defined as:     $\\(V^{\\pi}(s) = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t R(S_t, A_t, S_{t+1}) \\mid S_0 = s, \\pi \\right]\\)$</li> <li>Here, \\(\\gamma\\) is the discount factor, and \\(R(S_t, A_t, S_{t+1})\\) is the reward received at time \\(t\\).</li> </ul> </li> <li> <p>Action-Value Function (\\(Q^{\\pi}(s, a)\\)):</p> <ul> <li>The action-value function \\(Q^{\\pi}(s, a)\\) represents the expected cumulative reward when starting from state \\(s\\), taking action \\(a\\), and following policy \\(\\pi\\) thereafter.</li> <li>Mathematically, it is defined as:     $\\(Q^{\\pi}(s, a) = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t R(S_t, A_t, S_{t+1}) \\mid S_0 = s, A_0 = a, \\pi \\right]\\)$</li> </ul> </li> </ol>"},{"location":"course_notes/intro-to-rl/#why-is-the-utility-function-important","title":"Why is the Utility Function Important?","text":"<p>The utility function is crucial for several reasons:</p> <ul> <li>Policy Evaluation: It allows the agent to evaluate how good a particular policy \\(\\pi\\) is by estimating the expected cumulative reward for each state or state-action pair.</li> <li>Policy Improvement: By comparing the utility of different actions, the agent can improve its policy by choosing actions that lead to higher cumulative rewards.</li> <li>Optimal Policy: The ultimate goal of RL is to find the optimal policy \\(\\pi^*\\) that maximizes the utility function for all states or state-action pairs.</li> </ul>"},{"location":"course_notes/intro-to-rl/#bellman-equation-for-utility-functions","title":"Bellman Equation for Utility Functions","text":"<p>The utility functions satisfy the Bellman equation, which provides a recursive relationship between the value of a state (or state-action pair) and the values of its successor states. The Bellman equation is fundamental for solving MDPs and is used in many RL algorithms.</p> <ol> <li>Bellman Equation for State Value Function:</li> </ol> \\[V^{\\pi}(s) = \\sum_{a} \\pi(a | s) \\sum_{s'} P(s' | s, a) \\left[ R(s, a, s') + \\gamma V^{\\pi}(s') \\right]\\] <ul> <li> <p>This equation states that the value of a state \\(s\\) under policy \\(\\pi\\) is the expected immediate reward plus the discounted value of the next state \\(s'\\).</p> </li> <li> <p>Bellman Equation for Action-Value Function:</p> </li> </ul> \\[Q^{\\pi}(s, a) = \\sum_{s'} P(s' | s, a) \\left[ R(s, a, s') + \\gamma \\sum_{a'} \\pi(a' | s') Q^{\\pi}(s', a') \\right]\\] <ul> <li>This equation states that the value of taking action \\(a\\) in state \\(s\\) under policy \\(\\pi\\) is the expected immediate reward plus the discounted value of the next state \\(s'\\) and action \\(a'\\).</li> </ul>"},{"location":"course_notes/intro-to-rl/#example-grid-world_1","title":"Example: Grid World","text":"<p>Consider the Grid World example where the agent navigates to a goal while avoiding obstacles. The utility function helps the agent evaluate the long-term desirability of each cell (state) in the grid:</p> <ul> <li>State Value Function (\\(V^{\\pi}(s)\\)): The agent calculates the expected cumulative reward for each cell, considering the rewards for reaching the goal and penalties for hitting obstacles.</li> <li>Action-Value Function (\\(Q^{\\pi}(s, a)\\)): The agent evaluates the expected cumulative reward for each possible action (up, down, left, right) in each cell, helping it decide the best action to take.</li> </ul>       Fig8. An example of estimated $V^{\\pi}(s)$ values in grid world         Fig9. An example of estimated $Q^{\\pi}(s, a)$ values in grid world"},{"location":"course_notes/intro-to-rl/#authors","title":"Author(s)","text":"<ul> <li> <p>Masoud Tahmasbi</p> <p>Teaching Assistant</p> <p>masoudtahmasbifard@gmail.com</p> <p> </p> </li> </ul>"},{"location":"course_notes/inverse/","title":"Inverse RL","text":""},{"location":"course_notes/meta/","title":"Meta-RL","text":""},{"location":"course_notes/model-based/","title":"Week 5: Model-Based Methods","text":"<p>Note</p> <p>This document merges Lectures 9 and 10 from Prof. Mohammad Hossein Rohban's DRL course, Lecture 9: Model-Based RL slides from Prof. Sergey Levine\u2019s CS 294-112 (Deep RL) with a more rigorous, survey-based structure drawing on Moerland et al. (2022). We provide intuitions, mathematical details, and references to relevant works.</p>"},{"location":"course_notes/model-based/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Week 5: Model-Based Methods</li> <li>Table of Contents</li> <li>1. Introduction \\&amp; Scope</li> <li>2. Markov Decision Processes</li> <li>3. Categories of Model-Based RL<ul> <li>3.1 Planning</li> <li>3.2 Model-Free RL</li> <li>3.3 Model-Based RL (Model + Global Policy/Value)</li> <li>3.3.1 Model-Based RL with a Known Model</li> <li>3.3.2 Model-Based RL with a Learned Model</li> <li>3.4 Planning Over a Learned Model Without a Global Policy</li> </ul> </li> <li>4. Basic Schemes<ul> <li>Version 0.5: Single-Shot Model + Planning</li> <li>Core Steps</li> <li>Shortcomings</li> <li>Why Move On?</li> <li>Version 1.0: Iterative Re-Fitting + Planning</li> <li>Core Steps</li> <li>Shortcomings</li> <li>Why Move On?</li> <li>Version 1.5: Model Predictive Control (MPC)</li> <li>Core Steps</li> <li>Shortcomings</li> <li>Why Move On?</li> <li>Version 2.0: Backprop Through the Learned Model</li> <li>Core Steps</li> <li>Shortcomings</li> <li>Why Move On?</li> <li>Reward Overshooting (Overoptimistic Planning)</li> <li>What It Is</li> <li>Why It Happens</li> <li>Consequences</li> <li>Mitigation Strategies</li> <li>Other Challenges and Notes</li> </ul> </li> <li>5. Dynamics Model Learning<ul> <li>5.1 Basic Considerations</li> <li>5.2 Stochasticity</li> <li>5.2.1 Multi-Modal Transitions and the Conditional Mean Problem</li> <li>5.2.2 Descriptive (Distribution) Models</li> <li>5.2.3 Generative Approaches</li> <li>5.2.4 Training Objectives</li> <li>5.2.5 Practical Considerations and Challenges</li> <li>5.2.6 Example: Gaussian Transitions via Maximum Likelihood</li> <li>5.2.7 Concluding Remarks on Stochastic Transitions</li> <li>5.3 Uncertainty</li> <li>Bayesian Neural Networks</li> <li>Ensembles and Bootstrapping</li> <li>5.4 Partial Observability</li> <li>5.5 Non-Stationarity</li> <li>5.6 Multi-Step Prediction</li> <li>5.7 State Abstraction</li> <li>5.7.1 Common Approaches to Representation Learning</li> <li>5.7.2 Planning in Latent Space</li> <li>5.8 Temporal Abstraction</li> <li>5.8.1 Options Framework</li> <li>5.8.2 Goal-Conditioned Policies</li> <li>5.8.3 Subgoal Discovery</li> <li>5.8.4 Benefits of Temporal Abstraction</li> </ul> </li> <li>6. Integration of Planning and Learning<ul> <li>6.1 Which State to Start Planning From?</li> <li>6.2 Planning Budget vs. Real Data Collection</li> <li>6.3 How to Plan? (Planning Algorithms)</li> <li>6.3.1 Monte Carlo Tree Search (MCTS)</li> <li>6.4 Integration in the Learning and Acting Loop</li> <li>6.5 Dyna and Dyna-Style Methods</li> </ul> </li> <li>7. Modern Model-Based RL Algorithms<ul> <li>7.1 World Models (Ha \\&amp; Schmidhuber, 2018)</li> <li>7.2 PETS (Chua et al., 2018)</li> <li>7.3 MBPO (Janner et al., 2019)</li> <li>7.4 Dreamer (Hafner et al., 2020\u20132023)</li> <li>7.5 MuZero (DeepMind, 2020)</li> </ul> </li> <li>8. Key Benefits (and Drawbacks) of MBRL<ul> <li>8.1 Data Efficiency</li> <li>8.2 Exploration</li> <li>8.3 Optimality</li> <li>8.4 Transfer</li> <li>8.5 Safety</li> <li>8.6 Explainability</li> <li>8.7 Disbenefits</li> </ul> </li> <li>9. Conclusion</li> <li>10. References</li> </ul>"},{"location":"course_notes/model-based/#1-introduction-scope","title":"1. Introduction &amp; Scope","text":"<p>Model-Based Reinforcement Learning (MBRL) combines planning (using a model of environment dynamics) and learning (to approximate value functions or policies globally). MBRL benefits from being able to reason about environment dynamics \u201cin imagination,\u201d thus often achieving higher sample efficiency than purely model-free RL. However, ensuring accurate models and mitigating compounding errors pose key challenges.</p> <p>We address:</p> <ol> <li>The MDP framework and definitions.  </li> <li>Model learning: from basic supervised regression to advanced methods handling stochasticity, uncertainty, partial observability, etc.  </li> <li>Integrating planning: how to incorporate planning loops, short vs. long horizons, and real-world data interplay.  </li> <li>Modern MBRL algorithms (World Models, PETS, MBPO, Dreamer, MuZero).  </li> <li>Benefits and drawbacks of MBRL.</li> </ol>"},{"location":"course_notes/model-based/#2-markov-decision-processes","title":"2. Markov Decision Processes","text":"<p>We adopt the standard Markov Decision Process (MDP) formulation [Puterman, 2014]:</p> \\[ \\mathcal{M} = \\bigl(\\mathcal{S}, \\mathcal{A}, P, R, p(s_0), \\gamma\\bigr), \\] <p>where: - \\(\\mathcal{S}\\) is the (possibly high-dimensional) state space. - \\(\\mathcal{A}\\) is the action space (can be discrete or continuous). - \\(P(s_{t+1}\\mid s_t,a_t)\\) is the transition distribution. - \\(R(s_t,a_t,s_{t+1})\\) is the reward function. - \\(p(s_0)\\) is the initial-state distribution. - \\(\\gamma \\in [0,1]\\) is the discount factor.</p> <p>A policy \\(\\pi(a \\mid s)\\) dictates which action to choose at each state. The value function and action-value function are:</p> \\[ V^\\pi(s) \\;=\\; \\mathbb{E}\\Bigl[\\sum_{k=0}^\\infty \\gamma^k r_{t+k}\\;\\big|\\;s_t = s,\\;\\pi\\Bigr], \\] \\[ Q^\\pi(s,a) \\;=\\; \\mathbb{E}\\Bigl[\\sum_{k=0}^\\infty \\gamma^k r_{t+k}\\;\\big|\\;s_t = s,\\,a_t = a,\\;\\pi\\Bigr]. \\] <p>We want to find \\(\\pi^\\star\\) that maximizes expected return. Model-Based RL obtains a model of the environment\u2019s dynamics \\( \\hat{P}, \\hat{R}\\), then uses planning with that model (e.g., rollouts, search) to aid in learning or acting.</p>"},{"location":"course_notes/model-based/#3-categories-of-model-based-rl","title":"3. Categories of Model-Based RL","text":"<p>Following Moerland et al., we distinguish:</p> <ol> <li>Planning (known model, local solutions).  </li> <li>Model-Free RL (no explicit model, but learns a global policy or value).  </li> <li>Model-Based RL (learned or known model and a global policy/value solution).</li> </ol> <p>Model-Based RL itself splits into two key variants:</p> <ul> <li>Model-based RL with a known model: E.g., AlphaZero uses perfect board-game rules.  </li> <li>Model-based RL with a learned model: E.g., Dyna, MBPO, Dreamer, where the agent must learn \\(\\hat{P}(s_{t+1}\\mid s_t,a_t)\\).</li> </ul> <p>In addition, one could do planning over a learned model but never store a global policy or value (just do local search each time)\u2014that\u2019s still \u201cplanning + learning,\u201d but not strictly \u201cmodel-based RL\u201d if no global policy is learned in the end .</p>"},{"location":"course_notes/model-based/#31-planning","title":"3.1 Planning","text":"<p>Planning methods assume access to a perfect model of the environment\u2019s dynamics \\(\\mathcal{P}(s_{t+1} \\mid s_t, a_t)\\) and reward function \\(r(s_t,a_t)\\). In other words, the transition probabilities and/or the state transitions are fully known. Given this perfect model, the agent can perform a search procedure (e.g., lookahead search, tree search) to find the best action from the current state.</p> <ul> <li>Local Search: Typically, planning algorithms only compute a solution locally, from the agent\u2019s current state or a small set of states. They do not necessarily store or learn a global policy (i.e., a mapping from any possible state to an action).</li> <li>Classical Example: In board games (like chess or Go), an algorithm such as minimax with alpha\u2013beta pruning uses the known, perfect rules of the game to explore future states and pick an optimal move from the current position.</li> </ul> <p>Because these approaches do not usually store or learn a parametric global policy or value function, they fall under \u201cPlanning\u201d rather than \u201cModel-Based RL.\u201d</p>"},{"location":"course_notes/model-based/#32-model-free-rl","title":"3.2 Model-Free RL","text":"<p>Model-Free RL methods do not explicitly use or learn the environment\u2019s transition model. Instead, they optimize a policy \\(\\pi_\\theta(a_t \\mid s_t)\\) or a value function \\(V_\\theta(s_t)\\) (or both) solely based on interactions with the environment.</p> <ul> <li>No Transition Model: The policy or value function is learned directly from sampled trajectories \\((s_t, a_t, r_t, s_{t+1}, \\dots)\\). There is no component that learns \\(\\hat{P}(s_{t+1} \\mid s_t,a_t)\\).</li> <li>Global Solutions: Model-free methods generally learn global solutions: policies or value functions valid across all states encountered during training.</li> <li>Examples: Deep Q-Networks (DQN), Policy Gradient methods (REINFORCE, PPO), and actor\u2013critic approaches.</li> </ul> <p>Despite being effective, model-free methods may require large amounts of environment interaction, since they cannot leverage planning over a learned or known model.</p>"},{"location":"course_notes/model-based/#33-model-based-rl-model-global-policyvalue","title":"3.3 Model-Based RL (Model + Global Policy/Value)","text":"<p>In Model-Based RL, the agent has (or learns) a model of the environment and uses it to learn a global policy or value function. The policy or value function can then be used to make decisions for all states, not just the current one. This class of methods can further be subdivided into two main variants:</p>"},{"location":"course_notes/model-based/#331-model-based-rl-with-a-known-model","title":"3.3.1 Model-Based RL with a Known Model","text":"<p>In some tasks, the transition model \\(\\mathcal{P}(s_{t+1} \\mid s_t, a_t)\\) is known in advance (e.g., it is given by the rules of the environment). The agent can then use this perfect model to plan and to learn a global policy or value function. </p> <ul> <li>AlphaZero: A canonical example in board games (chess, Go, shogi). The rules of the game form a perfect simulator. AlphaZero does extensive lookahead (tree search), but it also uses that data to update a global policy and value network. Thus, it integrates planning and policy learning.</li> <li>Advantages: Since the model is perfect, there is no model-learning error. The primary challenge is how to efficiently search with that model and how to integrate the search results into a global solution.</li> </ul>"},{"location":"course_notes/model-based/#332-model-based-rl-with-a-learned-model","title":"3.3.2 Model-Based RL with a Learned Model","text":"<p>In many real-world tasks, the transition model is not known in advance. The agent must learn an approximate model \\(\\hat{P}(s_{t+1} \\mid s_t,a_t)\\) from environment interactions.</p> <ul> <li>Learning the Model: The agent collects transitions \\((s_t, a_t, r_t, s_{t+1})\\) and trains a parametric model to predict the next state(s) and reward given the current state\u2013action pair.</li> <li>Planning with the Learned Model: The agent can then plan ahead (e.g., via simulated rollouts or lookahead) in this learned model. Although approximate, it allows the agent to generate additional training data or refine its strategy without costly real-world interactions.</li> <li>Examples:</li> <li>Dyna (Sutton, 1990): Interleaves real experience with \u201cimagined\u201d experience from the learned model to update the value function or policy.</li> <li>MBPO (Model-Based Policy Optimization): Uses a learned model to generate short rollouts for policy optimization.</li> <li>Dreamer: Trains a world model and then uses latent imagination (rollouts in latent space) to learn a global policy.</li> </ul>"},{"location":"course_notes/model-based/#34-planning-over-a-learned-model-without-a-global-policy","title":"3.4 Planning Over a Learned Model Without a Global Policy","text":"<p>An additional possibility is to only do planning with a learned model\u2014without ever storing or committing to a parametric global policy or value function. In this scenario, the agent:</p> <ul> <li>Learns or refines a model of the environment.</li> <li>Uses local search (e.g., tree search or some other planning method) each time to select actions.</li> <li>Does not maintain a single policy or value function that applies across all states.</li> </ul> <p>Although this constitutes \u201cplanning + learning\u201d (the learning is in the model, and the planning is local search), it does not fully qualify as \u201cModel-Based RL\u201d in the strict sense\u2014because there is no global policy or value function being learned. Instead, the agent repeatedly plans from scratch (or near-scratch) in the learned model.</p>"},{"location":"course_notes/model-based/#4-basic-schemes","title":"4. Basic Schemes","text":"<p>References present high-level approaches, sometimes referred to as:</p> <ul> <li>Version 0.5: Collect random samples once, fit a model, do single-shot planning. Risks severe distribution mismatch.  </li> <li>Version 1.0: Iterative approach (collect data, re-fit model, plan). Improves coverage, but naive open-loop plans can fail.  </li> <li>Version 1.5: Model Predictive Control (MPC): replan at each step or on short horizons \u2013 more robust but computationally heavier.  </li> <li>Version 2.0: Backprop through the learned model directly into the policy \u2013 can be efficient at runtime, but numerically unstable for complex or stochastic tasks.</li> </ul>"},{"location":"course_notes/model-based/#version-05-single-shot-model-planning","title":"Version 0.5: Single-Shot Model + Planning","text":""},{"location":"course_notes/model-based/#core-steps","title":"Core Steps","text":"<ol> <li> <p>One-Time Data Collection </p> <ul> <li>Collect a static dataset of (state, action, next-state, reward) tuples, typically via random or fixed exploration.</li> <li>No further data is gathered afterward.</li> </ul> </li> <li> <p>One-Time Model Learning </p> <ul> <li>Fit a dynamics model \\( p_\\theta(s' \\mid s, a) \\) using the static dataset.</li> <li>The model may be inaccurate in regions not well-represented in the dataset.</li> </ul> </li> <li> <p>One-Time Planning </p> <ul> <li>Use the learned model to plan or optimize a policy (e.g., via trajectory optimization or tree search).</li> <li>The plan is executed in the real environment without re-planning.</li> </ul> </li> </ol>"},{"location":"course_notes/model-based/#shortcomings","title":"Shortcomings","text":"<ul> <li>Distribution Mismatch: The policy can enter states the model has never \u201cseen,\u201d leading to large prediction errors (extrapolation).  </li> <li>Compounding Errors: Small modeling inaccuracies early on can push the system into unmodeled states, magnifying the errors.  </li> <li>No Iterative Refinement: With no new data collection, there\u2019s no way to correct model inaccuracies discovered during execution.</li> </ul>"},{"location":"course_notes/model-based/#why-move-on","title":"Why Move On?","text":"<ul> <li>Severe Mismatch &amp; Error Growth: Because you never adapt to real outcomes beyond the initial dataset, errors can escalate and result in catastrophic failures.  </li> <li>Limited Practicality: Version 0.5 can work in highly controlled or small problems, but most real tasks require iterative data gathering and model improvements.</li> </ul>"},{"location":"course_notes/model-based/#version-10-iterative-re-fitting-planning","title":"Version 1.0: Iterative Re-Fitting + Planning","text":""},{"location":"course_notes/model-based/#core-steps_1","title":"Core Steps","text":"<ol> <li> <p>Iterative Data Collection</p> <ul> <li>Use a current policy (or plan) to interact with the environment.</li> <li>Gather new transitions (state, action, next-state, reward) and add them to the dataset.</li> </ul> </li> <li> <p>Re-Fit the Model</p> <ul> <li>Update the dynamics model \\( p_\\theta(s' \\mid s, a) \\) using the expanded dataset.</li> <li>The model gradually learns the dynamics in regions the policy visits.</li> </ul> </li> <li> <p>Re-Plan or Update the Policy</p> <ul> <li>After each model update, re-run planning or policy optimization to refine the policy.</li> <li>Deploy the updated policy in the real environment, collect more data, and repeat.</li> </ul> </li> </ol>"},{"location":"course_notes/model-based/#shortcomings_1","title":"Shortcomings","text":"<ul> <li>Open-Loop Execution: Even though the model is updated iteratively, each plan can be executed \u201copen loop,\u201d so stochastic events or modest model errors can derail a plan until the next re-planning cycle.  </li> <li>Long-Horizon Vulnerability: If the planning horizon is substantial, inaccuracies can still compound within a single rollout.</li> </ul>"},{"location":"course_notes/model-based/#why-move-on_1","title":"Why Move On?","text":"<ul> <li>Stochastic / Complex Tasks: Open-loop plans are brittle. You need a way to correct for real-time deviations instead of waiting until the next iteration.  </li> <li>Distribution Still Grows: While iterating helps gather more relevant data, you may still face large drifts in states if the environment is noisy or high-dimensional.</li> </ul>"},{"location":"course_notes/model-based/#version-15-model-predictive-control-mpc","title":"Version 1.5: Model Predictive Control (MPC)","text":""},{"location":"course_notes/model-based/#core-steps_2","title":"Core Steps","text":"<ol> <li>Short-Horizon Re-Planning at Each Step</li> <li> <p>At each time \\(t\\):</p> <ol> <li>Observe the real current state \\( s_t \\).</li> <li>Plan a short sequence of actions \\(\\{a_t, \\dots, a_{t+H-1}\\}\\) with the learned model.</li> <li>Execute only the first action \\( a_t \\).</li> <li>Observe the next real state \\( s_{t+1} \\).</li> <li>Re-plan from \\( s_{t+1} \\).</li> </ol> </li> <li> <p>Closed-Loop Control</p> </li> <li> <p>Frequent re-planning reduces the impact of model errors because the system constantly \u201cchecks in\u201d with reality.</p> </li> <li> <p>Iterative Model Updates (Optional)</p> </li> <li>As in Version 1.0, you can continue to collect data and update the model periodically.</li> </ol>"},{"location":"course_notes/model-based/#shortcomings_2","title":"Shortcomings","text":"<ul> <li>High Computational Cost: Planning at every time step can be expensive, especially in high-dimensional or time-critical domains.  </li> <li>Still Requires a Good Model: Significant model inaccuracies can still cause erroneous plans, though re-planning mitigates compounding errors.</li> </ul>"},{"location":"course_notes/model-based/#why-move-on_2","title":"Why Move On?","text":"<ul> <li>Continuous Planning Overhead: MPC may be infeasible when real-time constraints or massive action spaces make on-the-fly planning too slow.  </li> <li>Desire for a Learned Policy: A direct policy could yield near-instant action selection at runtime, motivating Version 2.0.</li> </ul>"},{"location":"course_notes/model-based/#version-20-backprop-through-the-learned-model","title":"Version 2.0: Backprop Through the Learned Model","text":""},{"location":"course_notes/model-based/#core-steps_3","title":"Core Steps","text":"<ol> <li>Differentiable Dynamics Model</li> <li> <p>Train a neural network or another differentiable function \\( p_\\theta(s' \\mid s, a) \\).</p> </li> <li> <p>End-to-End Policy Optimization</p> </li> <li>Unroll the model over multiple timesteps, applying the policy \\(\\pi_\\phi\\) to get actions, and accumulate predicted rewards.</li> <li>Backpropagate through the learned model to update policy parameters \\(\\phi\\).</li> <li> <p>Once trained, the resulting policy can be deployed directly\u2014no online planning is needed.</p> </li> <li> <p>High Efficiency at Deployment</p> </li> <li>Action selection is a simple forward pass of the policy network, suitable for time-critical or large-scale applications.</li> </ol>"},{"location":"course_notes/model-based/#shortcomings_3","title":"Shortcomings","text":"<ul> <li>Numerical Instability: Backpropagating through many timesteps can lead to exploding or vanishing gradients.  </li> <li>Model Exploitation: The policy can \u201cexploit\u201d any inaccuracies in the model, especially over long horizons or in stochastic environments.  </li> <li>Careful Regularization: Shorter horizon unrolls, ensembles, or uncertainty estimation are often used to keep policy learning stable and robust.</li> </ul>"},{"location":"course_notes/model-based/#why-move-on_3","title":"Why Move On?","text":"<p>Well, Version 2.0 is often seen as an \u201cend goal\u201d rather than a stepping stone\u2014because once you have a learned policy that requires no online planning, you enjoy high-speed inference. However: - Complexity and Instability: Real-world tasks may need a mix of methods (e.g., partial MPC, ensembles) to handle uncertainty and prevent exploitation of model errors.</p>"},{"location":"course_notes/model-based/#reward-overshooting-overoptimistic-planning","title":"Reward Overshooting (Overoptimistic Planning)","text":""},{"location":"course_notes/model-based/#what-it-is","title":"What It Is","text":"<ul> <li>Definition: When a planner or policy exploits incorrect or extrapolated high reward predictions from the learned model, leading to unrealistic or unsafe behavior in the real environment.</li> </ul>"},{"location":"course_notes/model-based/#why-it-happens","title":"Why It Happens","text":"<ul> <li>Sparse Coverage: The model has little data in certain regions, so it overestimates rewards there.  </li> <li>Open-Loop Plans: Versions 0.5 and 1.0 may chase these \u201cfantasy\u201d states for an entire rollout before correcting in the next iteration.  </li> <li>Uncertainty Blindness: If the approach doesn\u2019t penalize states with high model uncertainty, the planner may favor them solely because the model \u201cthinks\u201d they are high-reward.</li> </ul>"},{"location":"course_notes/model-based/#consequences","title":"Consequences","text":"<ul> <li>Poor Real-World Transfer: The agent\u2019s performance can appear great under the learned model but fail in actual interaction.  </li> <li>Safety Violations: In real-world robotics or critical applications, overshooting can lead to dangerous actions.</li> </ul>"},{"location":"course_notes/model-based/#mitigation-strategies","title":"Mitigation Strategies","text":"<ul> <li>Frequent Re-Planning (MPC): Correct for erroneous predictions step by step.  </li> <li>Iterative Data Collection: Gradually gather real data in uncertain regions.  </li> <li>Uncertainty-Aware Models: Ensembles or Bayesian approaches that identify high-uncertainty states and penalize them.  </li> <li>Regularization: Shorter horizon rollouts, trust regions, or other constraints limit destructive exploitation of model errors.</li> </ul>"},{"location":"course_notes/model-based/#other-challenges-and-notes","title":"Other Challenges and Notes","text":"<ol> <li> <p>Reward Function Misspecification. If the reward function itself is imperfect or learned, it can exacerbate overshooting or produce unintended behaviors.</p> </li> <li> <p>Stochastic Environments. Open-loop methods (Version 0.5, 1.0) can fail if they don\u2019t adapt in real time. MPC (Version 1.5) or robust policy optimization (Version 2.0) are better at handling randomness.</p> </li> <li> <p>Exploding/Vanishing Gradients. A big challenge for Version 2.0 when unrolling many timesteps through a neural model.</p> </li> <li> <p>Safety Concerns. In physical or high-stakes domains, any form of model inaccuracy can be dangerous. MPC is often the pragmatic choice in safety-critical tasks.</p> </li> <li> <p>Computational Trade-Offs. MPC (Version 1.5) can be expensive online. End-to-end policy learning (Version 2.0) moves the heavy lifting offline, but training is more delicate.</p> </li> </ol>"},{"location":"course_notes/model-based/#5-dynamics-model-learning","title":"5. Dynamics Model Learning","text":"<p>Learning a model \\(\\hat{P}(s_{t+1}\\mid s_t,a_t)\\) + \\(\\hat{R}(s_t,a_t)\\) is often done via supervised learning on transitions \\((s_t,a_t,s_{t+1},r_t)\\). This section surveys advanced considerations from Moerland et al. .</p>"},{"location":"course_notes/model-based/#51-basic-considerations","title":"5.1 Basic Considerations","text":"<ol> <li> <p>Type of Model </p> <ul> <li>Forward (most common): \\((s_t,a_t)\\mapsto s_{t+1}\\).  </li> <li>Backward (reverse model): \\(s_{t+1}\\mapsto (s_t,a_t)\\). Used in prioritized sweeping.  </li> <li>Inverse: \\((s_t, s_{t+1}) \\mapsto a_t\\). Sometimes used in representation learning or feedback control.</li> </ul> </li> <li> <p>Estimation Method </p> <ul> <li>Parametric (e.g., neural networks, linear regressors, GPs).  </li> <li>Non-parametric (e.g., nearest neighbors, kernel methods).  </li> <li>Exact (tabular) vs. approximate (function approximation).</li> </ul> </li> <li> <p>Region of Validity </p> <ul> <li>Global model: Attempt to capture all states. Common in large-scale MBRL.  </li> <li>Local model: Fit only around the current trajectory or region of interest (common in robotics, e.g., local linearization).</li> </ul> </li> </ol> <p> Overview of different types of mappings in model learning. 1) Standard Markovian transition model \\( s_t, a_t \\rightarrow s_{t+1} \\). 2) Partial observability. We model \\( s_0 \\ldots s_t, a_t \\rightarrow s_{t+1} \\), leveraging the state history to make an accurate prediction. 3) Multi-step prediction (Section 4.6), where we model \\( s_t, a_t \\ldots a_{t+n-1} \\rightarrow s_{t+n} \\), to predict the \\( n \\) step effect of a sequence of actions. 4) State abstraction, where we compress the state into a compact representation \\( z_t \\) and model the transition in this latent space. 5) Temporal/action abstraction, better known as hierarchical reinforcement learning, where we learn an abstract action \\( u_t \\) that brings us to \\( s_{t+n} \\). Temporal abstraction directly implies multi-step prediction, as otherwise the abstract action \\( u_t \\) is equal to the low level action \\( a_t \\). All the above ideas (2\u20135) are orthogonal and can be combined.</p>"},{"location":"course_notes/model-based/#52-stochasticity","title":"5.2 Stochasticity","text":"<p>Real MDPs can be stochastic, meaning that the environment transition from \\((s_t, a_t)\\) to \\(s_{t+1}\\) is governed by a distribution:</p> \\[ P\\bigl(s_{t+1} \\mid s_t, a_t \\bigr). \\] <p>Unlike a deterministic setting (where we might write \\(s_{t+1} = f(s_t, a_t)\\)), this transition function yields a probability distribution over all possible next states rather than a single outcome.</p>"},{"location":"course_notes/model-based/#521-multi-modal-transitions-and-the-conditional-mean-problem","title":"5.2.1 Multi-Modal Transitions and the Conditional Mean Problem","text":"<p>When training a purely deterministic network (e.g., a standard neural network with mean-squared error, MSE) to predict \\(s_{t+1}\\) from \\((s_t, a_t)\\), the model typically learns the conditional mean of the next-state distribution. This can be problematic if the true transition distribution is multi-modal, since the mean might not align with any actual or likely realization of the environment.</p> <p></p> <p>Illustration of stochastic transition dynamics. Left: 500 samples from an example transition function \\(P(s_{t+1} \\mid s, a)\\). The vertical dashed line indicates the cross-section distribution on the right. Right: distribution of \\(s_{t+1}\\) for a particular \\((s, a)\\). We observe a multimodal distribution. The conditional mean of this distribution, which would be predicted by MSE training, is shown as a vertical line.</p> <p>Formally, if the true next state is a random variable \\(S_{t+1}\\), then MSE-based regression gives</p> \\[ \\hat{s}_{t+1} \\;=\\; \\mathbb{E}\\bigl[S_{t+1}\\,\\big|\\,(s_t,a_t)\\bigr]. \\] <p>Hence, if \\(S_{t+1}\\) can take on several distinct modes with similar probabilities, a single mean prediction \\(\\hat{s}_{t+1}\\) may not capture the actual modes at all.</p>"},{"location":"course_notes/model-based/#522-descriptive-distribution-models","title":"5.2.2 Descriptive (Distribution) Models","text":"<p>To capture multi-modal dynamics rigorously, one can represent the full distribution \\(P(s_{t+1}\\mid s_t,a_t)\\). Common choices include:</p> <ol> <li> <p>Gaussian Distribution     Assume</p> \\[ s_{t+1} \\;\\sim\\; \\mathcal{N}\\bigl(\\mu_\\theta(s_t,a_t),\\,\\Sigma_\\theta(s_t,a_t)\\bigr), \\] <p>where \\(\\theta\\) denotes model parameters. Typically trained by maximizing log-likelihood of observed transitions.</p> </li> <li> <p>Gaussian Mixture Models (GMM)     Use a sum of \\(K\\) Gaussians:</p> \\[ s_{t+1} \\;\\sim\\; \\sum_{k=1}^{K}\\;\\alpha_k(\\theta; s_t,a_t)\\;\\mathcal{N}\\!\\Bigl(\\mu_k,\\;\\Sigma_k\\Bigr). \\] <p>The mixture weights \\(\\alpha_k\\) sum to 1. This better captures multi-modality than a single Gaussian but can be more complex to train (e.g., via EM).</p> </li> <li> <p>Tabular/Histogram-Based     For lower-dimensional or discrete states:</p> \\[ \\hat{P}\\bigl(s' \\mid s,a\\bigr) \\;=\\; \\frac{n(s,a,s')}{\\sum_{\\tilde{s}}\\,n(s,a,\\tilde{s})}, \\] <p>where \\(n(\\cdot)\\) counts observed transitions. This is often infeasible in large or continuous domains.</p> </li> </ol>"},{"location":"course_notes/model-based/#523-generative-approaches","title":"5.2.3 Generative Approaches","text":"<p>Instead of closed-form probability distributions, one might learn a generative mapping that samples from \\(P(s_{t+1}\\mid s_t,a_t)\\). Examples:</p> <ol> <li> <p>Variational Autoencoders (VAEs)     Introduce a latent variable \\(\\mathbf{z}\\). Then</p> \\[     s_{t+1} \\;=\\; f_\\theta\\!\\bigl(\\mathbf{z},\\,s_t,\\,a_t\\bigr),      \\quad      \\mathbf{z}\\;\\sim\\;\\mathcal{N}(\\mathbf{0},\\mathbf{I}), \\] <p>and fit \\(\\theta\\) via variational inference. Inference-time sampling of \\(\\mathbf{z}\\) yields diverse future states.</p> </li> <li> <p>Normalizing Flows     Transform a simple base distribution (like a Gaussian) through a stack of invertible mappings \\(\\{f_\\theta^{(\\ell)}\\}\\):</p> \\[ \\mathbf{z}\\,\\sim\\,\\mathcal{N}(\\mathbf{0},\\mathbf{I}),  \\quad s_{t+1} \\;=\\; (f_\\theta^{(L)} \\circ \\cdots \\circ f_\\theta^{(1)})(\\mathbf{z}). \\] <p>Optimized via maximum likelihood, enabling expressive densities.</p> </li> <li> <p>Generative Adversarial Networks (GANs)     A discriminator \\(D\\) distinguishes real vs. generated next states, while the generator \\(G\\) attempts to fool \\(D\\). Though flexible, GAN training can be unstable or prone to mode collapse.</p> </li> <li> <p>Autoregressive Models     Factorize high-dimensional \\(s_{t+1}\\) into a chain of conditionals. Useful for image-based transitions but can be computationally heavy.</p> </li> </ol>"},{"location":"course_notes/model-based/#524-training-objectives","title":"5.2.4 Training Objectives","text":"<p>Most distribution models are trained by maximizing likelihood or minimizing negative log-likelihood over a dataset \\(\\{(s_t^{(i)}, a_t^{(i)}, s_{t+1}^{(i)})\\}\\). For example:</p> \\[     \\theta^*      =      \\arg\\max_\\theta      \\sum_{i=1}^N      \\log P\\bigl(s_{t+1}^{(i)} \\mid s_t^{(i)},a_t^{(i)};\\,\\theta \\bigr)     -\\;     \\Omega(\\theta), \\] <p>where \\(\\Omega(\\theta)\\) might be a regularization term. GAN-based models use a min-max objective, while VAE-based methods use an ELBO that includes a reconstruction term and a KL prior penalty on the latent space.</p>"},{"location":"course_notes/model-based/#525-practical-considerations-and-challenges","title":"5.2.5 Practical Considerations and Challenges","text":"<ul> <li> <p>Divergence in Multi-Step Rollouts   Even with a stochastic model, errors can accumulate as predictions are fed back into the model. Mitigations include unrolling during training, multi-step loss functions, or specialized architectures.</p> </li> <li> <p>Mode Collapse / Rare Transitions   Multi-modal distributions can be hard to learn in practice. Models must capture all critical modes, especially for safety or robotics contexts where minority transitions may be crucial.</p> </li> <li> <p>High-Dimensional Observations   Image-based tasks often leverage latent-variable models (e.g., VAE-like) to reduce dimensionality. Encoding \\(\\rightarrow\\) (predict in latent space) \\(\\rightarrow\\) decoding is common.</p> </li> <li> <p>Expressiveness vs. Efficiency   Complex generative models (e.g., large mixtures or flows) capture stochasticity better but are often slower to train and evaluate. Many real-world agents resort to simpler unimodal Gaussians, balancing speed and accuracy.</p> </li> </ul>"},{"location":"course_notes/model-based/#526-example-gaussian-transitions-via-maximum-likelihood","title":"5.2.6 Example: Gaussian Transitions via Maximum Likelihood","text":"<p>A common assumption uses a unimodal Gaussian:</p> \\[ s_{t+1} \\;\\sim\\; \\mathcal{N}\\Bigl(\\mu_\\theta(s_t,a_t),\\;\\Sigma_\\theta(s_t,a_t)\\Bigr). \\] <p>Assume diagonal covariance \\(\\Sigma_\\theta=\\mathrm{diag}(\\sigma_1^2,\\dots,\\sigma_d^2)\\). The log-likelihood for one observed transition is:</p> \\[ \\log p\\bigl(s_{t+1}\\mid s_t,a_t;\\,\\theta\\bigr) \\,=\\, \\sum_{j=1}^d  \\Bigl[ -\\tfrac12\\,\\ln\\bigl(2\\pi\\,\\sigma_j^2(\\cdot)\\bigr) -\\; \\tfrac{\\bigl(s_{t+1}[j]-\\mu_j(\\cdot)\\bigr)^2}{2\\,\\sigma_j^2(\\cdot)} \\Bigr]. \\] <p>Maximizing this finds a Gaussian best-fit to the empirical data. While unimodal, it remains a popular, tractable choice for continuous control.</p>"},{"location":"course_notes/model-based/#527-concluding-remarks-on-stochastic-transitions","title":"5.2.7 Concluding Remarks on Stochastic Transitions","text":"<ul> <li> <p>Why Model Stochasticity?   Real-world dynamics often have multiple plausible next states. Failing to capture these can produce inaccurate planning and suboptimal policies.</p> </li> <li> <p>Descriptive vs. Generative   Some tasks demand full density estimation (e.g., risk-sensitive planning), while others only require sampling plausible transitions (e.g., for Monte Carlo rollouts).</p> </li> <li> <p>Integration with RL   Once a model is learned, the agent can plan by either averaging over transitions or sampling them. Handling many branching futures can be computationally expensive; practical approaches limit the depth or use heuristic expansions.</p> </li> </ul> <p>In sum, real-world MDPs often present multi-modal and stochastic dynamics. A purely deterministic predictor trained via MSE collapses the distribution to a single mean. Instead, we can use distribution (e.g., GMM) or generative (e.g., VAE, flows, GANs) approaches, trained via maximum likelihood, adversarial losses, or variational inference. Proper handling of stochastic transitions is essential for robust planning, policy optimization, and multi-step simulation in model-based RL.</p>"},{"location":"course_notes/model-based/#53-uncertainty","title":"5.3 Uncertainty","text":"<p>A critical challenge in MBRL is model uncertainty\u2014the model is learned from limited data, so predictions can be unreliable in unfamiliar state-action regions. We distinguish:</p> <ul> <li>Aleatoric (intrinsic) uncertainty: inherent stochasticity in transitions.  </li> <li>Epistemic (model) uncertainty: arises from limited training data. This can, in principle, be reduced by gathering more data.</li> </ul> <p>A rigorous approach is to maintain a distribution over possible models, then plan by integrating or sampling from that distribution to avoid catastrophic exploitation of untrusted model regions.</p>"},{"location":"course_notes/model-based/#bayesian-neural-networks","title":"Bayesian Neural Networks","text":"<p>One approach is a Bayesian neural network (BNN):</p> \\[ \\theta \\sim p(\\theta), \\quad s_{t+1} \\sim P_\\theta(\\cdot \\mid s_t, a_t). \\] <p>We keep a posterior \\(p(\\theta\\mid D)\\) over network weights \\(\\theta\\) given dataset \\(D\\). Predictive distribution for the next state is then:</p> \\[ p(s_{t+1}\\mid s_t,a_t, D) = \\int P_\\theta(s_{t+1}\\mid s_t,a_t)\\,p(\\theta\\mid D)\\,d\\theta. \\] <p>In practice, approximations like variational dropout or Laplace approximation are used to sample from \\(p(\\theta)\\).</p>"},{"location":"course_notes/model-based/#ensembles-and-bootstrapping","title":"Ensembles and Bootstrapping","text":"<p>Another popular method is an ensemble of \\(N\\) models \\(\\{\\hat{P}_{\\theta_i}\\}\\). Each model is trained on a bootstrapped subset of the data (or with different initialization seeds). The variance across predictions:</p> \\[ \\mathrm{Var}\\bigl[\\hat{P}_{\\theta_i}(s_{t+1}\\mid s_t,a_t)\\bigr] \\] <p>indicates epistemic uncertainty. In practice:</p> \\[ \\hat{\\mu}_\\mathrm{ensemble}(s_{t+1}) \\approx \\frac{1}{N}\\sum_{i=1}^N \\hat{\\mu}_i(s_{t+1}), \\quad \\hat{\\Sigma}_\\mathrm{ensemble}(s_{t+1}) \\approx \\frac{1}{N}\\sum_{i=1}^N \\bigl(\\hat{\\mu}_i - \\hat{\\mu}_\\mathrm{ensemble}\\bigr)^2. \\] <p>During planning, one may:</p> <ol> <li>Sample one model from the ensemble at each step (like PETS).</li> <li>Average the predictions or treat it as a Gaussian mixture model.</li> </ol> <p>Either way, uncertain regions typically manifest as a large disagreement among ensemble members, warning the planner not to trust that zone.</p> <p></p> <p>Mathematically: if the \u201ctrue\u201d dynamics distribution is \\(P^\\star\\) and each model \\(\\hat{P}_{\\theta_i}\\) is an unbiased estimator, then large variance across \\(\\{\\hat{P}_{\\theta_i}\\}\\) signals a region outside the training distribution. Minimizing that variance can guide exploration or help shape conservative planning.</p>"},{"location":"course_notes/model-based/#54-partial-observability","title":"5.4 Partial Observability","text":"<p>Sometimes the environment is not fully observable. We can\u2019t identify the full state \\(s\\) from a single observation \\(o\\). Solutions:</p> <ul> <li>Windowing: Keep last \\(n\\) observations \\((o_t, o_{t-1}, \\dots)\\).  </li> <li>Belief state: Use a hidden Markov model or Bayesian filter.  </li> <li>Recurrence: Use RNNs/LSTMs that carry hidden state \\(\\mathbf{h}_t\\).  </li> <li>External Memory: Neural Turing Machines, etc., for long-range dependencies.</li> </ul>"},{"location":"course_notes/model-based/#55-non-stationarity","title":"5.5 Non-Stationarity","text":"<p>Non-stationary dynamics means that \\(P\\) or \\(R\\) changes over time. A single learned model can become stale. Approaches include:</p> <ul> <li>Partial models [Doya et al., 2002]: Maintain multiple submodels for different regimes, detect changes in transition error.  </li> <li>High learning-rate or forgetting older data to adapt quickly.</li> </ul>"},{"location":"course_notes/model-based/#56-multi-step-prediction","title":"5.6 Multi-Step Prediction","text":"<p>One-step predictions can accumulate error when rolled out repeatedly. Some solutions:</p> <ul> <li>Train for multi-step: Unroll predictions for \\(k\\) steps and backprop against ground truth \\((s_{t+k})\\).  </li> <li>Dedicated multi-step models: Instead of chaining one-step predictions, learn \\(f^{(k)}(s_t,a_{t:k-1})\\approx s_{t+k}\\).</li> </ul> <p></p>"},{"location":"course_notes/model-based/#57-state-abstraction","title":"5.7 State Abstraction","text":"<p>In many domains, the raw observation space \\(s\\) can be very high-dimensional (e.g., pixel arrays), making direct modeling of \\((s_t, a_t) \\mapsto s_{t+1}\\) intractable. State abstraction (or representation learning) tackles this issue by learning a more compact latent state \\(\\mathbf{z}_t\\), capturing the essential factors of variation. Once in this latent space, the model learns transitions \\(\\mathbf{z}_{t+1} = f_{\\text{trans}}(\\mathbf{z}_t, a_t)\\), and can decode back to the original space if needed:</p> \\[ \\mathbf{z}_t = f_{\\text{enc}}(s_t),  \\quad \\mathbf{z}_{t+1} = f_{\\text{trans}}(\\mathbf{z}_t, a_t),  \\quad s_{t+1} \\approx f_{\\text{dec}}(\\mathbf{z}_{t+1}). \\] <p>This structure reduces modeling complexity and can enable more efficient planning or control in the latent domain.</p>"},{"location":"course_notes/model-based/#571-common-approaches-to-representation-learning","title":"5.7.1 Common Approaches to Representation Learning","text":"<ol> <li> <p>Autoencoders and Variational Autoencoders (VAEs)     An autoencoder aims to learn an encoding \\(\\mathbf{z}=f_{\\text{enc}}(s)\\) that, when passed through a decoder \\(f_{\\text{dec}}\\), reconstructs the original state \\(s\\). In variational autoencoders (VAEs), one imposes a prior distribution \\(p(\\mathbf{z})\\) (often Gaussian) over the latent space, adding a Kullback\u2013Leibler (KL) divergence penalty:</p> \\[ \\max_{\\theta,\\phi}  \\;\\; \\mathbb{E}_{q_\\phi(\\mathbf{z}\\mid s)} \\Bigl[\\log p_\\theta(s\\mid \\mathbf{z})\\Bigr] \\;-\\; D_{\\mathrm{KL}}\\bigl(q_\\phi(\\mathbf{z}\\mid s)\\,\\|\\,p(\\mathbf{z})\\bigr), \\] <p>where \\(q_\\phi(\\mathbf{z}\\mid s)\\) is the encoder distribution and \\(p_\\theta(s\\mid \\mathbf{z})\\) is the decoder. This ensures the learned latent code \\(\\mathbf{z}\\) both reconstructs well and remains \u201corganized\u201d under the prior \\(p(\\mathbf{z})\\). Once learned, a latent dynamics model \\(\\mathbf{z}_{t+1}=f_{\\text{trans}}(\\mathbf{z}_t,a_t)\\) can be fitted by minimizing a prediction loss (e.g., mean-squared error on \\(\\mathbf{z}\\)-space).</p> </li> <li> <p>Object-Based Approaches     For environments that can be factorized into distinct objects (e.g., multiple physical entities), Graph Neural Networks (GNNs) or object-centric models can more naturally capture the underlying structure. Concretely, each latent node \\(\\mathbf{z}_i\\) corresponds to an object\u2019s state (e.g., location, velocity, shape), and edges model interactions among objects. Formally,</p> \\[ \\mathbf{z}_{t+1}^{(i)}  \\;=\\; f_{\\text{trans}}^{(i)} \\Bigl(     \\mathbf{z}_t^{(i)},\\,a_t,\\,     \\{\\mathbf{z}_t^{(j)}\\}_{j \\in \\mathcal{N}(i)} \\Bigr), \\] <p>where \\(\\mathcal{N}(i)\\) denotes neighbors of object \\(i\\). This is particularly effective in physics-based settings [Battaglia et al., 2016], allowing each object\u2019s transition to depend primarily on relevant neighbors (e.g., collisions). Such structured representations often facilitate better generalization to new configurations (e.g., changing the number or arrangement of objects).</p> </li> <li> <p>Contrastive Losses for Semantic/Controllable Features     Sometimes, a purely reconstruction-based loss can over-focus on visually salient but decision-irrelevant details. Contrastive methods use pairs (or triplets) of observations to emphasize meaningful relationships. For instance, if two states \\(s\\) and \\(s'\\) are known to be dynamically close (reachable with few actions), one encourages their embeddings \\(\\mathbf{z}\\) and \\(\\mathbf{z}'\\) to be close under some metric. Formally, a contrastive loss might look like:</p> \\[ \\ell_{\\mathrm{contrast}}(\\mathbf{z}, \\mathbf{z}')  \\;=\\; y\\,\\|\\mathbf{z}-\\mathbf{z}'\\|^2  \\;+\\; (1-y)\\,\\bigl[\\alpha - \\|\\mathbf{z}-\\mathbf{z}'\\|\\bigr]_{+}, \\] <p>where \\(y=1\\) if the states should be similar, and \\(y=0\\) otherwise, and \\(\\alpha\\) is a margin. Examples include time-contrastive approaches [Sermanet et al., 2018] that bring together frames from different camera angles but the same physical scene, or goal-oriented distances [Ghosh et al., 2018]. These tasks guide the learned \\(\\mathbf{z}\\)-space to capture features that matter for control, rather than trivial background details.</p> </li> </ol>"},{"location":"course_notes/model-based/#572-planning-in-latent-space","title":"5.7.2 Planning in Latent Space","text":"<p>Once a latent representation is established, an agent can:</p> <ol> <li> <p>Plan directly in \\(\\mathbf{z}\\)-space    For instance, run a forward-search or gradient-based policy optimization with states \\(\\mathbf{z}\\) and transitions \\(f_{\\text{trans}}(\\mathbf{z}, a)\\). If the decoder \\(f_{\\text{dec}}\\) is not explicitly needed (e.g., if the agent only needs to output actions), planning in the latent domain can reduce computational overhead and reduce the \u201ccurse of dimensionality.\u201d</p> </li> <li> <p>Decode for interpretability or environment feedback    If the environment requires real-world actions or if interpretability is desired, one can decode predicted latent states \\(\\mathbf{z}_{t+1}\\) to \\(\\hat{s}_{t+1}\\). The environment then checks feasibility or yields a reward. This is especially relevant when the environment is external (like a simulator or the real world) expecting inputs in the original space.</p> </li> </ol> <p>A primary challenge is rollout mismatch: if \\(\\mathbf{z}_{t+1}\\) is never trained to match \\(f_{\\text{enc}}(s_{t+1})\\), repeated application of \\(f_{\\text{trans}}\\) might accumulate errors. Solutions include explicit consistency constraints (i.e., \\(\\|\\mathbf{z}_{t+1} - f_{\\text{enc}}(s_{t+1})\\|\\) penalties) or probabilistic latent inference (e.g., Kalman filter variants, sequential VAEs).</p>"},{"location":"course_notes/model-based/#58-temporal-abstraction","title":"5.8 Temporal Abstraction","text":"<p>In Markov Decision Processes (MDPs), each action typically spans one environment step. But many tasks have natural subroutines that can be chunked into higher-level actions. This is the essence of hierarchical reinforcement learning (HRL): define \u201cmacro-actions\u201d that unfold over multiple timesteps, reducing effective planning depth and often improving data efficiency.</p>"},{"location":"course_notes/model-based/#581-options-framework","title":"5.8.1 Options Framework","text":"<p>An Option \\(\\omega\\) is a tuple \\((I_\\omega, \\pi_\\omega, \\beta_\\omega)\\) [Sutton et al., 1999]:</p> <ul> <li>Initiation set \\(I_\\omega \\subseteq \\mathcal{S}\\): states from which \\(\\omega\\) can be invoked.</li> <li>Subpolicy \\(\\pi_\\omega(a \\mid s)\\): governs low-level actions while the option runs.</li> <li>Termination condition \\(\\beta_\\omega(s)\\): probability that \\(\\omega\\) terminates upon reaching state \\(s\\).</li> </ul> <p>When executing option \\(\\omega\\), the agent follows \\(\\pi_\\omega\\) until a stochastic termination event triggers, transitioning back to the high-level policy\u2019s control. The high-level policy thus selects from a set of options, each a multi-step \u201cchunk\u201d of actions. This can drastically reduce the horizon of the planning problem.</p> <p>Mathematically, a semi-MDP formalism captures these temporally extended actions, where option \\(\\omega\\) yields a multi-step transition \\((s, \\omega)\\mapsto s'\\). One can learn option-value functions \\(Q(s,\\omega)\\) or plan with pseudo-rewards inside each option. In practice, good options can significantly accelerate learning and planning compared to primitive actions.</p>"},{"location":"course_notes/model-based/#582-goal-conditioned-policies","title":"5.8.2 Goal-Conditioned Policies","text":"<p>Alternatively, goal-conditioned or universal value functions [Schaul et al., 2015] define a function \\(Q(s, a, g)\\) that specifies the expected return for taking action \\(a\\) in state \\(s\\) while aiming to achieve goal \\(g\\). One can then plan by selecting subgoals:</p> \\[ g_1, g_2, \\dots, g_K, \\] <p>where each \u201cmacro-step\u201d is the agent trying to reach \\(g_i\\) from the current state. Feudal RL frameworks [Dayan and Hinton, 1993] similarly treat higher-level \u201cmanagers\u201d that set subgoals for lower-level \u201cworkers.\u201d A learned, goal-conditioned subpolicy \\(\\pi(a\\mid s,g)\\) can generalize across different goals \\(g\\), unlike the options approach that typically uses separate subpolicies per option.</p>"},{"location":"course_notes/model-based/#583-subgoal-discovery","title":"5.8.3 Subgoal Discovery","text":"<p>A key research question is how to discover effective macro-actions (options) or subgoals. Approaches include:</p> <ol> <li> <p>Graph-based Bottlenecks    Construct or approximate a graph of states/regions. Identify bottlenecks as states that connect densely connected regions. Formally, if a state \\(s\\) lies on many shortest paths between subregions of the state space, it can be a strategic \u201cbridge.\u201d Setting it as a subgoal can simplify global planning [Menache et al., 2002].</p> </li> <li> <p>Empowerment / Coverage    Encourage subpolicies that cover different parts of the state space or yield high controllability. For instance, one might maximize mutual information between subpolicy latent codes and resulting states, ensuring distinct subpolicies lead to distinct outcomes. This fosters diverse skill discovery.</p> </li> <li> <p>End-to-End Learning    Methods like Option-Critic [Bacon et al., 2017] embed option structure into a differentiable policy architecture and optimize for return. The subpolicies and termination functions emerge from gradient-based training, though careful regularization is often needed to avoid degenerate solutions (e.g., a single subpolicy doing everything).</p> </li> </ol>"},{"location":"course_notes/model-based/#584-benefits-of-temporal-abstraction","title":"5.8.4 Benefits of Temporal Abstraction","text":"<ul> <li>Reduced Planning Depth   Since each macro-action can span multiple timesteps, the effective decision horizon shrinks, often simplifying search or dynamic programming.</li> <li>Transfer and Reuse   Once discovered, options/subpolicies can be reused in related tasks. If those subroutines correspond to meaningful skills, the agent may quickly adapt to new goals.</li> <li>Data Efficiency   Higher-level actions can yield more stable and purposeful exploration, collecting relevant experience faster than random primitive actions.</li> </ul>"},{"location":"course_notes/model-based/#6-integration-of-planning-and-learning","title":"6. Integration of Planning and Learning","text":"<p>With a learned model in hand (or a known one), we combine planning and learning to optimize a policy \\(\\pi\\). We address four major questions:</p> <ol> <li>Which state to start planning from? </li> <li>Budget and frequency: how many real steps vs. planning steps?  </li> <li>Planning algorithm: forward search, MCTS, gradient-based, etc.  </li> <li>Integration: how planning outputs feed into policy/value updates and final action selection.</li> </ol> <p></p>"},{"location":"course_notes/model-based/#61-which-state-to-start-planning-from","title":"6.1 Which State to Start Planning From?","text":"<ul> <li>Uniform over all states (like classical dynamic programming).  </li> <li>Visited states only (like Dyna [Sutton, 1990], which samples from replay).  </li> <li>Prioritized (Prioritized Sweeping [Moore &amp; Atkeson, 1993]) if some states need urgent update.  </li> <li>Current state only (common in online MPC or MCTS from the real agent\u2019s state).</li> </ul>"},{"location":"course_notes/model-based/#62-planning-budget-vs-real-data-collection","title":"6.2 Planning Budget vs. Real Data Collection","text":"<p>Two sub-questions:</p> <ol> <li> <p>Frequency: plan after every environment step, or collect entire episodes first?  </p> <ul> <li>Dyna plans after each step (like 100 imaginary updates per real step).  </li> <li>PILCO [Deisenroth &amp; Rasmussen, 2011] fits a GP model after each episode.</li> </ul> </li> <li> <p>Budget: how many model rollouts or expansions per planning cycle?  </p> <ul> <li>Dyna might do 100 short rollouts.  </li> <li>AlphaZero expands a single MCTS iteration by up to 1600 \u00d7 depth calls.</li> </ul> </li> </ol> <p>Some methods adaptively adjust planning vs. real data based on model uncertainty [Kalweit &amp; Boedecker, 2017]. The right ratio can significantly affect performance.</p>"},{"location":"course_notes/model-based/#63-how-to-plan-planning-algorithms","title":"6.3 How to Plan? (Planning Algorithms)","text":"<p>Broadly:</p> <ol> <li> <p>Discrete (non-differentiable) search:</p> <ul> <li>One-step lookahead  </li> <li>Tree search (MCTS, minimax)  </li> <li>Forward vs. backward: e.g., prioritized sweeping uses a reverse model to propagate value changes quickly</li> </ul> </li> <li> <p>Differential (gradient-based) planning:</p> <ul> <li>Requires a differentiable model \\(\\hat{P}\\).  </li> <li>E.g., iterative LQR, or direct backprop through unrolled transitions (Dreamer).  </li> <li>Suited for continuous control with smooth dynamics.</li> </ul> </li> <li> <p>Depth &amp; Breadth choices:</p> <ul> <li>Some do short-horizon expansions (MBPO uses 1\u20135 step imaginary rollouts).  </li> <li>Others do deeper expansions if computing resources allow (AlphaZero MCTS).</li> </ul> </li> <li> <p>Uncertainty handling:</p> <ul> <li>Plan only near states with low model uncertainty or penalize uncertain states.  </li> <li>Ensemble-based expansions [Chua et al., 2018].</li> </ul> </li> </ol> <p>Cross-Entropy Method (CEM) \u2013 Pseudocode</p> <pre><code>    # Suppose we want to find the best action sequence of length H\n    # that maximizes the expected return under our model.\n\n    Initialize distribution params (mean mu, covariance Sigma)\n    for iteration in range(N_iterations):\n        # 1. Sample K sequences from current distribution\n        candidate_sequences = sample_from_gaussian(mu, Sigma, K)\n\n        # 2. Evaluate each sequence's return\n        returns = []\n        for seq in candidate_sequences:\n            returns.append( evaluate_return(seq, model) )\n\n        # 3. Select the top M (elite) sequences\n        elite_indices = top_indices(returns, M)\n        elites = [candidate_sequences[i] for i in elite_indices]\n\n        # 4. Update mu, Sigma to fit the elites\n        mu = mean(elites)\n        Sigma = cov(elites)\n\n    # Final distribution reflects the best action sequence\n    best_action_seq = mu\n    return best_action_seq\n</code></pre>"},{"location":"course_notes/model-based/#631-monte-carlo-tree-search-mcts","title":"6.3.1 Monte Carlo Tree Search (MCTS)","text":"<p>Monte Carlo Tree Search is a powerful method for discrete action planning\u2014famously used in AlphaGo, AlphaZero, MuZero. Key components:</p> <ol> <li> <p>Tree Representation </p> <ul> <li>Each node is a state, edges correspond to actions.  </li> <li>MCTS incrementally expands the search tree from a root (the current state).</li> </ul> </li> <li> <p>Four Steps commonly described as:</p> <ol> <li>Selection: Repeatedly choose child nodes (actions) from the root, typically via Upper Confidence Bound or policy heuristics, until reaching a leaf node.  </li> <li>Expansion: If the leaf is not terminal (or at max depth), add one or more child nodes for possible next actions.  </li> <li>Simulation: From that new node, simulate a rollout (random or policy-driven) until reaching a terminal state or horizon.  </li> <li>Backpropagation: Propagate the return from the simulation up the tree to update value/statistics at each node.</li> </ol> </li> <li> <p>Mathematical Form </p> <ul> <li>Let \\(N(s,a)\\) be the number of visits to child action \\(a\\) from state \\(s\\).  </li> <li>Let \\(\\hat{Q}(s,a)\\) be the estimated action-value from MCTS.  </li> <li> <p>UCB selection uses:</p> \\[ a_\\text{select} = \\arg\\max_{a}\\Bigl[\\hat{Q}(s,a) + c \\sqrt{\\frac{\\ln \\sum_{b} N(s,b)}{N(s,a)}}\\Bigr]. \\] <p>(One can also incorporate a learned prior policy \\(\\pi_\\theta\\) to bias exploration.)</p> </li> </ul> </li> <li> <p>Planning &amp; Policy Extraction </p> <ul> <li>After many simulations from the root, MCTS typically normalizes node visits or Q-values to produce a final policy distribution \\(\\alpha\\).  </li> <li>This policy \\(\\alpha\\) may be used for real action selection, or to train a global policy network (as in AlphaZero).</li> </ul> </li> </ol> <p>MCTS Pseudocode</p> <pre><code>MCTS(root_state, model, N_simulations)\nInitialize the search tree with root_state\n\nfor simulation in 1 to N_simulations do\n    node \u2190 root of the tree\n\n    # Selection\n    while node is fully expanded and node is not terminal do\n        action \u2190 select child of node using UCB\n        node \u2190 child corresponding to action\n\n    # Expansion\n    if node is not terminal then\n        expand node using model (generate all children)\n        node \u2190 select one of the new children\n\n    # Simulation\n    reward \u2190 simulate from node.state using model\n\n    # Backpropagation\n    backpropagate reward up the tree from node\n\n# Final decision\nReturn action from root with highest visit count\n</code></pre> <p></p>"},{"location":"course_notes/model-based/#64-integration-in-the-learning-and-acting-loop","title":"6.4 Integration in the Learning and Acting Loop","text":"<p>Key integration channels:</p> <ol> <li> <p>Planning input from existing policy/value?  </p> <ul> <li>E.g., MCTS uses a prior policy to guide expansions.</li> </ul> </li> <li> <p>Planning output as a training target for the global policy/value?  </p> <ul> <li>E.g., Dyna uses imaginary transitions to update Q-values.  </li> <li>AlphaZero uses MCTS results as a learning target for \\(\\pi\\) and \\(V\\).</li> </ul> </li> <li> <p>Action selection from the planning procedure or from the learned policy?</p> <ul> <li>E.g., MPC picks the best action from a planned sequence.  </li> <li>Or a final learned policy is used if no real-time planning is feasible.</li> </ul> </li> </ol> <p></p> <p>Various combinations exist: some methods rely mostly on the learned policy but refine or correct it with a short replan (MBPO), while others do a full MCTS at every step (MuZero).</p>"},{"location":"course_notes/model-based/#65-dyna-and-dyna-style-methods","title":"6.5 Dyna and Dyna-Style Methods","text":"<p>One of the earliest and most influential frameworks for model-based RL is Dyna [Sutton, 1990]. The key insight is to integrate:</p> <ul> <li>Real experience from the environment (sampled transitions)  </li> <li>Model learning from that real data  </li> <li>Imagined experience from the learned model to augment the policy/value updates.</li> </ul> <p>Dyna Pseudocode</p> <pre><code>Input: \u03b1 (learning rate), \u03b3 (discount factor), \u03b5 (exploration rate), \n       n (number of planning steps), num_episodes\n\nInitialize Q(s, a) arbitrarily for all states s \u2208 S, actions a \u2208 A\nInitialize Model as an empty mapping: Model(s, a) \u2192 (r, s')\n\nfor each episode do\n    Initialize state s \u2190 starting state\n\n    while s is not terminal do\n        \u25b8 Action Selection (\u03b5-greedy)\n        With probability \u03b5: choose random action a\n        Else: choose a \u2190 argmax_a Q(s, a)\n\n        \u25b8 Real Interaction\n        Take action a, observe reward r and next state s'\n\n        \u25b8 Q-Learning Update\n        Q(s, a) \u2190 Q(s, a) + \u03b1 [ r + \u03b3 \u00b7 max_a' Q(s', a') \u2212 Q(s, a) ]\n\n        \u25b8 Model Update\n        Model(s, a) \u2190 (r, s')\n\n        \u25b8 Planning (n simulated updates)\n        for i = 1 to n do\n            Randomly select previously seen (\u015d, \u00e2)\n            (r\u0302, \u015d') \u2190 Model(\u015d, \u00e2)\n\n            Q(\u015d, \u00e2) \u2190 Q(\u015d, \u00e2) + \u03b1 [ r\u0302 + \u03b3 \u00b7 max_a' Q(\u015d', a') \u2212 Q(\u015d, \u00e2) ]\n\n        \u25b8 Move to next real state\n        s \u2190 s'\n\n\n</code></pre> <ol> <li>Real Interaction: We pick action \\(a\\) in state \\(s\\) using \\(\\epsilon\\)-greedy w.r.t. \\(Q\\).  </li> <li>Update Q from real transition \\((s,a,s',r)\\).  </li> <li>Update Model: store or learn to predict \\(\\hat{P}(s'\\mid s,a)\\), \\(\\hat{R}(s,a)\\).  </li> <li>Imagination (N_planning steps): randomly sample a state-action pair from replay or memory, query the model for \\(\\hat{s}', \\hat{r}\\). Update \\(Q\\) with that synthetic transition.</li> </ol> <p></p> <p>Benefits:       - Dyna can drastically reduce real environment interactions by effectively replaying or generating new transitions from the learned model.       - Even short rollouts or repeated \u201cone-step planning\u201d from random visited states helps refine Q-values more quickly.</p> <p>Dyna-Style in modern deep RL:       - Many algorithms (e.g., MBPO) add short-horizon imaginary transitions to an off-policy buffer.       - They differ in details: how many model steps, how they sample states for imagination, how they manage uncertainty, etc.</p>"},{"location":"course_notes/model-based/#7-modern-model-based-rl-algorithms","title":"7. Modern Model-Based RL Algorithms","text":"<p>Modern model-based RL builds on classical ideas (global/local models, MPC, iterative re-fitting) but incorporates powerful neural representations, uncertainty handling, and integrated planning-learning frameworks. Below are five influential algorithms that illustrate the state of the art in contemporary MBRL.</p>"},{"location":"course_notes/model-based/#71-world-models-ha-schmidhuber-2018","title":"7.1 World Models (Ha &amp; Schmidhuber, 2018)","text":"<p>Core Idea Train a latent generative model of the environment (specifically from high-dimensional inputs like images), and then learn or optimize a policy entirely within this learned latent space\u2014the so-called \u201cdream environment.\u201d</p> <p></p> <p>Key Components</p> <ol> <li> <p>Variational Autoencoder (VAE):  </p> <ul> <li>Maps raw observation \\(\\mathbf{o}_t\\) to a compact latent representation \\(\\mathbf{z}_t\\).  </li> <li>\\(\\mathbf{z}_t = E_\\phi(\\mathbf{o}_t)\\) where \\(E_\\phi\\) is the learned encoder.  </li> <li>Reconstruction loss ensures \\(E_\\phi\\) and a corresponding decoder \\(D_\\phi\\) compress and reconstruct images effectively.</li> </ul> </li> <li> <p>Recurrent Dynamics Model (MDN-RNN):  </p> <ul> <li>Predicts the next latent \\(\\mathbf{z}_{t+1}\\) given \\(\\mathbf{z}_t\\) and action \\(a_t\\).  </li> <li> <p>Often parameterized as a Mixture Density Network inside an RNN:  </p> \\[   \\mathbf{z}_{t+1} \\sim p_\\theta(\\mathbf{z}_{t+1} \\mid \\mathbf{z}_t, a_t). \\] </li> <li> <p>This distribution can be modeled by a mixture of Gaussians, providing a probabilistic estimate of the next latent state.</p> </li> </ul> </li> <li> <p>Controller (Policy):  </p> <ul> <li>A small neural network \\(\\pi_\\eta\\) that outputs actions \\(a_t = \\pi_\\eta(\\mathbf{z}_t)\\) in the latent space.  </li> <li>Trained (in the original paper) via an evolutionary strategy (e.g., CMA-ES) entirely in the dream world.  </li> </ul> </li> </ol> <p>Algorithmic Flow</p> <ol> <li>Unsupervised Phase: Run a random or exploratory policy in the real environment, collect observations \\(\\mathbf{o}_1, \\mathbf{o}_2, ...\\).  </li> <li>Train the VAE to learn \\(\\mathbf{z} = E_\\phi(\\mathbf{o})\\).  </li> <li>Train the MDN-RNN on sequences \\((\\mathbf{z}_t, a_t, \\mathbf{z}_{t+1})\\).  </li> <li>\u201cDream\u201d: Roll out the MDN-RNN from random latents and evaluate candidate controllers \\(\\pi_\\eta\\).  </li> <li>Update \\(\\pi_\\eta\\) based on the dream performance (e.g., via evolutionary search).</li> </ol> <p>Significance</p> <ul> <li>Demonstrated that an agent can learn a world model of high-dimensional environments (CarRacing, VizDoom) and train policies in \u201clatent imagination.\u201d  </li> <li>Paved the way for subsequent latent-space MBRL (PlaNet, Dreamer).</li> </ul>"},{"location":"course_notes/model-based/#72-pets-chua-et-al-2018","title":"7.2 PETS (Chua et al., 2018)","text":"<p>Core Idea Probabilistic Ensembles with Trajectory Sampling (PETS) uses ensemble neural network dynamics models to capture epistemic uncertainty, combined with sampling-based planning (like the Cross-Entropy Method, CEM) for continuous control.</p> <p></p> <p>Modeling Uncertainty</p> <ul> <li>Train \\(N\\) distinct neural networks \\(\\{\\hat{P}_{\\theta_i}\\}\\), each predicting \\(\\mathbf{s}_{t+1}\\) given \\(\\mathbf{s}_t, a_t\\).  </li> <li>Each network outputs a mean \\(\\mathbf{\\mu}_i\\) and variance \\(\\mathbf{\\Sigma}_i\\) for \\(\\mathbf{s}_{t+1}\\).  </li> <li>Ensemble Disagreement can signal model uncertainty, guiding more cautious or exploratory planning.</li> </ul> <p>Planning via Trajectory Sampling </p> <ol> <li>At state \\(\\mathbf{s}_0\\), sample multiple candidate action sequences \\(\\{\\mathbf{a}_{0:H}\\}\\).  </li> <li>For each sequence, roll out in all or a subset of the ensemble models:</li> <li> \\[     \\mathbf{s}_{t+1}^{(i)} \\sim \\hat{P}_{\\theta_i}(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_t^{(i)}, a_t). \\] </li> <li> <p>Evaluate cumulative predicted reward \\(\\sum_{t=0}^{H-1} r(\\mathbf{s}_t^{(i)}, a_t)\\).  </p> </li> <li>(Optional) Refine the action distribution using CEM:  <ul> <li>Fit a Gaussian to the top-performing sequences.  </li> <li>Resample from that Gaussian, repeat until convergence.</li> </ul> </li> </ol> <p>Mathematically, the planning objective is:</p> \\[ \\max_{\\{a_0, \\ldots, a_{H-1}\\}} \\;\\; \\mathbb{E}_{\\hat{P}_{\\theta_i}}\\!\\Bigl[\\sum_{t=0}^{H-1} \\gamma^t r(\\mathbf{s}_t, a_t)\\Bigr], \\] <p>where the expectation is approximated by sampling from the ensemble.</p> <p>Significance</p> <ul> <li>Achieved strong sample efficiency on continuous control (HalfCheetah, Ant, etc.), often matching model-free baselines (SAC, PPO) with far fewer environment interactions.  </li> <li>Demonstrated the importance of probabilistic ensembling to avoid catastrophic model exploitation.</li> </ul>"},{"location":"course_notes/model-based/#73-mbpo-janner-et-al-2019","title":"7.3 MBPO (Janner et al., 2019)","text":"<p>Core Idea Model-Based Policy Optimization (MBPO) merges the Dyna-like approach (using a learned model to generate synthetic experience) with a short rollout horizon to control compounding errors. It then trains a model-free RL algorithm (Soft Actor-Critic, SAC) using both real and model-generated data.</p> <p>Algorithmic Steps</p> <ol> <li>Learn an ensemble of dynamics models \\(\\{\\hat{P}_{\\theta_i}\\}\\) from real data.  </li> <li>From each real state \\(\\mathbf{s}\\) in the replay buffer:</li> <li>Sample a short-horizon trajectory (1\u20135 steps) using \\(\\hat{P}_{\\theta_i}\\), with actions from the current policy \\(\\pi_\\phi\\).  </li> <li>Store these \u201cimagined\u201d transitions \\(\\bigl(\\mathbf{s}, a, \\hat{r}, \\mathbf{s}'\\bigr)\\) in the replay buffer.</li> <li>Train SAC on the combined real + model-generated transitions.  </li> <li>Periodically collect more real data with the updated policy, re-fit the model ensemble, and repeat.</li> </ol> <p>Key Equations</p> <ul> <li> <p>The model-based transitions:</p> \\[     \\mathbf{s}_{t+1}^\\text{model} \\sim \\hat{P}_\\theta(\\mathbf{s}_{t+1} \\mid \\mathbf{s}_t, a_t),     \\quad     r_t^\\text{model} \\sim \\hat{R}_\\theta(\\mathbf{s}_t, a_t). \\] </li> <li> <p>The short horizon \\(H_\\text{roll}\\) is chosen to limit error accumulation, e.g. \\(H_\\text{roll} = 1\\) or \\(5\\).</p> </li> </ul> <p>Why Short Rollouts?</p> <ul> <li>Long-horizon imagination can deviate quickly from real states =&gt; inaccurate transitions.  </li> <li>By restricting to a small horizon, MBPO ensures the model is only used in near-realistic states, greatly reducing compounding bias.</li> </ul> <p>Performance</p> <ul> <li>MBPO matches or exceeds the final returns of top model-free algorithms using ~10% of the environment interactions, combining high sample efficiency with strong asymptotic performance.</li> </ul>"},{"location":"course_notes/model-based/#74-dreamer-hafner-et-al-20202023","title":"7.4 Dreamer (Hafner et al., 2020\u20132023)","text":"<p>Core Idea Learn a recurrent latent dynamics model from images, then backprop through multi-step model rollouts to train a policy and value function. Dreamer exemplifies a \u201clearned simulator + actor-critic in latent space.\u201d</p> <p></p> <p>Latent World Model</p> <ol> <li>Encoder \\(e_\\phi(\\mathbf{o}_t)\\) compresses raw observation \\(\\mathbf{o}_t\\) into a latent state \\(\\mathbf{z}_t\\).  </li> <li>Recurrent Transition \\(p_\\theta(\\mathbf{z}_{t+1}\\mid \\mathbf{z}_t, a_t)\\) predicts the next latent, plus a reward model \\(\\hat{r}_\\theta(\\mathbf{z}_t,a_t)\\).  </li> <li>Decoder \\(d_\\phi(\\mathbf{z}_t)\\) (optional) can reconstruct \\(\\mathbf{o}_t\\) for training, but not necessarily used at inference.</li> </ol> <p>Policy Learning in Imagination</p> <ul> <li> <p>An actor \\(\\pi_\\psi(a_t\\mid \\mathbf{z}_t)\\) and critic \\(V_\\psi(\\mathbf{z}_t)\\) are learned by backprop through the latent rollouts:</p> \\[     \\max_{\\psi} \\;\\; \\mathbb{E}_{\\substack{\\mathbf{z}_0 \\sim q(\\mathbf{z}_0|\\mathbf{o}_0) \\\\ a_t \\sim \\pi_\\psi(\\cdot|\\mathbf{z}_t) \\\\ \\mathbf{z}_{t+1} \\sim p_\\theta(\\cdot|\\mathbf{z}_t,a_t)}}\\!\\biggl[\\sum_{t=0}^{H-1} \\gamma^t \\hat{r}_\\theta(\\mathbf{z}_t, a_t)\\biggr]. \\] </li> <li> <p>Dreamer uses advanced techniques (e.g., reparameterization, actor-critic with value expansion, etc.) to stabilize training.</p> </li> </ul> <p>Highlights</p> <ul> <li>DreamerV1: SOTA results on DM Control from image inputs.  </li> <li>DreamerV2: Extended to Atari, surpassing DQN with a single architecture.  </li> <li>DreamerV3: Achieved multi-domain generality (Atari, ProcGen, DM Control, robotics, Minecraft). The first algorithm to solve \u201ccollect a diamond\u201d in Minecraft from scratch without demonstrations.</li> </ul> <p>Significance</p> <ul> <li>Demonstrates that purely learning a latent world model + training by imagination can match or surpass leading model-free methods in terms of both sample efficiency and final returns.</li> </ul>"},{"location":"course_notes/model-based/#75-muzero-deepmind-2020","title":"7.5 MuZero (DeepMind, 2020)","text":"<p>Core Idea Combines Monte Carlo Tree Search (MCTS) with a learned latent state to achieve superhuman performance on Go, Chess, Shogi, and set records on Atari\u2014without knowing the environment\u2019s rules explicitly.</p> <p></p> <p>Network Architecture</p> <ol> <li>Representation Function \\(h\\):  </li> <li>Maps the observation history to a latent state \\(s_0 = h(\\mathbf{o}_{1:t})\\).  </li> <li>Dynamics Function \\(g\\):  </li> <li>Predicts the next latent state \\(s_{k+1}=g(s_k, a_k)\\) and immediate reward \\(r_k\\).  </li> <li>Prediction Function \\(f\\):  </li> <li>From a latent state \\(s_k\\), outputs a policy \\(\\pi_k\\) (action logits) and a value \\(v_k\\).</li> </ol> <p>MCTS in Latent Space</p> <ul> <li>Starting from \\(s_0\\), expand a search tree by simulating actions via \\(g\\).  </li> <li>Each node stores mean value estimates \\(\\hat{V}\\), visit counts, etc.  </li> <li>The final search policy \\(\\alpha\\) is used to update the network (target policy), and the environment reward is used to refine the reward/dynamics parameters.</li> </ul> <p>Key Equations</p> <ul> <li> <p>MuZero is trained to minimize errors in reward, value, and policy predictions:</p> \\[     \\mathcal{L}(\\theta) = \\sum_{t=1}^{T} \\bigl(\\ell_\\mathrm{value}(v_\\theta(s_t), z_t) + \\ell_\\mathrm{policy}(\\pi_\\theta(s_t), \\pi_t) + \\ell_\\mathrm{dyn}\\bigl(g_\\theta(s_t, a_t), s_{t+1}\\bigr)\\bigr), \\] </li> </ul> <p>with \\(\\pi_t\\) and \\(z_t\\) from the improved MCTS-based targets.</p> <p>Achievements</p> <ul> <li>Matches AlphaZero performance on Go, Chess, Shogi, but without an explicit rules model.  </li> <li>Set new records on Atari-57.  </li> <li>Demonstrates that an end-to-end learned model can be as effective for MCTS as a known simulator, provided it is trained to be \u201cvalue-equivalent\u201d (predict future rewards and values accurately).</li> </ul> <p>Impact</p> <ul> <li>Showed that \u201clearning to model the environment\u2019s reward and value structure is enough\u201d\u2014MuZero does not need pixel-perfect next-state reconstructions.  </li> <li>Successfully extended MCTS-based planning to domains with unknown or complex dynamics.</li> </ul>"},{"location":"course_notes/model-based/#8-key-benefits-and-drawbacks-of-mbrl","title":"8. Key Benefits (and Drawbacks) of MBRL","text":""},{"location":"course_notes/model-based/#81-data-efficiency","title":"8.1 Data Efficiency","text":"<p>MBRL can yield higher sample efficiency:</p> <ul> <li>Simulating transitions in the model extracts more learning signal from each real step  </li> <li>E.g., PETS, MBPO, Dreamer require fewer environment interactions than top model-free methods</li> </ul>"},{"location":"course_notes/model-based/#82-exploration","title":"8.2 Exploration","text":"<p>A learned uncertainty-aware model can direct exploration to uncertain states:</p> <ul> <li>Bayesian or ensemble-based MBRL  </li> <li>Potentially more efficient than naive \\(\\epsilon\\)-greedy in high dimensions</li> </ul>"},{"location":"course_notes/model-based/#83-optimality","title":"8.3 Optimality","text":"<p>With a perfect model, MBRL can find better or equal policies vs. model-free. But if the model is imperfect, compounding errors can lead to suboptimal solutions. Research aims to close that gap (MBPO, Dreamer, MuZero).</p>"},{"location":"course_notes/model-based/#84-transfer","title":"8.4 Transfer","text":"<p>A global dynamics model can be re-used across tasks or reward functions:</p> <ul> <li>E.g., a learned robotic physics model can quickly adapt to new goals</li> <li>Saves extensive retraining</li> </ul>"},{"location":"course_notes/model-based/#85-safety","title":"8.5 Safety","text":"<p>In real-world tasks (robotics, healthcare), we can plan or verify constraints inside the model before acting. Uncertainty estimation is crucial.</p>"},{"location":"course_notes/model-based/#86-explainability","title":"8.6 Explainability","text":"<p>A learned model can sometimes be probed or visualized, offering partial interpretability (though deep generative models remain somewhat opaque).</p>"},{"location":"course_notes/model-based/#87-disbenefits","title":"8.7 Disbenefits","text":"<ol> <li>Model bias: Imperfect models =&gt; compounding errors  </li> <li>Computational overhead: Planning can be expensive  </li> <li>Implementation complexity: We must keep models accurate, stable, and do policy updates in tandem</li> </ol>"},{"location":"course_notes/model-based/#9-conclusion","title":"9. Conclusion","text":"<p>Model-Based RL integrates planning and learning in the RL framework, offering strong sample efficiency and structured decision-making. Algorithms like MBPO, Dreamer, and MuZero demonstrate that short rollouts, uncertainty estimates, or latent value-equivalent models can yield high final performance with fewer real samples.</p> <p>Still, challenges remain:</p> <ul> <li>Robustness under partial observability, stochastic transitions, or non-stationary tasks  </li> <li>Balancing planning vs. data collection adaptively  </li> <li>Scaling to high-dimensional, real-world tasks with safety constraints</li> </ul> <p>Future work includes deeper hierarchical methods, advanced uncertainty modeling, bridging theory and practice, and constructing more interpretable or structured models.</p>"},{"location":"course_notes/model-based/#10-references","title":"10. References","text":"<ul> <li> <p>S. Levine (CS 294-112: Deep RL) Model-Based Reinforcement Learning, Lecture 9 slides. Lecture Site, Video Repository Additional resources: Open course materials from UC Berkeley\u2019s Deep RL class </p> </li> <li> <p>T. Moerland et al. (2022) \u201cModel-based Reinforcement Learning: A Survey.\u201d arXiv:2006.16712v4 Additional resources: Official GitHub for references and code snippets mentioned in the paper </p> </li> <li> <p>Sutton, R.S. &amp; Barto, A.G. Reinforcement Learning: An Introduction (2nd edition). MIT Press, 2018. Online Draft Additional resources: Exercise solutions and discussion forum </p> </li> <li> <p>Puterman, M.L. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley &amp; Sons, 2014. Publisher Link Additional resources: Various lecture slides summarizing MDP fundamentals </p> </li> <li> <p>Deisenroth, M. &amp; Rasmussen, C.E. (2011) PILCO: A Model-Based and Data-Efficient Approach to Policy Search. ICML. Paper PDF Additional resources: Official code release on GitHub </p> </li> <li> <p>Chua, K. et al. (2018) \u201cDeep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models (PETS).\u201d NeurIPS. Paper Link Additional resources: Author\u2019s implementation </p> </li> <li> <p>Janner, M. et al. (2019) \u201cWhen to Trust Your Model: Model-Based Policy Optimization.\u201d NeurIPS (MBPO). Paper Link Additional resources: Berkeley AI Research blog post </p> </li> <li> <p>Hafner, D. et al. (2020\u20132023) \u201cDreamer\u201d line of papers (ICML, arXiv). DreamerV2 Code, Dreamer Blog Additional resources: Tutorial videos by Danijar Hafner on latent world models </p> </li> <li> <p>Ha, D. &amp; Schmidhuber, J. (2018) \u201cWorld Models.\u201d NeurIPS. Paper PDF, Project Site Additional resources: Interactive demos and blog articles from David Ha </p> </li> <li> <p>Silver, D. et al. (various) AlphaGo, AlphaZero, MuZero lines of research. DeepMind\u2019s MuZero Blog Additional resources: Further reading on MCTS, AlphaGo, and AlphaZero in \u201cMastering the Game of Go\u201d series </p> </li> </ul>"},{"location":"course_notes/multi-agent/","title":"Multi-Agent RL","text":""},{"location":"course_notes/offline/","title":"Offline RL","text":""},{"location":"course_notes/policy-based/","title":"Week 3: Policy-Based Methods","text":"<p>Reinforcement Learning (RL) focuses on training an agent to interact with an environment by learning a policy \\(\\pi_{\\theta}(a | s)\\) that maximizes the cumulative reward. Policy gradient methods are a class of algorithms that directly optimize the policy by adjusting the parameters \\(\\theta\\) via gradient ascent.</p>"},{"location":"course_notes/policy-based/#why-policy-gradient-methods","title":"Why Policy Gradient Methods?","text":"<p>Unlike value-based methods (e.g., Q-learning), which rely on estimating value functions, policy gradient methods: - Can naturally handle stochastic policies, which are crucial in environments requiring exploration.</p> <ul> <li> <p>Work well in continuous action spaces, where discrete action methods become infeasible.</p> </li> <li> <p>Can directly optimize differentiable policy representations, such as neural networks.</p> </li> <li> <p>Avoid the need for an explicit action-value function approximation, making them more robust in high-dimensional problems.</p> </li> <li> <p>Are capable of optimizing parameterized policies without relying on action selection heuristics.</p> </li> <li> <p>Can incorporate entropy regularization to improve exploration and prevent premature convergence to suboptimal policies.</p> </li> <li> <p>Allow for more stable convergence in some cases compared to value-based methods, which may suffer from instability due to bootstrapping.</p> </li> <li> <p>Can leverage variance reduction techniques (e.g., advantage estimation, baseline subtraction) to improve learning efficiency.</p> </li> </ul>"},{"location":"course_notes/policy-based/#policy-gradient","title":"Policy Gradient","text":"<p>The goal of reinforcement learning is to find an optimal behavior strategy for the agent to obtain optimal rewards. The policy gradient methods target at modeling and optimizing the policy directly. The policy is usually modeled with a parameterized function respect to \\(\\theta\\), \\(\\pi_{\\theta}(a|s)\\). The value of the reward (objective) function depends on this policy and then various algorithms can be applied to optimize \\(\\theta\\) for the best reward.</p> <p>The reward function is defined as:</p> \\[J(\\theta) = \\sum_{s \\in  \\mathcal{S}} d^{\\pi}(s) V^{\\pi}(s) = \\sum_{s \\in  \\mathcal{S}} d^{\\pi}(s) \\sum_{a \\in  \\mathcal{A}} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a)\\] <p>where \\(d^{\\pi}(s)\\) is the stationary distribution of Markov chain for \\(\\pi_{\\theta}\\) (on-policy state distribution under \\(\\pi\\)). For simplicity, the parameter \\(\\theta\\) would be omitted for the policy \\(\\pi_{\\theta}\\) when the policy is present in the subscript of other functions; for example, \\(d^{\\pi}\\) and \\(Q^{\\pi}\\) should be \\(d^{\\pi_{\\theta}}\\) and \\(Q^{\\pi_{\\theta}}\\) if written in full.</p> <p>Imagine that you can travel along the Markov chain's states forever, and eventually, as the time progresses, the probability of you ending up with one state becomes unchanged --- this is the stationary probability for \\(\\pi_{\\theta}\\). \\(d^{\\pi}(s) = \\lim_{t \\to  \\infty} P(s_t = s | s_0, \\pi_{\\theta})\\) is the probability that \\(s_t = s\\) when starting from \\(s_0\\) and following policy \\(\\pi_{\\theta}\\) for \\(t\\) steps. Actually, the existence of the stationary distribution of Markov chain is one main reason for why PageRank algorithm works.</p> <p>It is natural to expect policy-based methods are more useful in the continuous space. Because there is an infinite number of actions and (or) states to estimate the values for and hence value-based approaches are way too expensive computationally in the continuous space. For example, in generalized policy iteration, the policy improvement step \\(\\arg  \\max_{a \\in  \\mathcal{A}} Q^{\\pi}(s,a)\\) requires a full scan of the action space, suffering from the curse of dimensionality.</p> <p>Using gradient ascent, we can move \\(\\theta\\) toward the direction suggested by the gradient \\(\\nabla_{\\theta} J(\\theta)\\) to find the best \\(\\theta\\) for \\(\\pi_{\\theta}\\) that produces the highest return.</p>"},{"location":"course_notes/policy-based/#policy-gradient-theorem","title":"Policy Gradient Theorem","text":"<p>Computing the gradient \\(\\nabla_{\\theta}J(\\theta)\\) is tricky because it depends on both the action selection (directly determined by \\(\\pi_{\\theta}\\)) and the stationary distribution of states following the target selection behavior (indirectly determined by \\(\\pi_{\\theta}\\)). Given that the environment is generally unknown, it is difficult to estimate the effect on the state distribution by a policy update.</p> <p>Luckily, the policy gradient theorem comes to save the world!  It provides a nice reformation of the derivative of the objective function to not involve the derivative of the state distribution \\(d^{\\pi}(\\cdot)\\) and simplify the gradient computation \\(\\nabla_{\\theta}J(\\theta)\\) a lot.</p> \\[\\nabla_{\\theta}J(\\theta) = \\nabla_{\\theta} \\sum_{s \\in  \\mathcal{S}} d^{\\pi}(s) \\sum_{a \\in  \\mathcal{A}} Q^{\\pi}(s,a) \\pi_{\\theta}(a|s)\\] \\[\\propto  \\sum_{s \\in  \\mathcal{S}} d^{\\pi}(s) \\sum_{a \\in  \\mathcal{A}} Q^{\\pi}(s,a) \\nabla_{\\theta} \\pi_{\\theta}(a|s)\\]"},{"location":"course_notes/policy-based/#proof-of-policy-gradient-theorem","title":"Proof of Policy Gradient Theorem","text":"<p>This session is pretty dense, as it is the time for us to go through the proof and figure out why the policy gradient theorem is correct.</p> Warning <p>This proof may be unnecessary for the first phase of the course. </p> proof <p>We first start with the derivative of the state value function:</p> \\[ \\begin{aligned} \\nabla_{\\theta} V^{\\pi}(s) &amp;= \\nabla_{\\theta} \\left( \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) \\right) \\\\ &amp;= \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) + \\pi_{\\theta}(a|s) \\nabla_{\\theta} Q^{\\pi}(s,a) \\right) \\quad \\text{; Derivative product rule.} \\\\ &amp;= \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) + \\pi_{\\theta}(a|s) \\nabla_{\\theta} \\sum_{s', r} P(s',r|s,a) (r + V^{\\pi}(s')) \\right) \\quad \\text{; Extend } Q^{\\pi} \\text{ with future state value.} \\\\ &amp;= \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) + \\pi_{\\theta}(a|s) \\sum_{s',r} P(s',r|s,a) \\nabla_{\\theta} V^{\\pi}(s') \\right) \\\\ &amp;= \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) + \\pi_{\\theta}(a|s) \\sum_{s'} P(s'|s,a) \\nabla_{\\theta} V^{\\pi}(s') \\right) \\quad \\text{; Because } P(s'|s,a) = \\sum_{r} P(s',r|s,a) \\end{aligned} \\] <p>Now we have:</p> \\[ \\begin{aligned} \\nabla_{\\theta} V^{\\pi}(s) &amp;= \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) + \\pi_{\\theta}(a|s) \\sum_{s'} P(s'|s,a) \\nabla_{\\theta} V^{\\pi}(s') \\right) \\end{aligned} \\] <p>This equation has a nice recursive form, and the future state value function \\(V^{\\pi}(s')\\) can be repeatedly unrolled by following the same equation.</p> <p>Let's consider the following visitation sequence and label the probability of transitioning from state \\(s\\) to state \\(x\\) with policy \\(\\pi_{\\theta}\\) after \\(k\\) steps as \\(\\rho^{\\pi}(s \\to x, k)\\).</p> \\[ s \\xrightarrow{a \\sim \\pi_{\\theta}(\\cdot | s)} s' \\xrightarrow{a' \\sim \\pi_{\\theta}(\\cdot | s')} s'' \\xrightarrow{a'' \\sim \\pi_{\\theta}(\\cdot | s'')} \\dots \\] <ul> <li> <p>When \\(k = 0\\): \\(\\rho^{\\pi}(s \\to s, k = 0) = 1\\).</p> </li> <li> <p>When \\(k = 1\\), we scan through all possible actions and sum up the transition probabilities to the target state:</p> </li> </ul> \\[ \\rho^{\\pi}(s \\to s', k = 1) = \\sum_{a} \\pi_{\\theta}(a|s) P(s'|s,a). \\] <ul> <li>Imagine that the goal is to go from state \\(s\\) to \\(x\\) after \\(k+1\\) steps while following policy \\(\\pi_{\\theta}\\). We can first travel from \\(s\\) to a middle point \\(s'\\) (any state can be a middle point, \\(s' \\in S\\)) after \\(k\\) steps and then go to the final state \\(x\\) during the last step. In this way, we are able to update the visitation probability recursively:</li> </ul> \\[ \\rho^{\\pi}(s \\to x, k + 1) = \\sum_{s'} \\rho^{\\pi}(s \\to s', k) \\rho^{\\pi}(s' \\to x, 1). \\] <p>Then we go back to unroll the recursive representation of \\(\\nabla_{\\theta}V^{\\pi}(s)\\). Let</p> \\[ \\phi(s) = \\sum_{a \\in \\mathcal{A}} \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) \\] <p>to simplify the equations. If we keep on extending \\(\\nabla_{\\theta}V^{\\pi}(\\cdot)\\) infinitely, it is easy to find out that we can transition from the starting state \\(s\\) to any state after any number of steps in this unrolling process and by summing up all the visitation probabilities, we get \\(\\nabla_{\\theta}V^{\\pi}(s)\\)!</p> \\[ \\begin{aligned} \\nabla_{\\theta}V^{\\pi}(s) &amp;= \\phi(s) + \\sum_{a} \\pi_{\\theta}(a|s) \\sum_{s'} P(s'|s,a) \\nabla_{\\theta}V^{\\pi}(s') \\\\ &amp;= \\phi(s) + \\sum_{s'} \\sum_{a} \\pi_{\\theta}(a|s) P(s'|s,a) \\nabla_{\\theta}V^{\\pi}(s') \\\\ &amp;= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\nabla_{\\theta}V^{\\pi}(s') \\\\ &amp;= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s') Q^{\\pi}(s',a) + \\pi_{\\theta}(a|s') \\sum_{s''} P(s''|s',a) \\nabla_{\\theta}V^{\\pi}(s'') \\right) \\\\ &amp;= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\left[ \\phi(s') + \\sum_{s''} \\rho^{\\pi}(s' \\to s'', 1) \\nabla_{\\theta}V^{\\pi}(s'') \\right] \\\\ &amp;= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\phi(s') + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\sum_{s''} \\rho^{\\pi}(s' \\to s'', 1) \\nabla_{\\theta}V^{\\pi}(s'') \\\\ &amp;= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\phi(s') + \\sum_{s''} \\rho^{\\pi}(s \\to s'', 2) \\nabla_{\\theta}V^{\\pi}(s'') \\quad \\text{; Consider } s' \\text{ as the middle point for } s \\to s''. \\\\ &amp;= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\phi(s') + \\sum_{s''} \\rho^{\\pi}(s \\to s'', 2) \\phi(s'') + \\sum_{s'''} \\rho^{\\pi}(s \\to s''', 3) \\nabla_{\\theta}V^{\\pi}(s''') \\\\ &amp;= \\dots \\quad \\text{; Repeatedly unrolling the part of } \\nabla_{\\theta}V^{\\pi}(\\cdot) \\\\ &amp;= \\sum_{x \\in \\mathcal{S}} \\sum_{k=0}^{\\infty} \\rho^{\\pi}(s \\to x, k) \\phi(x) \\end{aligned} \\] <p>The nice rewriting above allows us to exclude the derivative of Q-value function, \\(\\nabla_{\\theta} Q^{\\pi}(s,a)\\). By plugging it into the objective function \\(J(\\theta)\\), we are getting the following:</p> \\[ \\begin{aligned} \\nabla_{\\theta}J(\\theta) &amp;= \\nabla_{\\theta}V^{\\pi}(s_0) \\\\ &amp;= \\sum_{s} \\sum_{k=0}^{\\infty} \\rho^{\\pi}(s_0 \\to s, k) \\phi(s) \\quad \\text{; Starting from a random state } s_0 \\\\ &amp;= \\sum_{s} \\eta(s) \\phi(s) \\quad \\text{; Let } \\eta(s) = \\sum_{k=0}^{\\infty} \\rho^{\\pi}(s_0 \\to s, k) \\\\ &amp;= \\left( \\sum_{s} \\eta(s) \\right) \\sum_{s} \\frac{\\eta(s)}{\\sum_{s} \\eta(s)} \\phi(s) \\quad \\text{; Normalize } \\eta(s), s \\in \\mathcal{S} \\text{ to be a probability distribution.} \\\\ &amp;\\propto \\sum_{s} \\frac{\\eta(s)}{\\sum_{s} \\eta(s)} \\phi(s) \\quad \\text{; } \\sum_{s} \\eta(s) \\text{ is a constant} \\\\ &amp;= \\sum_{s} d^{\\pi}(s) \\sum_{a} \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) \\quad d^{\\pi}(s) = \\frac{\\eta(s)}{\\sum_{s} \\eta(s)} \\text{ is stationary distribution.} \\end{aligned} \\] <p>In the episodic case, the constant of proportionality (\\(\\sum_{s} \\eta(s)\\)) is the average length of an episode; in the continuing case, it is 1. The gradient can be further written as:</p> \\[ \\begin{aligned} \\nabla_{\\theta}J(\\theta) &amp;\\propto \\sum_{s \\in \\mathcal{S}} d^{\\pi}(s) \\sum_{a \\in \\mathcal{A}} Q^{\\pi}(s,a) \\nabla_{\\theta} \\pi_{\\theta}(a|s) \\\\ &amp;= \\sum_{s \\in \\mathcal{S}} d^{\\pi}(s) \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) \\frac{\\nabla_{\\theta} \\pi_{\\theta}(a|s)}{\\pi_{\\theta}(a|s)} \\quad \\text{; Because } \\ln(x)'=1/x \\\\ &amp;= \\mathbb{E}_{\\pi} [Q^{\\pi}(s,a) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a|s)] \\end{aligned} \\] <p>Where \\(\\mathbb{E}_{\\pi}\\) refers to \\(\\mathbb{E}_{s \\sim d^{\\pi}, a \\sim \\pi_{\\theta}}\\) when both state and action distributions follow the policy \\(\\pi_{\\theta}\\) (on policy).</p> <p>The policy gradient theorem lays the theoretical foundation for various policy gradient algorithms. This vanilla policy gradient update has no bias but high variance. Many following algorithms were proposed to reduce the variance while keeping the bias unchanged.</p> \\[ \\nabla_{\\theta}J(\\theta) = \\mathbb{E}_{\\pi} [Q^{\\pi}(s,a) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a|s)] \\]"},{"location":"course_notes/policy-based/#policy-gradient-in-continuous-action-space","title":"Policy Gradient in Continuous Action Space","text":"<p>In a continuous action space, the policy gradient theorem is given by:</p> \\[\\nabla_{\\theta}J(\\theta) = \\mathbb{E}_{s \\sim d^{\\pi}, a \\sim  \\pi_{\\theta}} \\left[ Q^{\\pi}(s,a) \\nabla_{\\theta} \\ln  \\pi_{\\theta}(a|s) \\right]\\] <p>Since the action space is continuous, the summation over actions in the discrete case is replaced by an integral:</p> \\[\\nabla_{\\theta} J(\\theta) = \\int_{\\mathcal{S}} d^{\\pi}(s) \\int_{\\mathcal{A}} Q^{\\pi}(s,a) \\nabla_{\\theta} \\ln  \\pi_{\\theta}(a|s) \\pi_{\\theta}(a|s) \\, da \\, ds\\] <p>where:</p> <ul> <li> <p>\\(d^{\\pi}(s)\\) is the stationary state distribution under policy \\(\\pi_{\\theta}\\),</p> </li> <li> <p>\\(\\pi_{\\theta}(a|s)\\) is the probability density function for the continuous action \\(a\\) given state \\(s\\),</p> </li> <li> <p>\\(Q^{\\pi}(s,a)\\) is the state-action value function,</p> </li> <li> <p>\\(\\nabla_{\\theta} \\ln  \\pi_{\\theta}(a|s)\\) is the score function (policy gradient term),</p> </li> <li> <p>The integral is taken over all possible states \\(s\\) and actions \\(a\\).</p> </li> </ul> Gaussian Policy Example <p>A common choice for a continuous policy is a Gaussian distribution:</p> \\[a \\sim  \\pi_{\\theta}(a|s) = \\mathcal{N}(\\mu_{\\theta}(s), \\Sigma_{\\theta}(s))\\] <p>where:</p> <ul> <li> <p>\\(\\mu_{\\theta}(s)\\) is the mean of the action distribution, parameterized by \\(\\theta\\),</p> </li> <li> <p>\\(\\Sigma_{\\theta}(s)\\) is the covariance matrix (often assumed diagonal or fixed).</p> </li> </ul> <p>For a Gaussian policy, the logarithm of the probability density is:</p> \\[\\ln  \\pi_{\\theta}(a|s) = -\\frac{1}{2} (a - \\mu_{\\theta}(s))^T \\Sigma_{\\theta}^{-1} (a - \\mu_{\\theta}(s)) - \\frac{1}{2} \\ln |\\Sigma_{\\theta}|\\] <p>Taking the gradient:</p> \\[\\nabla_{\\theta} \\ln  \\pi_{\\theta}(a|s) = \\Sigma_{\\theta}^{-1} (a - \\mu_{\\theta}(s)) \\nabla_{\\theta} \\mu_{\\theta}(s)\\] <p>Thus, the policy gradient update becomes:</p> \\[\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{s \\sim d^{\\pi}, a \\sim  \\pi_{\\theta}} \\left[ Q^{\\pi}(s,a) \\Sigma_{\\theta}^{-1} (a - \\mu_{\\theta}(s)) \\nabla_{\\theta} \\mu_{\\theta}(s) \\right]\\]"},{"location":"course_notes/policy-based/#reinforce","title":"REINFORCE","text":"<p>REINFORCE (Monte-Carlo policy gradient) relies on an estimated return by Monte-Carlo methods using episode samples to update the policy parameter \\(\\theta\\). REINFORCE works because the expectation of the sample gradient is equal to the actual gradient:</p> \\[ \\begin{aligned} \\nabla_{\\theta}J(\\theta) &amp;= \\mathbb{E}_{\\pi} \\left[ Q^{\\pi}(s,a) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a|s) \\right] \\\\ &amp;= \\mathbb{E}_{\\pi} \\left[ G_t \\nabla_{\\theta} \\ln \\pi_{\\theta}(A_t|S_t) \\right] \\quad \\text{; Because } Q^{\\pi}(S_t, A_t) = \\mathbb{E}_{\\pi} \\left[ G_t \\mid S_t, A_t \\right] \\end{aligned} \\] <p>where \\(G_t = \\sum_{k=0}^{\\infty} \\gamma^kR_{t+k+1}\\) is the discounted future reward starting from time setp \\(t\\).</p> <p>Therefore we are able to measure \\(G_t\\) from real sample trajectories and use that to update our policy gradient. It relies on a full trajectory and that's why it is a Monte-Carlo method.</p>"},{"location":"course_notes/policy-based/#algorithm","title":"Algorithm","text":"<p>The process is pretty straightforward:</p> <ol> <li> <p>Initialize the policy parameter \\(\\theta\\) at random.</p> </li> <li> <p>Generate one trajectory on policy \\(\\pi_{\\theta}\\): \\(S_1, A_1, R_2, S_2, A_2, \\dots, S_T\\).</p> </li> <li> <p>For \\(t = 1, 2, \\dots, T\\):</p> <ol> <li> <p>Estimate the return \\(G_t\\);</p> </li> <li> <p>Update policy parameters: \\(\\theta  \\leftarrow  \\theta + \\alpha  \\gamma^t G_t \\nabla_{\\theta} \\ln  \\pi_{\\theta}(A_t|S_t)\\)</p> </li> </ol> </li> </ol> <p>A widely used variation of REINFORCE is to subtract a baseline value from the return \\(G_t\\) to reduce the variance of gradient estimation while keeping the bias unchanged (Remember we always want to do this when possible).</p> <p>For example, a common baseline is to subtract state-value from action-value, and if applied, we would use advantage \\(A(s,a) = Q(s,a) - V(s)\\) in the gradient ascent update. This post nicely explained why a baseline works for reducing the variance, in addition to a set of fundamentals of policy gradient.</p>"},{"location":"course_notes/policy-based/#bias-and-variance","title":"Bias and Variance","text":"<p>As introduced in previous sections, REINFORCE employs Monte Carlo estimation of returns. Recall that Monte Carlo methods estimate expected values by sampling trajectories from the environment. While these estimators are unbiased (they converge to the true expected value given enough samples), they often suffer from high variance, making policy gradient methods challenging to stabilize.</p> <p>In this section, we delve deeper into the bias-variance tradeoff in reinforcement learning, with a focus on policy gradient methods. While these concepts were mentioned earlier, we now analyze them as the central topic:</p> <p>Bias occurs when our gradient estimates systematically deviate from the true expected gradient, leading to suboptimal policy updates.</p> <p>Variance measures how much our gradient estimates fluctuate across different batches of samples, affecting training stability.</p> <p>Policy gradient methods face unique challenges:</p> <ul> <li> <p>High variance from Monte Carlo sampling of full trajectories.</p> </li> <li> <p>Potential bias from function approximation (e.g., neural networks) or improper baselines.</p> </li> </ul> <p>Below, we formalize these concepts and explore techniques to mitigate their effects.</p>"},{"location":"course_notes/policy-based/#monte-carlo-estimators-in-reinforcement-learning","title":"Monte Carlo Estimators in Reinforcement Learning","text":"<p>A Monte Carlo estimator is a method used to approximate the expected value of a function \\(f(X)\\) over a random variable \\(X\\) with a given probability distribution \\(p(X)\\). The true expectation is:</p> \\[E[f(X)] = \\int f(x) p(x) \\, dx\\] <p>However, directly computing this integral may be complex. Instead, we use Monte Carlo estimation by drawing \\(N\\) independent samples \\(X_1, X_2, \\dots, X_N\\) from \\(p(X)\\) and computing:</p> \\[\\hat{\\mu}_{MC} = \\frac{1}{N} \\sum_{i=1}^{N} f(X_i)\\] <p>This estimator provides an approximation to the true expectation \\(E[f(X)]\\).</p> <p>By the law of large numbers (LLN), as \\(N \\to  \\infty\\), we have:</p> \\[\\hat{X}_N \\to  \\mathbb{E}[X] \\quad  \\text{(almost surely)}\\] <p>Monte Carlo methods are commonly used in RL for estimating expected rewards, state-value functions, and action-value functions.</p>"},{"location":"course_notes/policy-based/#bias-in-policy-gradient-methods","title":"Bias in Policy Gradient Methods","text":"<p>Bias in reinforcement learning arises when an estimator systematically deviates from the true value. In policy gradient methods, bias is introduced due to function approximation, reward estimation, or gradient computation errors.</p>"},{"location":"course_notes/policy-based/#sources-of-bias","title":"Sources of Bias","text":"<ul> <li> <p>Function Approximation Bias: Policy gradient methods often rely on neural networks or other function approximators for policy representation. Imperfect approximations introduce systematic errors, leading to biased policy updates.</p> </li> <li> <p>Reward Clipping or Discounting: Algorithms using reward clipping or high discount factors (\\(\\gamma\\)) can distort return estimates, causing the learned policy to be biased toward short-term rewards.</p> </li> <li> <p>Baseline Approximation: Variance reduction techniques like baseline subtraction use estimates of expected returns. If the baseline is inaccurately estimated, it introduces bias in the policy gradient computation.</p> </li> </ul> Example of Bias <p>Consider a self-driving car optimizing for fuel efficiency. If the reward function prioritizes immediate fuel consumption over long-term efficiency, the learned policy may favor suboptimal strategies that minimize fuel use in the short term while missing globally optimal driving behaviors.</p>"},{"location":"course_notes/policy-based/#biased-vs-unbiased-estimation","title":"Biased vs. Unbiased Estimation","text":"<p>For example: The biased formula for the sample variance \\(S^2\\) is given by:</p> \\[S^2_{\\text{biased}} = \\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\overline{X})^2\\] <p>This is an underestimation of the true population variance \\(\\sigma^2\\) because it does not account for the degrees of freedom in estimation.</p> <p>Instead, the unbiased estimator is:</p> \\[S^2_{\\text{unbiased}} = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\overline{X})^2.\\] <p>This unbiased estimator correctly accounts for variance in small sample sizes, ensuring \\(\\mathbb{E}[S^2_{\\text{unbiased}}] = \\sigma^2\\).</p>"},{"location":"course_notes/policy-based/#variance-in-policy-gradient-methods","title":"Variance in Policy Gradient Methods","text":"<p>Variance in policy gradient estimates refers to fluctuations in gradient estimates across different training episodes. High variance leads to instability and slow convergence.</p>"},{"location":"course_notes/policy-based/#sources-of-variance","title":"Sources of Variance","text":"<ul> <li> <p>Monte Carlo Estimation: REINFORCE estimates gradients using complete episodes, leading to high variance due to trajectory randomness.</p> </li> <li> <p>Stochastic Policy Outputs: Policies represented as probability distributions (e.g., Gaussian policies) introduce additional randomness in gradient updates.</p> </li> <li> <p>Exploration Strategies: Methods like softmax or epsilon-greedy increase variance by adding stochasticity to action selection.</p> </li> </ul> Example of Variance <p>Consider a robotic arm learning to grasp objects. Due to high variance, in some episodes, it succeeds, while in others, minor variations cause failure. These inconsistencies slow down convergence.</p>"},{"location":"course_notes/policy-based/#techniques-to-reduce-variance-in-policy-gradient-methods","title":"Techniques to Reduce Variance in Policy Gradient Methods","text":"<p>Several strategies help mitigate variance in policy gradient methods while preserving unbiased gradient estimates.</p>"},{"location":"course_notes/policy-based/#baseline-subtraction","title":"Baseline Subtraction","text":"<p>A baseline function \\(b\\) reduces variance without introducing bias:</p> \\[\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}} \\left[ \\nabla_{\\theta} \\log  \\pi_{\\theta}(a_t | s_t) (G_t - b) \\right].\\] <p>A common choice for \\(b\\) is the average return over trajectories:</p> \\[b = \\frac{1}{N} \\sum_{i=1}^{N} G_i.\\] <p>Since \\(b\\) is independent of actions, it does not introduce bias in the gradient estimate while reducing variance. A simple proof for this is illustrated bleow.</p> proof \\[\\begin{aligned} E\\left[\\nabla_\\theta  \\log p_\\theta(\\tau) b\\right] &amp;= \\int p_\\theta(\\tau) \\nabla_\\theta  \\log p_\\theta(\\tau) b \\, d\\tau \\\\ &amp;= \\int  \\nabla_\\theta p_\\theta(\\tau) b \\, d\\tau \\\\ &amp;= b \\nabla_\\theta  \\int p_\\theta(\\tau) \\, d\\tau \\\\ &amp;= b \\nabla_\\theta  1 \\\\ &amp;= 0 \\end{aligned}\\]"},{"location":"course_notes/policy-based/#causality-trick-and-reward-to-go-estimation","title":"Causality Trick and Reward-to-Go Estimation","text":"<p>To ensure that policy updates at time \\(t\\) are only influenced by rewards from that time step onward, we use the causality trick:</p> \\[\\nabla_{\\theta} J(\\theta) \\approx  \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log  \\pi_{\\theta}(a_{i,t} | s_{i,t}) \\left( \\sum_{t'=t}^{T} r(a_{i,t'}, s_{i,t'}) \\right).\\] <p>Instead of summing over all rewards, the reward-to-go estimate restricts the sum to future rewards only:</p> \\[Q(s_t, a_t) = \\sum_{t'=t}^{T} \\mathbb{E}_{\\pi_{\\theta}} [r(s_{t'}, a_{t'}) | s_t, a_t].\\] \\[\\nabla_{\\theta} J(\\theta) \\approx  \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log  \\pi_{\\theta}(a_{i,t} | s_{i,t}) Q(s_{i,t}, a_{i,t}).\\] <p>This prevents rewards from future time steps from affecting past actions, reducing variance. This approach results in much lower variance compared to the traditional Monte Carlo methods.</p> proof \\[ \\begin{aligned} A_{t_0-1} &amp;= s_{t_0-1}, a_{t_0-1}, \\dots, a_0, s_0 \\\\ \\mathbb{E}_{A_{t_0-1}} &amp;\\left[ \\mathbb{E}_{s_{t_0}, a_{t_0} | A_{t_0-1}} \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) \\sum_{t=0}^{t_0 - 1} r(s_t, a_t) \\right] \\right] \\\\ U_{t_0-1} &amp;= \\sum_{t=0}^{t_0 - 1} r(s_t, a_t) \\\\ &amp;= \\mathbb{E}_{A_{t_0-1}} \\left[ U_{t_0-1} \\mathbb{E}_{s_{t_0}, a_{t_0} | s_{t_0-1}, a_{t_0-1}} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) \\right] \\\\ &amp;= \\mathbb{E}_{A_{t_0-1}} \\left[ U_{t_0-1} \\mathbb{E}_{s_{t_0} | s_{t_0-1}, a_{t_0-1}} \\mathbb{E}_{a_{t_0} | s_{t_0-1}, a_{t_0-1}, s_{t_0}} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) \\right] \\\\ &amp;= \\mathbb{E}_{A_{t_0-1}} \\left[ U_{t_0-1} \\mathbb{E}_{s_{t_0} | s_{t_0-1}, a_{t_0-1}} \\mathbb{E}_{a_{t_0} | s_{t_0}} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) \\right] \\\\ &amp;= \\mathbb{E}_{A_{t_0-1}} \\left[ U_{t_0-1} \\mathbb{E}_{s_{t_0} | s_{t_0-1}, a_{t_0-1}} \\mathbb{E}_{\\pi_{\\theta} (a_{t_0} | s_{t_0})} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) \\right] \\\\ \\mathbb{E}_{\\pi_{\\theta} (a_{t_0} | s_{t_0})} &amp;\\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) = 0 \\\\ \\mathbb{E}_{A_{t_0-1}}&amp; \\left[ \\mathbb{E}_{s_{t_0}, a_{t_0} | A_{t_0-1}} \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) \\sum_{t=0}^{t_0 - 1} r(s_t, a_t) \\right] \\right] = 0 \\end{aligned} \\]"},{"location":"course_notes/policy-based/#discount-factor-adjustment","title":"Discount Factor Adjustment","text":"<p>The discount factor \\(\\gamma\\) helps reduce variance by weighting rewards closer to the present more heavily:</p> \\[G_t = \\sum_{t' = t}^{T} \\gamma^{t'-t} r(s_{t'}, a_{t'}).\\] proof \\[ \\begin{aligned} \\nabla_{\\theta} J(\\theta) &amp;\\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{i,t} | s_{i,t}) \\left( \\sum_{t' = t}^{T} \\gamma^{t' - t} r(s_{i,t'}, a_{i,t'}) \\right) \\\\ \\nabla_{\\theta} J(\\theta) &amp;\\approx \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{i,t} | s_{i,t}) \\right) \\left( \\sum_{t=1}^{T} \\gamma^{t-1} r(s_{i,t}, a_{i,t}) \\right) \\\\ \\nabla_{\\theta} J(\\theta) &amp;\\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{i,t} | s_{i,t}) \\left( \\sum_{t' = t}^{T} \\gamma^{t' - t} r(s_{i,t'}, a_{i,t'}) \\right) \\\\ \\nabla_{\\theta} J(\\theta) &amp;\\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\gamma^{t-1} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{i,t} | s_{i,t}) \\left( \\sum_{t' = t}^{T} \\gamma^{t' - t} r(s_{i,t'}, a_{i,t'}) \\right) \\end{aligned} \\] <p>A lower \\(\\gamma\\) (e.g., 0.9) reduces variance but increases bias, while a higher \\(\\gamma\\) (e.g., 0.99) improves long-term estimation but increases variance. A balance is needed.</p>"},{"location":"course_notes/policy-based/#advantage-estimation-and-actor-critic-methods","title":"Advantage Estimation and Actor-Critic Methods","text":"<p>Actor-critic methods combine policy optimization (actor) with value function estimation (critic). The advantage function is defined as:</p> \\[A^{\\pi}(s_t, a_t) = Q^{\\pi}(s_t, a_t) - V^{\\pi}(s_t),\\] <p>where the action-value function is:</p> \\[Q^{\\pi}(s_t, a_t) = \\sum_{t' = t}^{T} \\mathbb{E}_{\\pi} [r(s_{t'}, a_{t'}) | s_t, a_t],\\] <p>and the state-value function is:</p> \\[V^{\\pi}(s_t) = \\mathbb{E}_{a_t \\sim  \\pi_{\\theta}(a_t | s_t)} [Q^{\\pi}(s_t, a_t)].\\] <p>The policy gradient update using the advantage function becomes:</p> \\[\\nabla_{\\theta} J(\\theta) \\approx  \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log  \\pi_{\\theta}(a_{i,t} | s_{i,t}) A^{\\pi}(s_{i,t}, a_{i,t}).\\] <p>This formulation allows for lower variance in policy updates while leveraging learned state-value estimates. Actor-critic methods are widely used in modern reinforcement learning due to their stability and efficiency.</p>"},{"location":"course_notes/policy-based/#actor-critic","title":"Actor-Critic","text":"<p>Two main components in policy gradient methods are the policy model and the value function. It makes a lot of sense to learn the value function in addition to the policy since knowing the value function can assist the policy update, such as by reducing gradient variance in vanilla policy gradients. That is exactly what the Actor-Critic method does.</p> <p>Actor-Critic methods consist of two models, which may optionally share parameters:</p> <ul> <li> <p>Critic: Updates the value function parameters \\(w\\). Depending on the algorithm, it could be an action-value function \\(Q(s, a)\\) or a state-value function \\(V(s)\\).</p> </li> <li> <p>Actor: Updates the policy parameters \\(\\theta\\) for \\(\\pi_{\\theta}(a | s)\\), in the direction suggested by the critic.</p> </li> </ul> <p>Let's see how it works in a simple action-value Actor-Critic algorithm:</p> <ol> <li> <p>Initialize policy parameters \\(\\theta\\) and value function parameters \\(w\\) at random.</p> </li> <li> <p>Sample initial state \\(s_0\\).</p> </li> <li> <p>For each time step \\(t\\):</p> <ol> <li> <p>Sample reward \\(r_t\\) and next state \\(s_{t+1}\\).</p> </li> <li> <p>Then sample the next action \\(a_{t+1}\\) from policy: \\(\\pi_{\\theta}(s_{t+1})\\)</p> </li> <li> <p>Update the policy parameters: \\(\\theta  \\leftarrow  \\theta + \\alpha  \\nabla_{\\theta} \\log  \\pi_{\\theta}(a_t | s_t) Q(s_t, a_t)\\)</p> </li> </ol> </li> <li> <p>Compute the correction (TD error) for action-value at time \\(t\\): \\(\\delta_t = r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q_w(s_t, a_t)\\)</p> </li> <li> <p>Compute MSE loss : \\(\\mathcal{L}(w) = \\frac{1}{2} \\mathbb{E}_{(s_t, a_t, r_t, s_{t+1}) \\sim \\pi_\\theta} \\left[ \\delta_t^2 \\right]\\)</p> </li> <li> <p>Use it to update the parameters of the action-value function : \\(w \\leftarrow w + \\beta  \\delta_t  \\nabla_w Q_w(s_t, a_t)\\)</p> </li> <li> <p>Update \\(\\theta\\) and \\(w\\).</p> </li> </ol> <p>Two learning rates, \\(\\alpha\\) and \\(\\beta\\), are predefined for policy and value function parameter updates, respectively. Also note that the \\(Q(s_{t+1}, a_{t+1})\\) in the TD error uses the freezed values of \\(w\\) for better stablity.</p> Actor-Critic Architecture: Cartpole Example <p>Let's illustrate the Actor-Critic architecture with an example of a classic reinforcement learning problem: the Cartpole environment.</p> <p> </p> <p>In the Cartpole environment, the agent controls a cart that can move horizontally on a track. A pole is attached to the cart, and the agent's task is to balance the pole upright for as long as possible.</p> <ol> <li> <p>Actor (Policy-Based): The actor is responsible for learning the policy, which is the agent's strategy for selecting actions (left or right) based on the observed state (cart position, cart velocity, pole angle, and pole angular velocity).</p> </li> <li> <p>Critic (Value-Based): The critic is responsible for learning the value function, which estimates the expected total reward (return) from each state. The value function helps evaluate how good or bad a specific state is, which guides the actor's updates.</p> </li> <li> <p>Policy Representation: For simplicity, let's use a neural network as the actor. The neural network takes the current state of the cart and pole as input and outputs the probabilities of selecting actions (left or right).</p> </li> <li> <p>Value Function Representation: For the critic, we also use a neural network. The neural network takes the current state as input and outputs an estimate of the expected total reward (value) for that state.</p> </li> <li> <p>Collecting Experiences: The agent interacts with the environment, using the current policy to select actions (left or right). As it moves through the environment, it collects experiences, including states, actions, rewards, and next states.</p> </li> <li> <p>Updating the Critic (Value Function): The critic learns to estimate the value function using the collected experiences. It optimizes its neural network parameters to minimize the difference between the predicted values and the actual rewards experienced by the agent.</p> </li> <li> <p>Calculating the Advantage: The advantage represents how much better or worse an action is compared to the average expected value. It is calculated as the difference between the total return (reward) and the value function estimate for each state-action pair.</p> </li> <li> <p>Updating the Actor (Policy): The actor updates its policy to increase the probabilities of actions with higher advantages and decrease the probabilities of actions with lower advantages. This process helps the actor learn from the critic's feedback and improve its policy to maximize the expected rewards.</p> </li> <li> <p>Iteration and Learning: The learning process is repeated over multiple episodes and iterations. As the agent explores and interacts with the environment, the actor and critic networks gradually improve their performance and converge to better policies and value function estimates.</p> </li> </ol> <p>Through these steps, the Actor-Critic architecture teaches the agent how to balance the pole effectively in the Cartpole environment. The actor learns the best actions to take in different states, while the critic provides feedback on the quality of the actor's decisions. As a result, the agent converges to a more optimal policy, achieving longer balancing times and better performance in the task.</p>"},{"location":"course_notes/policy-based/#summary-of-variance-reduction-methods","title":"Summary of Variance Reduction Methods","text":"<p>To summarize, the key methods for reducing variance in policy gradient methods include:</p> <ul> <li> <p>Baseline Subtraction: Subtracting an average return baseline to reduce variance while keeping gradients unbiased.</p> </li> <li> <p>Causality Trick and Reward-to-Go: Using future rewards from time step \\(t\\) onward to prevent variance from irrelevant past rewards.</p> </li> <li> <p>Discount Factor Adjustment: Adjusting \\(\\gamma\\) to balance variance reduction and long-term reward optimization.</p> </li> <li> <p>Advantage Estimation: Using the advantage function \\(A(s_t, a_t)\\) instead of raw returns to stabilize learning.</p> </li> <li> <p>Actor-Critic Methods: Combining policy gradient updates with value function estimation to create more stable and efficient training.</p> </li> </ul> <p>By employing these techniques, policy gradient methods can achieve more stable and efficient learning with reduced variance.</p>"},{"location":"course_notes/policy-based/#concluding-remarks","title":"Concluding Remarks","text":"<p>Now that we have seen the principles behind a policy-based algorithm, let us see how policy-based algorithms work in practice, and compare advantages and disadvantages of the policy-based approach.</p> <p>Let us start with the advantages. First of all, parameterization is at the core of policy-based methods, making them a good match for deep learning. For value- based methods, deep learning had to be retrofitted, giving rise to complications. Second, policy-based methods can easily find stochastic policies, whereas value- based methods find deterministic policies. Due to their stochastic nature, policy- based methods naturally explore, without the need for methods such as \\(\\epsilon\\)-greedy, or more involved methods that may require tuning to work well. Third, policy-based methods are effective in large or continuous action spaces. Small changes in \\(\\theta\\) lead to small changes in \\(\\pi\\), and to small changes in state distributions (they are smooth). Policy-based algorithms do not suffer (as much) from convergence and stability issues that are seen in \\(\\arg\\max\\)-based algorithms in large or continuous action spaces.</p> <p>On the other hand, there are disadvantages to the episodic Monte Carlo version of the REINFORCE algorithm. Remember that REINFORCE generates a full random episode in each iteration before it assesses the quality. (Value-based methods use a reward to select the next action in each time step of the episode.) Because of this, policy-based methods exhibit low bias since full random trajectories are generated. However, they are also high variance, since the full trajectory is generated randomly, whereas value-based methods use the value for guidance at each selection step.</p> <p>What are the consequences? First, policy evaluation of full trajectories has low sample efficiency and high variance. As a consequence, policy improvement happens infrequently, leading to slow convergence compared to value-based methods. Second, this approach often finds a local optimum, since convergence to the global optimum takes too long.</p> <p>Much research has been performed to address the high variance of the episode- based vanilla policy gradient. The enhancements that have been found have greatly improved performance, so much so that policy-based approaches---such as A3C, PPO, SAC, and DDPG---have become favorite model-free reinforcement learning algorithms for many applications.</p>"},{"location":"course_notes/policy-based/#authors","title":"Author(s)","text":"<ul> <li> <p>Nima Shirzady</p> <p>Teaching Assistant</p> <p>shirzady.1934@gmail.com</p> <p> </p> </li> <li> <p>SeyyedAli MirGhasemi</p> <p>Teaching Assistant</p> <p>sam717269@gmail.com</p> <p> </p> </li> <li> <p>Hesam Hosseini</p> <p>Teaching Assistant</p> <p>hesam138122@gmail.com</p> <p> </p> </li> </ul>"},{"location":"course_notes/policy-based/#references","title":"References","text":"<ol> <li> <p>Reinforcement Learning Explained</p> </li> <li> <p>An Introduction to Deep Reinforcement Learning</p> </li> <li> <p>Deep Reinforcement Learning Processor Design for Mobile Applications</p> </li> <li> <p>REINFORCE \u2014 a policy-gradient based reinforcement Learning algorithm</p> </li> <li> <p>Policy Gradient Algorithms</p> </li> <li> <p>Deep Reinforcement Learning</p> </li> <li> <p>Reinforcement Learning (BartoSutton)</p> </li> </ol>"},{"location":"course_notes/policy-based2/","title":"Week 8: Policy-Based Methods","text":"<p>Reinforcement Learning (RL) focuses on training an agent to interact with an environment by learning a policy \\(\\pi_{\\theta}(a | s)\\) that maximizes the cumulative reward. Policy gradient methods are a class of algorithms that directly optimize the policy by adjusting the parameters \\(\\theta\\) via gradient ascent.</p>"},{"location":"course_notes/policy-based2/#why-policy-gradient-methods","title":"Why Policy Gradient Methods?","text":"<p>Unlike value-based methods (e.g., Q-learning), which rely on estimating value functions, policy gradient methods: - Can naturally handle stochastic policies, which are crucial in environments requiring exploration.</p> <ul> <li> <p>Work well in continuous action spaces, where discrete action methods become infeasible.</p> </li> <li> <p>Can directly optimize differentiable policy representations, such as neural networks.</p> </li> <li> <p>Avoid the need for an explicit action-value function approximation, making them more robust in high-dimensional problems.</p> </li> <li> <p>Are capable of optimizing parameterized policies without relying on action selection heuristics.</p> </li> <li> <p>Can incorporate entropy regularization to improve exploration and prevent premature convergence to suboptimal policies.</p> </li> <li> <p>Allow for more stable convergence in some cases compared to value-based methods, which may suffer from instability due to bootstrapping.</p> </li> <li> <p>Can leverage variance reduction techniques (e.g., advantage estimation, baseline subtraction) to improve learning efficiency.</p> </li> </ul>"},{"location":"course_notes/policy-based2/#policy-gradient","title":"Policy Gradient","text":"<p>The goal of reinforcement learning is to find an optimal behavior strategy for the agent to obtain optimal rewards. The policy gradient methods target at modeling and optimizing the policy directly. The policy is usually modeled with a parameterized function respect to \\(\\theta\\), \\(\\pi_{\\theta}(a|s)\\). The value of the reward (objective) function depends on this policy and then various algorithms can be applied to optimize \\(\\theta\\) for the best reward.</p> <p>The reward function is defined as:</p> \\[J(\\theta) = \\sum_{s \\in  \\mathcal{S}} d^{\\pi}(s) V^{\\pi}(s) = \\sum_{s \\in  \\mathcal{S}} d^{\\pi}(s) \\sum_{a \\in  \\mathcal{A}} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a)\\] <p>where \\(d^{\\pi}(s)\\) is the stationary distribution of Markov chain for \\(\\pi_{\\theta}\\) (on-policy state distribution under \\(\\pi\\)). For simplicity, the parameter \\(\\theta\\) would be omitted for the policy \\(\\pi_{\\theta}\\) when the policy is present in the subscript of other functions; for example, \\(d^{\\pi}\\) and \\(Q^{\\pi}\\) should be \\(d^{\\pi_{\\theta}}\\) and \\(Q^{\\pi_{\\theta}}\\) if written in full.</p> <p>Imagine that you can travel along the Markov chain's states forever, and eventually, as the time progresses, the probability of you ending up with one state becomes unchanged --- this is the stationary probability for \\(\\pi_{\\theta}\\). \\(d^{\\pi}(s) = \\lim_{t \\to  \\infty} P(s_t = s | s_0, \\pi_{\\theta})\\) is the probability that \\(s_t = s\\) when starting from \\(s_0\\) and following policy \\(\\pi_{\\theta}\\) for \\(t\\) steps. Actually, the existence of the stationary distribution of Markov chain is one main reason for why PageRank algorithm works.</p> <p>It is natural to expect policy-based methods are more useful in the continuous space. Because there is an infinite number of actions and (or) states to estimate the values for and hence value-based approaches are way too expensive computationally in the continuous space. For example, in generalized policy iteration, the policy improvement step \\(\\arg  \\max_{a \\in  \\mathcal{A}} Q^{\\pi}(s,a)\\) requires a full scan of the action space, suffering from the curse of dimensionality.</p> <p>Using gradient ascent, we can move \\(\\theta\\) toward the direction suggested by the gradient \\(\\nabla_{\\theta} J(\\theta)\\) to find the best \\(\\theta\\) for \\(\\pi_{\\theta}\\) that produces the highest return.</p>"},{"location":"course_notes/policy-based2/#policy-gradient-theorem","title":"Policy Gradient Theorem","text":"<p>Computing the gradient \\(\\nabla_{\\theta}J(\\theta)\\) is tricky because it depends on both the action selection (directly determined by \\(\\pi_{\\theta}\\)) and the stationary distribution of states following the target selection behavior (indirectly determined by \\(\\pi_{\\theta}\\)). Given that the environment is generally unknown, it is difficult to estimate the effect on the state distribution by a policy update.</p> <p>Luckily, the policy gradient theorem comes to save the world!  It provides a nice reformation of the derivative of the objective function to not involve the derivative of the state distribution \\(d^{\\pi}(\\cdot)\\) and simplify the gradient computation \\(\\nabla_{\\theta}J(\\theta)\\) a lot.</p> \\[\\nabla_{\\theta}J(\\theta) = \\nabla_{\\theta} \\sum_{s \\in  \\mathcal{S}} d^{\\pi}(s) \\sum_{a \\in  \\mathcal{A}} Q^{\\pi}(s,a) \\pi_{\\theta}(a|s)\\] \\[\\propto  \\sum_{s \\in  \\mathcal{S}} d^{\\pi}(s) \\sum_{a \\in  \\mathcal{A}} Q^{\\pi}(s,a) \\nabla_{\\theta} \\pi_{\\theta}(a|s)\\]"},{"location":"course_notes/policy-based2/#proof-of-policy-gradient-theorem","title":"Proof of Policy Gradient Theorem","text":"<p>This session is pretty dense, as it is the time for us to go through the proof and figure out why the policy gradient theorem is correct.</p> Warning <p>This proof may be unnecessary for the first phase of the course. </p> proof <p>We first start with the derivative of the state value function:</p> \\[ \\begin{aligned} \\nabla_{\\theta} V^{\\pi}(s) &amp;= \\nabla_{\\theta} \\left( \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) \\right) \\\\ &amp;= \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) + \\pi_{\\theta}(a|s) \\nabla_{\\theta} Q^{\\pi}(s,a) \\right) \\quad \\text{; Derivative product rule.} \\\\ &amp;= \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) + \\pi_{\\theta}(a|s) \\nabla_{\\theta} \\sum_{s', r} P(s',r|s,a) (r + V^{\\pi}(s')) \\right) \\quad \\text{; Extend } Q^{\\pi} \\text{ with future state value.} \\\\ &amp;= \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) + \\pi_{\\theta}(a|s) \\sum_{s',r} P(s',r|s,a) \\nabla_{\\theta} V^{\\pi}(s') \\right) \\\\ &amp;= \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) + \\pi_{\\theta}(a|s) \\sum_{s'} P(s'|s,a) \\nabla_{\\theta} V^{\\pi}(s') \\right) \\quad \\text{; Because } P(s'|s,a) = \\sum_{r} P(s',r|s,a) \\end{aligned} \\] <p>Now we have:</p> \\[ \\begin{aligned} \\nabla_{\\theta} V^{\\pi}(s) &amp;= \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) + \\pi_{\\theta}(a|s) \\sum_{s'} P(s'|s,a) \\nabla_{\\theta} V^{\\pi}(s') \\right) \\end{aligned} \\] <p>This equation has a nice recursive form, and the future state value function \\(V^{\\pi}(s')\\) can be repeatedly unrolled by following the same equation.</p> <p>Let's consider the following visitation sequence and label the probability of transitioning from state \\(s\\) to state \\(x\\) with policy \\(\\pi_{\\theta}\\) after \\(k\\) steps as \\(\\rho^{\\pi}(s \\to x, k)\\).</p> \\[ s \\xrightarrow{a \\sim \\pi_{\\theta}(\\cdot | s)} s' \\xrightarrow{a' \\sim \\pi_{\\theta}(\\cdot | s')} s'' \\xrightarrow{a'' \\sim \\pi_{\\theta}(\\cdot | s'')} \\dots \\] <ul> <li> <p>When \\(k = 0\\): \\(\\rho^{\\pi}(s \\to s, k = 0) = 1\\).</p> </li> <li> <p>When \\(k = 1\\), we scan through all possible actions and sum up the transition probabilities to the target state:</p> </li> </ul> \\[ \\rho^{\\pi}(s \\to s', k = 1) = \\sum_{a} \\pi_{\\theta}(a|s) P(s'|s,a). \\] <ul> <li>Imagine that the goal is to go from state \\(s\\) to \\(x\\) after \\(k+1\\) steps while following policy \\(\\pi_{\\theta}\\). We can first travel from \\(s\\) to a middle point \\(s'\\) (any state can be a middle point, \\(s' \\in S\\)) after \\(k\\) steps and then go to the final state \\(x\\) during the last step. In this way, we are able to update the visitation probability recursively:</li> </ul> \\[ \\rho^{\\pi}(s \\to x, k + 1) = \\sum_{s'} \\rho^{\\pi}(s \\to s', k) \\rho^{\\pi}(s' \\to x, 1). \\] <p>Then we go back to unroll the recursive representation of \\(\\nabla_{\\theta}V^{\\pi}(s)\\)! Let</p> \\[ \\phi(s) = \\sum_{a \\in \\mathcal{A}} \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) \\] <p>to simplify the maths. If we keep on extending \\(\\nabla_{\\theta}V^{\\pi}(\\cdot)\\) infinitely, it is easy to find out that we can transition from the starting state \\(s\\) to any state after any number of steps in this unrolling process and by summing up all the visitation probabilities, we get \\(\\nabla_{\\theta}V^{\\pi}(s)\\)!</p> \\[ \\begin{aligned} \\nabla_{\\theta}V^{\\pi}(s) &amp;= \\phi(s) + \\sum_{a} \\pi_{\\theta}(a|s) \\sum_{s'} P(s'|s,a) \\nabla_{\\theta}V^{\\pi}(s') \\\\ &amp;= \\phi(s) + \\sum_{s'} \\sum_{a} \\pi_{\\theta}(a|s) P(s'|s,a) \\nabla_{\\theta}V^{\\pi}(s') \\\\ &amp;= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\nabla_{\\theta}V^{\\pi}(s') \\\\ &amp;= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\sum_{a \\in \\mathcal{A}} \\left( \\nabla_{\\theta} \\pi_{\\theta}(a|s') Q^{\\pi}(s',a) + \\pi_{\\theta}(a|s') \\sum_{s''} P(s''|s',a) \\nabla_{\\theta}V^{\\pi}(s'') \\right) \\\\ &amp;= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\left[ \\phi(s') + \\sum_{s''} \\rho^{\\pi}(s' \\to s'', 1) \\nabla_{\\theta}V^{\\pi}(s'') \\right] \\\\ &amp;= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\phi(s') + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\sum_{s''} \\rho^{\\pi}(s' \\to s'', 1) \\nabla_{\\theta}V^{\\pi}(s'') \\\\ &amp;= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\phi(s') + \\sum_{s''} \\rho^{\\pi}(s \\to s'', 2) \\nabla_{\\theta}V^{\\pi}(s'') \\quad \\text{; Consider } s' \\text{ as the middle point for } s \\to s''. \\\\ &amp;= \\phi(s) + \\sum_{s'} \\rho^{\\pi}(s \\to s', 1) \\phi(s') + \\sum_{s''} \\rho^{\\pi}(s \\to s'', 2) \\phi(s'') + \\sum_{s'''} \\rho^{\\pi}(s \\to s''', 3) \\nabla_{\\theta}V^{\\pi}(s''') \\\\ &amp;= \\dots \\quad \\text{; Repeatedly unrolling the part of } \\nabla_{\\theta}V^{\\pi}(\\cdot) \\\\ &amp;= \\sum_{x \\in \\mathcal{S}} \\sum_{k=0}^{\\infty} \\rho^{\\pi}(s \\to x, k) \\phi(x) \\end{aligned} \\] <p>The nice rewriting above allows us to exclude the derivative of Q-value function, \\(\\nabla_{\\theta} Q^{\\pi}(s,a)\\). By plugging it into the objective function \\(J(\\theta)\\), we are getting the following:</p> \\[ \\begin{aligned} \\nabla_{\\theta}J(\\theta) &amp;= \\nabla_{\\theta}V^{\\pi}(s_0) \\\\ &amp;= \\sum_{s} \\sum_{k=0}^{\\infty} \\rho^{\\pi}(s_0 \\to s, k) \\phi(s) \\quad \\text{; Starting from a random state } s_0 \\\\ &amp;= \\sum_{s} \\eta(s) \\phi(s) \\quad \\text{; Let } \\eta(s) = \\sum_{k=0}^{\\infty} \\rho^{\\pi}(s_0 \\to s, k) \\\\ &amp;= \\left( \\sum_{s} \\eta(s) \\right) \\sum_{s} \\frac{\\eta(s)}{\\sum_{s} \\eta(s)} \\phi(s) \\quad \\text{; Normalize } \\eta(s), s \\in \\mathcal{S} \\text{ to be a probability distribution.} \\\\ &amp;\\propto \\sum_{s} \\frac{\\eta(s)}{\\sum_{s} \\eta(s)} \\phi(s) \\quad \\text{; } \\sum_{s} \\eta(s) \\text{ is a constant} \\\\ &amp;= \\sum_{s} d^{\\pi}(s) \\sum_{a} \\nabla_{\\theta} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) \\quad d^{\\pi}(s) = \\frac{\\eta(s)}{\\sum_{s} \\eta(s)} \\text{ is stationary distribution.} \\end{aligned} \\] <p>In the episodic case, the constant of proportionality (\\(\\sum_{s} \\eta(s)\\)) is the average length of an episode; in the continuing case, it is 1. The gradient can be further written as:</p> \\[ \\begin{aligned} \\nabla_{\\theta}J(\\theta) &amp;\\propto \\sum_{s \\in \\mathcal{S}} d^{\\pi}(s) \\sum_{a \\in \\mathcal{A}} Q^{\\pi}(s,a) \\nabla_{\\theta} \\pi_{\\theta}(a|s) \\\\ &amp;= \\sum_{s \\in \\mathcal{S}} d^{\\pi}(s) \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta}(a|s) Q^{\\pi}(s,a) \\frac{\\nabla_{\\theta} \\pi_{\\theta}(a|s)}{\\pi_{\\theta}(a|s)} \\quad \\text{; Because } \\ln(x)'=1/x \\\\ &amp;= \\mathbb{E}_{\\pi} [Q^{\\pi}(s,a) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a|s)] \\end{aligned} \\] <p>Where \\(\\mathbb{E}_{\\pi}\\) refers to \\(\\mathbb{E}_{s \\sim d^{\\pi}, a \\sim \\pi_{\\theta}}\\) when both state and action distributions follow the policy \\(\\pi_{\\theta}\\) (on policy).</p> <p>The policy gradient theorem lays the theoretical foundation for various policy gradient algorithms. This vanilla policy gradient update has no bias but high variance. Many following algorithms were proposed to reduce the variance while keeping the bias unchanged.</p> \\[ \\nabla_{\\theta}J(\\theta) = \\mathbb{E}_{\\pi} [Q^{\\pi}(s,a) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a|s)] \\]"},{"location":"course_notes/policy-based2/#policy-gradient-in-continuous-action-space","title":"Policy Gradient in Continuous Action Space","text":"<p>In a continuous action space, the policy gradient theorem is given by:</p> \\[\\nabla_{\\theta}J(\\theta) = \\mathbb{E}_{s \\sim d^{\\pi}, a \\sim  \\pi_{\\theta}} \\left[ Q^{\\pi}(s,a) \\nabla_{\\theta} \\ln  \\pi_{\\theta}(a|s) \\right]\\] <p>Since the action space is continuous, the summation over actions in the discrete case is replaced by an integral:</p> \\[\\nabla_{\\theta} J(\\theta) = \\int_{\\mathcal{S}} d^{\\pi}(s) \\int_{\\mathcal{A}} Q^{\\pi}(s,a) \\nabla_{\\theta} \\ln  \\pi_{\\theta}(a|s) \\pi_{\\theta}(a|s) \\, da \\, ds\\] <p>where:</p> <ul> <li> <p>\\(d^{\\pi}(s)\\) is the stationary state distribution under policy \\(\\pi_{\\theta}\\),</p> </li> <li> <p>\\(\\pi_{\\theta}(a|s)\\) is the probability density function for the continuous action \\(a\\) given state \\(s\\),</p> </li> <li> <p>\\(Q^{\\pi}(s,a)\\) is the state-action value function,</p> </li> <li> <p>\\(\\nabla_{\\theta} \\ln  \\pi_{\\theta}(a|s)\\) is the score function (policy gradient term),</p> </li> <li> <p>The integral is taken over all possible states \\(s\\) and actions \\(a\\).</p> </li> </ul> Gaussian Policy Example <p>A common choice for a continuous policy is a Gaussian distribution:</p> \\[a \\sim  \\pi_{\\theta}(a|s) = \\mathcal{N}(\\mu_{\\theta}(s), \\Sigma_{\\theta}(s))\\] <p>where:</p> <ul> <li> <p>\\(\\mu_{\\theta}(s)\\) is the mean of the action distribution, parameterized by \\(\\theta\\),</p> </li> <li> <p>\\(\\Sigma_{\\theta}(s)\\) is the covariance matrix (often assumed diagonal or fixed).</p> </li> </ul> <p>For a Gaussian policy, the logarithm of the probability density is:</p> \\[\\ln  \\pi_{\\theta}(a|s) = -\\frac{1}{2} (a - \\mu_{\\theta}(s))^T \\Sigma_{\\theta}^{-1} (a - \\mu_{\\theta}(s)) - \\frac{1}{2} \\ln |\\Sigma_{\\theta}|\\] <p>Taking the gradient:</p> \\[\\nabla_{\\theta} \\ln  \\pi_{\\theta}(a|s) = \\Sigma_{\\theta}^{-1} (a - \\mu_{\\theta}(s)) \\nabla_{\\theta} \\mu_{\\theta}(s)\\] <p>Thus, the policy gradient update becomes:</p> \\[\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{s \\sim d^{\\pi}, a \\sim  \\pi_{\\theta}} \\left[ Q^{\\pi}(s,a) \\Sigma_{\\theta}^{-1} (a - \\mu_{\\theta}(s)) \\nabla_{\\theta} \\mu_{\\theta}(s) \\right]\\]"},{"location":"course_notes/policy-based2/#reinforce","title":"REINFORCE","text":"<p>REINFORCE (Monte-Carlo policy gradient) relies on an estimated return by Monte-Carlo methods using episode samples to update the policy parameter \\(\\theta\\). REINFORCE works because the expectation of the sample gradient is equal to the actual gradient:</p> \\[ \\begin{aligned} \\nabla_{\\theta}J(\\theta) &amp;= \\mathbb{E}_{\\pi} \\left[ Q^{\\pi}(s,a) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a|s) \\right] \\\\ &amp;= \\mathbb{E}_{\\pi} \\left[ G_t \\nabla_{\\theta} \\ln \\pi_{\\theta}(A_t|S_t) \\right] \\quad \\text{; Because } Q^{\\pi}(S_t, A_t) = \\mathbb{E}_{\\pi} \\left[ G_t \\mid S_t, A_t \\right] \\end{aligned} \\] <p>Therefore we are able to measure \\(G_t\\) from real sample trajectories and use that to update our policy gradient. It relies on a full trajectory and that's why it is a Monte-Carlo method.</p>"},{"location":"course_notes/policy-based2/#algorithm","title":"Algorithm","text":"<p>The process is pretty straightforward:</p> <ol> <li> <p>Initialize the policy parameter \\(\\theta\\) at random.</p> </li> <li> <p>Generate one trajectory on policy \\(\\pi_{\\theta}\\): \\(S_1, A_1, R_2, S_2, A_2, \\dots, S_T\\).</p> </li> <li> <p>For \\(t = 1, 2, \\dots, T\\):</p> <ol> <li> <p>Estimate the return \\(G_t\\);</p> </li> <li> <p>Update policy parameters: \\(\\theta  \\leftarrow  \\theta + \\alpha  \\gamma^t G_t \\nabla_{\\theta} \\ln  \\pi_{\\theta}(A_t|S_t)\\)</p> </li> </ol> </li> </ol> <p>A widely used variation of REINFORCE is to subtract a baseline value from the return \\(G_t\\) to reduce the variance of gradient estimation while keeping the bias unchanged (Remember we always want to do this when possible).</p> <p>For example, a common baseline is to subtract state-value from action-value, and if applied, we would use advantage \\(A(s,a) = Q(s,a) - V(s)\\) in the gradient ascent update. This post nicely explained why a baseline works for reducing the variance, in addition to a set of fundamentals of policy gradient.</p>"},{"location":"course_notes/policy-based2/#gs-in-continuous-action-space","title":"\\(G(s)\\) in Continuous Action Space","text":"<p>In the continuous setting, we define the return \\(G(s)\\) as:</p> \\[G(s) = \\sum_{k=0}^{\\infty} \\gamma^k R(s_k, a_k), \\quad s_0 = s, \\quad a_k \\sim  \\pi_{\\theta}(\\cdot | s_k)\\] <p>where:</p> <ul> <li> <p>\\(R(s_k, a_k)\\) is the reward function for state-action pair \\((s_k, a_k)\\).</p> </li> <li> <p>\\(\\gamma\\) is the discount factor.</p> </li> <li> <p>\\(s_k\\) evolves according to the environment dynamics.</p> </li> <li> <p>\\(a_k \\sim  \\pi_{\\theta}(\\cdot | s_k)\\) means actions are sampled from the policy.</p> </li> </ul>"},{"location":"course_notes/policy-based2/#monte-carlo-approximation-of-qpisa","title":"Monte Carlo Approximation of \\(Q^{\\pi}(s,a)\\)","text":"<p>In expectation, \\(G(s)\\) serves as an unbiased estimator of the state-action value function:</p> \\[Q^{\\pi}(s,a) = \\mathbb{E} \\left[ G(s) \\middle| s_0 = s, a_0 = a \\right]\\] <p>Using this, we rewrite the policy gradient update as:</p> \\[\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{s \\sim d^{\\pi}, a \\sim  \\pi_{\\theta}} \\left[ G(s) \\nabla_{\\theta} \\ln  \\pi_{\\theta}(a | s) \\right]\\]"},{"location":"course_notes/policy-based2/#variance-reduction-advantage-function","title":"Variance Reduction: Advantage Function","text":"<p>A baseline is often subtracted to reduce variance while keeping the expectation unchanged:</p> \\[A(s, a) = G(s) - V^{\\pi}(s)\\] \\[\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{s \\sim d^{\\pi}, a \\sim  \\pi_{\\theta}} \\left[ A(s, a) \\nabla_{\\theta} \\ln  \\pi_{\\theta}(a | s) \\right]\\] <p>where:</p> <ul> <li>\\(V^{\\pi}(s) = \\mathbb{E}_{a \\sim  \\pi_{\\theta}(\\cdot | s)} [Q^{\\pi}(s,a)]\\)</li> </ul> <p>is the state value function.</p> <ul> <li>\\(A(s,a)\\) measures the advantage of taking action \\(a\\) over the expected policy action.</li> </ul>"},{"location":"course_notes/policy-based2/#bias-and-variance","title":"Bias and Variance","text":"<p>In this section we delve deeper into the bias and variance problem in RL especially in policy gradient </p>"},{"location":"course_notes/policy-based2/#monte-carlo-estimators-in-reinforcement-learning","title":"Monte Carlo Estimators in Reinforcement Learning","text":"<p>A Monte Carlo estimator is a method used to approximate the expected value of a function \\(f(X)\\) over a random variable \\(X\\) with a given probability distribution \\(p(X)\\). The true expectation is:</p> \\[E[f(X)] = \\int f(x) p(x) \\, dx\\] <p>However, directly computing this integral may be complex. Instead, we use Monte Carlo estimation by drawing \\(N\\) independent samples \\(X_1, X_2, \\dots, X_N\\) from \\(p(X)\\) and computing:</p> \\[\\hat{\\mu}_{MC} = \\frac{1}{N} \\sum_{i=1}^{N} f(X_i)\\] <p>This estimator provides an approximation to the true expectation \\(E[f(X)]\\).</p> <p>By the law of large numbers (LLN), as \\(N \\to  \\infty\\), we have:</p> \\[\\hat{X}_N \\to  \\mathbb{E}[X] \\quad  \\text{(almost surely)}\\] <p>Monte Carlo methods are commonly used in RL for estimating expected rewards, state-value functions, and action-value functions.</p>"},{"location":"course_notes/policy-based2/#bias-in-policy-gradient-methods","title":"Bias in Policy Gradient Methods","text":"<p>Bias in reinforcement learning arises when an estimator systematically deviates from the true value. In policy gradient methods, bias is introduced due to function approximation, reward estimation, or gradient computation errors.</p>"},{"location":"course_notes/policy-based2/#sources-of-bias","title":"Sources of Bias","text":"<ul> <li> <p>Function Approximation Bias: Policy gradient methods often rely on neural networks or other function approximators for policy representation. Imperfect approximations introduce systematic errors, leading to biased policy updates.</p> </li> <li> <p>Reward Clipping or Discounting: Algorithms using reward clipping or high discount factors (\\(\\gamma\\)) can distort return estimates, causing the learned policy to be biased toward short-term rewards.</p> </li> <li> <p>Baseline Approximation: Variance reduction techniques like baseline subtraction use estimates of expected returns. If the baseline is inaccurately estimated, it introduces bias in the policy gradient computation.</p> </li> </ul> Example of Bias <p>Consider a self-driving car optimizing for fuel efficiency. If the reward function prioritizes immediate fuel consumption over long-term efficiency, the learned policy may favor suboptimal strategies that minimize fuel use in the short term while missing globally optimal driving behaviors.</p>"},{"location":"course_notes/policy-based2/#biased-vs-unbiased-estimation","title":"Biased vs. Unbiased Estimation","text":"<p>For example: The biased formula for the sample variance \\(S^2\\) is given by:</p> \\[S^2_{\\text{biased}} = \\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\overline{X})^2\\] <p>This is an underestimation of the true population variance \\(\\sigma^2\\) because it does not account for the degrees of freedom in estimation.</p> <p>Instead, the unbiased estimator is:</p> \\[S^2_{\\text{unbiased}} = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\overline{X})^2.\\] <p>This unbiased estimator correctly accounts for variance in small sample sizes, ensuring \\(\\mathbb{E}[S^2_{\\text{unbiased}}] = \\sigma^2\\).</p>"},{"location":"course_notes/policy-based2/#variance-in-policy-gradient-methods","title":"Variance in Policy Gradient Methods","text":"<p>Variance in policy gradient estimates refers to fluctuations in gradient estimates across different training episodes. High variance leads to instability and slow convergence.</p>"},{"location":"course_notes/policy-based2/#sources-of-variance","title":"Sources of Variance","text":"<ul> <li> <p>Monte Carlo Estimation: REINFORCE estimates gradients using complete episodes, leading to high variance due to trajectory randomness.</p> </li> <li> <p>Stochastic Policy Outputs: Policies represented as probability distributions (e.g., Gaussian policies) introduce additional randomness in gradient updates.</p> </li> <li> <p>Exploration Strategies: Methods like softmax or epsilon-greedy increase variance by adding stochasticity to action selection.</p> </li> </ul> Example of Variance <p>Consider a robotic arm learning to grasp objects. Due to high variance, in some episodes, it succeeds, while in others, minor variations cause failure. These inconsistencies slow down convergence.</p>"},{"location":"course_notes/policy-based2/#techniques-to-reduce-variance-in-policy-gradient-methods","title":"Techniques to Reduce Variance in Policy Gradient Methods","text":"<p>Several strategies help mitigate variance in policy gradient methods while preserving unbiased gradient estimates.</p>"},{"location":"course_notes/policy-based2/#baseline-subtraction","title":"Baseline Subtraction","text":"<p>A baseline function \\(b\\) reduces variance without introducing bias:</p> \\[\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}} \\left[ \\nabla_{\\theta} \\log  \\pi_{\\theta}(a_t | s_t) (G_t - b) \\right].\\] <p>A common choice for \\(b\\) is the average return over trajectories:</p> \\[b = \\frac{1}{N} \\sum_{i=1}^{N} G_i.\\] <p>Since \\(b\\) is independent of actions, it does not introduce bias in the gradient estimate while reducing variance.</p> proof \\[\\begin{aligned} E\\left[\\nabla_\\theta  \\log p_\\theta(\\tau) b\\right] &amp;= \\int p_\\theta(\\tau) \\nabla_\\theta  \\log p_\\theta(\\tau) b \\, d\\tau \\\\ &amp;= \\int  \\nabla_\\theta p_\\theta(\\tau) b \\, d\\tau \\\\ &amp;= b \\nabla_\\theta  \\int p_\\theta(\\tau) \\, d\\tau \\\\ &amp;= b \\nabla_\\theta  1 \\\\ &amp;= 0 \\end{aligned}\\]"},{"location":"course_notes/policy-based2/#causality-trick-and-reward-to-go-estimation","title":"Causality Trick and Reward-to-Go Estimation","text":"<p>To ensure that policy updates at time \\(t\\) are only influenced by rewards from that time step onward, we use the causality trick:</p> \\[\\nabla_{\\theta} J(\\theta) \\approx  \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log  \\pi_{\\theta}(a_{i,t} | s_{i,t}) \\left( \\sum_{t'=t}^{T} r(a_{i,t'}, s_{i,t'}) \\right).\\] <p>Instead of summing over all rewards, the reward-to-go estimate restricts the sum to future rewards only:</p> \\[Q(s_t, a_t) = \\sum_{t'=t}^{T} \\mathbb{E}_{\\pi_{\\theta}} [r(s_{t'}, a_{t'}) | s_t, a_t].\\] \\[\\nabla_{\\theta} J(\\theta) \\approx  \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log  \\pi_{\\theta}(a_{i,t} | s_{i,t}) Q(s_{i,t}, a_{i,t}).\\] <p>This prevents rewards from future time steps from affecting past actions, reducing variance. This approach results in much lower variance compared to the traditional Monte Carlo methods.</p> proof \\[ \\begin{aligned} A_{t_0-1} &amp;= s_{t_0-1}, a_{t_0-1}, \\dots, a_0, s_0 \\\\ \\mathbb{E}_{A_{t_0-1}} &amp;\\left[ \\mathbb{E}_{s_{t_0}, a_{t_0} | A_{t_0-1}} \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) \\sum_{t=0}^{t_0 - 1} r(s_t, a_t) \\right] \\right] \\\\ U_{t_0-1} &amp;= \\sum_{t=0}^{t_0 - 1} r(s_t, a_t) \\\\ &amp;= \\mathbb{E}_{A_{t_0-1}} \\left[ U_{t_0-1} \\mathbb{E}_{s_{t_0}, a_{t_0} | s_{t_0-1}, a_{t_0-1}} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) \\right] \\\\ &amp;= \\mathbb{E}_{A_{t_0-1}} \\left[ U_{t_0-1} \\mathbb{E}_{s_{t_0} | s_{t_0-1}, a_{t_0-1}} \\mathbb{E}_{a_{t_0} | s_{t_0-1}, a_{t_0-1}, s_{t_0}} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) \\right] \\\\ &amp;= \\mathbb{E}_{A_{t_0-1}} \\left[ U_{t_0-1} \\mathbb{E}_{s_{t_0} | s_{t_0-1}, a_{t_0-1}} \\mathbb{E}_{a_{t_0} | s_{t_0}} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) \\right] \\\\ &amp;= \\mathbb{E}_{A_{t_0-1}} \\left[ U_{t_0-1} \\mathbb{E}_{s_{t_0} | s_{t_0-1}, a_{t_0-1}} \\mathbb{E}_{\\pi_{\\theta} (a_{t_0} | s_{t_0})} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) \\right] \\\\ \\mathbb{E}_{\\pi_{\\theta} (a_{t_0} | s_{t_0})} &amp;\\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) = 0 \\\\ \\mathbb{E}_{A_{t_0-1}}&amp; \\left[ \\mathbb{E}_{s_{t_0}, a_{t_0} | A_{t_0-1}} \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{t_0} | s_{t_0}) \\sum_{t=0}^{t_0 - 1} r(s_t, a_t) \\right] \\right] = 0 \\end{aligned} \\]"},{"location":"course_notes/policy-based2/#discount-factor-adjustment","title":"Discount Factor Adjustment","text":"<p>The discount factor \\(\\gamma\\) helps reduce variance by weighting rewards closer to the present more heavily:</p> \\[G_t = \\sum_{t' = t}^{T} \\gamma^{t'-t} r(s_{t'}, a_{t'}).\\] proof \\[ \\begin{aligned} \\nabla_{\\theta} J(\\theta) &amp;\\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{i,t} | s_{i,t}) \\left( \\sum_{t' = t}^{T} \\gamma^{t' - t} r(s_{i,t'}, a_{i,t'}) \\right) \\\\ \\nabla_{\\theta} J(\\theta) &amp;\\approx \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{i,t} | s_{i,t}) \\right) \\left( \\sum_{t=1}^{T} \\gamma^{t-1} r(s_{i,t}, a_{i,t}) \\right) \\\\ \\nabla_{\\theta} J(\\theta) &amp;\\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{i,t} | s_{i,t}) \\left( \\sum_{t' = t}^{T} \\gamma^{t' - t} r(s_{i,t'}, a_{i,t'}) \\right) \\\\ \\nabla_{\\theta} J(\\theta) &amp;\\approx \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\gamma^{t-1} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_{i,t} | s_{i,t}) \\left( \\sum_{t' = t}^{T} \\gamma^{t' - t} r(s_{i,t'}, a_{i,t'}) \\right) \\end{aligned} \\] <p>A lower \\(\\gamma\\) (e.g., 0.9) reduces variance but increases bias, while a higher \\(\\gamma\\) (e.g., 0.99) improves long-term estimation but increases variance. A balance is needed.</p>"},{"location":"course_notes/policy-based2/#advantage-estimation-and-actor-critic-methods","title":"Advantage Estimation and Actor-Critic Methods","text":"<p>Actor-critic methods combine policy optimization (actor) with value function estimation (critic). The advantage function is defined as:</p> \\[A^{\\pi}(s_t, a_t) = Q^{\\pi}(s_t, a_t) - V^{\\pi}(s_t),\\] <p>where the action-value function is:</p> \\[Q^{\\pi}(s_t, a_t) = \\sum_{t' = t}^{T} \\mathbb{E}_{\\pi} [r(s_{t'}, a_{t'}) | s_t, a_t],\\] <p>and the state-value function is:</p> \\[V^{\\pi}(s_t) = \\mathbb{E}_{a_t \\sim  \\pi_{\\theta}(a_t | s_t)} [Q^{\\pi}(s_t, a_t)].\\] <p>The policy gradient update using the advantage function becomes:</p> \\[\\nabla_{\\theta} J(\\theta) \\approx  \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\nabla_{\\theta} \\log  \\pi_{\\theta}(a_{i,t} | s_{i,t}) A^{\\pi}(s_{i,t}, a_{i,t}).\\] <p>This formulation allows for lower variance in policy updates while leveraging learned state-value estimates. Actor-critic methods are widely used in modern reinforcement learning due to their stability and efficiency.</p>"},{"location":"course_notes/policy-based2/#actor-critic","title":"Actor-Critic","text":"<p>Two main components in policy gradient methods are the policy model and the value function. It makes a lot of sense to learn the value function in addition to the policy since knowing the value function can assist the policy update, such as by reducing gradient variance in vanilla policy gradients. That is exactly what the Actor-Critic method does.</p> <p>Actor-Critic methods consist of two models, which may optionally share parameters:</p> <ul> <li> <p>Critic: Updates the value function parameters \\(w\\). Depending on the algorithm, it could be an action-value function \\(Q(s, a)\\) or a state-value function \\(V(s)\\).</p> </li> <li> <p>Actor: Updates the policy parameters \\(\\theta\\) for \\(\\pi_{\\theta}(a | s)\\), in the direction suggested by the critic.</p> </li> </ul> <p>Let's see how it works in a simple action-value Actor-Critic algorithm:</p> <ol> <li> <p>Initialize policy parameters \\(\\theta\\) and value function parameters \\(w\\) at random.</p> </li> <li> <p>Sample initial state \\(s_0\\).</p> </li> <li> <p>For each time step \\(t\\):</p> <ol> <li> <p>Sample reward \\(r_t\\) and next state \\(s_{t+1}\\).</p> </li> <li> <p>Then sample the next action \\(a_{t+1}\\) from policy: \\(\\pi_{\\theta}(s_{t+1})\\)</p> </li> <li> <p>Update the policy parameters: \\(\\theta  \\leftarrow  \\theta + \\alpha  \\nabla_{\\theta} \\log  \\pi_{\\theta}(a_t | s_t) Q(s_t, a_t)\\)</p> </li> </ol> </li> <li> <p>Compute the correction (TD error) for action-value at time \\(t\\):</p> </li> </ol> \\[\\delta_t = r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)\\] <ol> <li>Use it to update the parameters of the action-value function:</li> </ol> \\[w \\leftarrow w + \\beta  \\delta_t  \\nabla_w Q(s_t, a_t)\\] <ol> <li>Update \\(\\theta\\) and \\(w\\).</li> </ol> <p>Two learning rates, \\(\\alpha\\) and \\(\\beta\\), are predefined for policy and value function parameter updates, respectively.</p> Actor-Critic Architecture: Cartpole Example <p>Let's illustrate the Actor-Critic architecture with an example of a classic reinforcement learning problem: the Cartpole environment.</p> <p> </p> <p>In the Cartpole environment, the agent controls a cart that can move horizontally on a track. A pole is attached to the cart, and the agent's task is to balance the pole upright for as long as possible.</p> <ol> <li> <p>Actor (Policy-Based): The actor is responsible for learning the policy, which is the agent's strategy for selecting actions (left or right) based on the observed state (cart position, cart velocity, pole angle, and pole angular velocity).</p> </li> <li> <p>Critic (Value-Based): The critic is responsible for learning the value function, which estimates the expected total reward (return) from each state. The value function helps evaluate how good or bad a specific state is, which guides the actor's updates.</p> </li> <li> <p>Policy Representation: For simplicity, let's use a neural network as the actor. The neural network takes the current state of the cart and pole as input and outputs the probabilities of selecting actions (left or right).</p> </li> <li> <p>Value Function Representation: For the critic, we also use a neural network. The neural network takes the current state as input and outputs an estimate of the expected total reward (value) for that state.</p> </li> <li> <p>Collecting Experiences: The agent interacts with the environment, using the current policy to select actions (left or right). As it moves through the environment, it collects experiences, including states, actions, rewards, and next states.</p> </li> <li> <p>Updating the Critic (Value Function): The critic learns to estimate the value function using the collected experiences. It optimizes its neural network parameters to minimize the difference between the predicted values and the actual rewards experienced by the agent.</p> </li> <li> <p>Calculating the Advantage: The advantage represents how much better or worse an action is compared to the average expected value. It is calculated as the difference between the total return (reward) and the value function estimate for each state-action pair.</p> </li> <li> <p>Updating the Actor (Policy): The actor updates its policy to increase the probabilities of actions with higher advantages and decrease the probabilities of actions with lower advantages. This process helps the actor learn from the critic's feedback and improve its policy to maximize the expected rewards.</p> </li> <li> <p>Iteration and Learning: The learning process is repeated over multiple episodes and iterations. As the agent explores and interacts with the environment, the actor and critic networks gradually improve their performance and converge to better policies and value function estimates.</p> </li> </ol> <p>Through these steps, the Actor-Critic architecture teaches the agent how to balance the pole effectively in the Cartpole environment. The actor learns the best actions to take in different states, while the critic provides feedback on the quality of the actor's decisions. As a result, the agent converges to a more optimal policy, achieving longer balancing times and better performance in the task.</p>"},{"location":"course_notes/policy-based2/#summary-of-variance-reduction-methods","title":"Summary of Variance Reduction Methods","text":"<p>To summarize, the key methods for reducing variance in policy gradient</p> <p>methods include:</p> <ul> <li> <p>Baseline Subtraction: Subtracting an average return baseline to reduce variance while keeping gradients unbiased.</p> </li> <li> <p>Causality Trick and Reward-to-Go: Using future rewards from time step \\(t\\) onward to prevent variance from irrelevant past rewards.</p> </li> <li> <p>Discount Factor Adjustment: Adjusting \\(\\gamma\\) to balance variance reduction and long-term reward optimization.</p> </li> <li> <p>Advantage Estimation: Using the advantage function \\(A(s_t, a_t)\\) instead of raw returns to stabilize learning.</p> </li> <li> <p>Actor-Critic Methods: Combining policy gradient updates with value function estimation to create more stable and efficient training.</p> </li> </ul> <p>By employing these techniques, policy gradient methods can achieve more stable and efficient learning with reduced variance.</p>"},{"location":"course_notes/policy-based2/#concluding-remarks","title":"Concluding Remarks","text":"<p>Now that we have seen the principles behind a policy-based algorithm, let us see how policy-based algorithms work in practice, and compare advantages and disadvantages of the policy-based approach.</p> <p>Let us start with the advantages. First of all, parameterization is at the core of policy-based methods, making them a good match for deep learning. For value- based methods, deep learning had to be retrofitted, giving rise to complications. Second, policy-based methods can easily find stochastic policies, whereas value- based methods find deterministic policies. Due to their stochastic nature, policy- based methods naturally explore, without the need for methods such as \\(\\epsilon\\)-greedy, or more involved methods that may require tuning to work well. Third, policy-based methods are effective in large or continuous action spaces. Small changes in \\(\\theta\\) lead to small changes in \\(\\pi\\), and to small changes in state distributions (they are smooth). Policy-based algorithms do not suffer (as much) from convergence and stability issues that are seen in \\(\\arg\\max\\)-based algorithms in large or continuous action spaces.</p> <p>On the other hand, there are disadvantages to the episodic Monte Carlo version of the REINFORCE algorithm. Remember that REINFORCE generates a full random episode in each iteration before it assesses the quality. (Value-based methods use a reward to select the next action in each time step of the episode.) Because of this, policy-based methods exhibit low bias since full random trajectories are generated. However, they are also high variance, since the full trajectory is generated randomly, whereas value-based methods use the value for guidance at each selection step.</p> <p>What are the consequences? First, policy evaluation of full trajectories has low sample efficiency and high variance. As a consequence, policy improvement happens infrequently, leading to slow convergence compared to value-based methods. Second, this approach often finds a local optimum, since convergence to the global optimum takes too long.</p> <p>Much research has been performed to address the high variance of the episode- based vanilla policy gradient. The enhancements that have been found have greatly improved performance, so much so that policy-based approaches---such as A3C, PPO, SAC, and DDPG---have become favorite model-free reinforcement learning algorithms for many applications.</p>"},{"location":"course_notes/policy-based2/#authors","title":"Author(s)","text":"<ul> <li> <p>Nima Shirzady</p> <p>Teaching Assistant</p> <p>shirzady.1934@gmail.com</p> <p> </p> </li> <li> <p>Hamidreza Ebrahimpour</p> <p>Teaching Assistant</p> <p>ebrahimpour.7879@gmail.com</p> <p> </p> </li> <li> <p>Hesam Hosseini</p> <p>Teaching Assistant</p> <p>hesam138122@gmail.com</p> <p> </p> </li> </ul>"},{"location":"course_notes/policy-based2/#references","title":"References","text":"<ol> <li> <p>Reinforcement Learning Explained</p> </li> <li> <p>An Introduction to Deep Reinforcement Learning</p> </li> <li> <p>Deep Reinforcement Learning Processor Design for Mobile Applications</p> </li> <li> <p>REINFORCE \u2014 a policy-gradient based reinforcement Learning algorithm</p> </li> <li> <p>Policy Gradient Algorithms</p> </li> <li> <p>Deep Reinforcement Learning</p> </li> <li> <p>Reinforcement Learning (BartoSutton)</p> </li> </ol>"},{"location":"course_notes/value-based/","title":"Week 2: Value-based Methods","text":""},{"location":"course_notes/value-based/#1-bellman-equations-and-value-functions","title":"1. Bellman Equations and Value Functions","text":""},{"location":"course_notes/value-based/#11-state-value-function-vs","title":"1.1. State Value Function \\(V(s)\\)","text":""},{"location":"course_notes/value-based/#definition","title":"Definition:","text":"<p>The state value function \\(V^\\pi(s)\\) measures the expected return when an agent starts in state \\(s\\) and follows a policy \\(\\pi\\). It provides a scalar value for each state that reflects the desirability of that state under the given policy. Formally, it is defined as:</p> \\[ V^\\pi(s) = \\mathbb{E} \\left[ G_t \\mid s_t = s \\right] \\] <p>Where \\(G_t\\) represents the return (total reward) from time step \\(t\\) onwards:</p> \\[ G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\] <ul> <li>\\(R_t\\) is the reward received at time step \\(t\\).</li> <li>\\(\\gamma\\) is the discount factor (\\(0 \\leq \\gamma \\leq 1\\)), controlling how much future rewards are valued compared to immediate rewards.</li> </ul>"},{"location":"course_notes/value-based/#bellman-expectation-equation-for-vpis","title":"Bellman Expectation Equation for \\(V^\\pi(s)\\):","text":"<p>The Bellman Expectation Equation for the state value function expresses the value of a state \\(s\\) in terms of the expected immediate reward and the discounted value of the next state. It is written as:</p> \\[ V^\\pi(s) = \\mathbb{E} \\left[ R_{t+1} + \\gamma V^\\pi(s_{t+1}) \\mid s_t = s \\right] \\] <p>Using the transition probabilities of the environment, this can be expanded as:</p> \\[ V^\\pi(s) = \\sum_{s'} P(s'|s, \\pi(s)) \\left[ R(s, \\pi(s), s') + \\gamma V^\\pi(s') \\right] \\] <p>Where: - \\(P(s'|s, \\pi(s))\\) is the probability of transitioning from state \\(s\\) to state \\(s'\\) when following action \\(\\pi(s)\\). - \\(R(s, \\pi(s), s')\\) is the reward for transitioning from state \\(s\\) to \\(s'\\) under action \\(\\pi(s)\\).</p> <p>This equation allows for the iterative computation of state values in a model-based setting.</p>"},{"location":"course_notes/value-based/#12-action-value-function-qs-a","title":"1.2. Action Value Function \\(Q(s, a)\\)","text":""},{"location":"course_notes/value-based/#definition_1","title":"Definition:","text":"<p>The action value function \\(Q^\\pi(s, a)\\) represents the expected return when an agent starts in state \\(s\\), takes action \\(a\\), and then follows policy \\(\\pi\\):</p> \\[ Q^\\pi(s, a) = \\mathbb{E} \\left[ G_t \\mid s_t = s, a_t = a \\right] \\] <p>Where \\(G_t\\) is the return starting at time \\(t\\).</p>"},{"location":"course_notes/value-based/#bellman-expectation-equation-for-qpis-a","title":"Bellman Expectation Equation for \\(Q^\\pi(s, a)\\):","text":"<p>The Bellman Expectation Equation for the action value function is similar to the one for the state value function but includes both the action and the subsequent states and actions. It is given by:</p> \\[ Q^\\pi(s, a) = \\mathbb{E} \\left[ R_{t+1} + \\gamma Q^\\pi(s_{t+1}, a_{t+1}) \\mid s_t = s, a_t = a \\right] \\] <p>Expanding this into a sum over possible next states, we get:</p> \\[ Q^\\pi(s, a) = \\sum_{s'} P(s'|s, a) \\left[ R(s, a, s') + \\gamma \\sum_{a'} \\pi(a'|s') Q^\\pi(s', a') \\right] \\] <p>Where: - \\(P(s'|s, a)\\) is the transition probability from state \\(s\\) to state \\(s'\\) under action \\(a\\). - \\(\\pi(a'|s')\\) is the probability of taking action \\(a'\\) in state \\(s'\\) under policy \\(\\pi\\).</p>"},{"location":"course_notes/value-based/#bellman-optimality-equation-for-qs-a","title":"Bellman Optimality Equation for \\(Q^*(s, a)\\):","text":"<p>The Bellman Optimality Equation for \\(Q^*(s, a)\\) expresses the optimal action value function. It is given by:</p> \\[ Q^*(s, a) = \\mathbb{E} \\left[ R_{t+1} + \\gamma \\max_{a'} Q^*(s_{t+1}, a') \\mid s_t = s, a_t = a \\right] \\] <p>This shows that the optimal action value at each state-action pair is the immediate reward plus the discounted maximum expected value from the next state, where the next action is chosen optimally.</p>"},{"location":"course_notes/value-based/#2-dynamic-programming","title":"2. Dynamic Programming","text":"<p>Dynamic Programming (DP) is a powerful technique used to solve reinforcement learning problems where the environment is fully known (i.e., the model is available). DP algorithms compute the optimal policy and value functions by iteratively updating estimates based on a model of the environment. </p>"},{"location":"course_notes/value-based/#21-value-iteration","title":"2.1. Value Iteration","text":""},{"location":"course_notes/value-based/#bellman-optimality-equation","title":"Bellman Optimality Equation:","text":"<p>The Bellman Optimality Equation for the value function is:</p> \\[ V_{k+1}(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V_k(s') \\right] \\] <p>Where: - \\(\\max_a\\) selects the action \\(a\\) that maximizes the expected return from state \\(s\\).</p>"},{"location":"course_notes/value-based/#value-iteration-algorithm","title":"Value Iteration Algorithm:","text":"<ol> <li>Initialize the value function \\(V_0(s)\\) arbitrarily.</li> <li> <p>Repeat until convergence:</p> <ul> <li>For each state \\(s\\), update the value function:</li> </ul> \\[ V_{k+1}(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V_k(s') \\right] \\] </li> <li> <p>Once the value function converges, the optimal policy \\(\\pi^*(s)\\) can be derived by selecting the action that maximizes the expected return:</p> </li> </ol> \\[ \\pi^*(s) = \\arg\\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V^*(s') \\right] \\]"},{"location":"course_notes/value-based/#convergence","title":"Convergence:","text":"<p>Value Iteration is guaranteed to converge to the optimal value function and policy. The number of iterations required depends on the problem's dynamics, but it typically converges faster than Policy Iteration in terms of the number of iterations, though it may require more computation per iteration.</p>"},{"location":"course_notes/value-based/#22-policy-evaluation","title":"2.2. Policy Evaluation","text":"<p>Policy Evaluation calculates the state value function \\(V^\\pi(s)\\) for a given policy \\(\\pi\\) by iteratively updating the value function using the Bellman Expectation Equation:</p> \\[ V^\\pi(s) = R(s, \\pi(s)) + \\gamma \\sum_{s'} P(s'|s, \\pi(s)) V^\\pi(s') \\] <p>This process is repeated until \\(V^\\pi(s)\\) converges to a fixed point for all s.</p>"},{"location":"course_notes/value-based/#23-policy-improvement","title":"2.3. Policy Improvement","text":"<p>Policy Improvement refines a policy \\(\\pi\\) by making it greedy with respect to the current value function:</p> \\[ \\pi'(s) = \\arg\\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V^\\pi(s') \\right] \\] <p>It is proven that the new policy\u2019s value function is at least as good as the previous one:</p> \\[ V^{\\pi'}(s) \\geq V^\\pi(s), \\quad \\forall s. \\] <p>By repeating policy evaluation and improvement, the policy converges to the optimal one.</p> <ol> <li> <p>Single-Step Improvement: </p> <ul> <li>Modify the policy at only \\(t = 0\\), keeping the rest unchanged.  </li> <li>This new policy achieves a higher or equal value:  </li> </ul> \\[ V^{\\pi_{(k+1)}^{(1)}}(s) \\geq V^{\\pi_k}(s), \\quad \\forall s. \\] </li> <li> <p>Extending to Multiple Steps: </p> <ul> <li>Modify the policy at \\(t = 0\\) and \\(t = 1\\), keeping the rest unchanged.  </li> <li>Again, the value function improves: </li> </ul> \\[ V^{\\pi_{(k+1)}^{(2)}}(s) \\geq V^{\\pi_{(k+1)}^{(1)}}(s) \\geq V^{\\pi_k}(s). \\] </li> <li> <p>Repeating for All Steps: </p> <ul> <li>After applying this to all time steps, the final policy matches the fully improved one:  </li> </ul> \\[ \\pi_{(k+1)}^{(\\infty)}(s) = \\pi_{k+1}(s). \\] <ul> <li>This ensures:  </li> </ul> \\[ V^{\\pi_{k+1}}(s) \\geq V^{\\pi_k}(s), \\quad \\forall s. \\] </li> </ol> <p>The value function never decreases with each update.  </p>"},{"location":"course_notes/value-based/#24-policy-iteration","title":"2.4. Policy Iteration","text":"<p>Policy Iteration alternates between policy evaluation and policy improvement to compute the optimal policy.</p> <ol> <li>Initialize policy \\(\\pi_0\\) randomly.</li> <li>Policy Evaluation: Compute the value function \\(V^{\\pi_k}(s)\\) for the current policy \\(\\pi_k\\) using the Bellman Expectation Equation.</li> <li>Policy Improvement: Update the policy \\(\\pi_{k+1}(s)\\) by making it greedy with respect to the current value function:</li> </ol> \\[  \\pi_{k+1}(s) = \\arg\\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V^{\\pi_k}(s') \\right]  \\] <ol> <li>Repeat the above steps until the policy converges (i.e., \\(\\pi_k = \\pi_{k+1}\\)).</li> </ol>"},{"location":"course_notes/value-based/#convergence_1","title":"Convergence:","text":"<p>Each policy update ensures that the value function does not decrease.</p> <p>Since there are only a finite number of deterministic policies in a finite Markov Decision Process (MDP), the sequence of improving policies must eventually reach an policy \\(\\pi^*\\), where further improvement do not change it.</p> <p>The value function of the fixed point \\(\\pi^*\\) satisfy the Bellman Optimality Equation:</p> \\[ V^*(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V^*(s') \\right] \\] <p>This confirms that the final policy \\(\\pi^*\\) is optimal.</p>"},{"location":"course_notes/value-based/#25-comparison-of-policy-iteration-and-value-iteration","title":"2.5. Comparison of Policy Iteration and Value Iteration","text":"<p>Policy Iteration and Value Iteration are two dynamic programming methods for finding the optimal policy in an MDP. Both rely on iterative updates but differ in efficiency and computation.</p> Feature Policy Iteration Value Iteration Update Method Alternates between policy evaluation and improvement Updates value function directly Computational Cost Per Iteration \\(O(\\|S\\|^3)\\) (solving linear equations) \\(O(\\|S\\| \\cdot \\|A\\|)\\) (maximization over actions) Number of Iterations Fewer iterations, but each is expensive More iterations, but each is cheaper Best for Small state spaces, deterministic transitions Large state spaces, stochastic transitions <p>Policy Iteration explicitly computes the value function for a given policy, requiring solving a system of equations. Each iteration is computationally expensive but results in a significant improvement, leading to faster convergence in terms of iterations.</p> <p>Value Iteration avoids solving a system of equations by updating the value function incrementally. Each iteration is computationally cheaper, but because the value function is updated gradually, more iterations are needed for convergence.</p> <p>Thus, Policy Iteration takes fewer iterations but is computationally heavy per step, while Value Iteration takes more iterations but is computationally lighter per step.</p> <p>Watch on YouTube</p>"},{"location":"course_notes/value-based/#3-monte-carlo-methods","title":"3. Monte Carlo Methods","text":""},{"location":"course_notes/value-based/#31-planning-vs-learning-in-rl","title":"3.1. Planning vs. Learning in RL","text":"<p>Reinforcement learning can be approached in two ways: planning and learning. The main difference is that planning relies on a model of the environment, while learning uses real-world interactions to improve decision-making.</p>"},{"location":"course_notes/value-based/#planning-model-based-rl","title":"Planning (Model-Based RL)","text":"<ul> <li>Uses a model to predict state transitions and rewards.</li> <li>The agent can simulate future actions without interacting with the environment.</li> <li>Examples: Dynamic Programming (DP), Monte Carlo Tree Search (MCTS).</li> </ul>"},{"location":"course_notes/value-based/#learning-model-free-rl","title":"Learning (Model-Free RL)","text":"<ul> <li>No access to a model; the agent learns by interacting with the environment.</li> <li>The agent updates value estimates based on observed rewards.</li> <li>Examples: Monte Carlo, Temporal Difference (TD), Q-Learning.</li> </ul> <p>Planning is efficient when a reliable model is available, but learning is necessary when the model is unknown or too complex to compute.</p> <p>Monte Carlo methods fall under Model-Free RL, where the agent improves through experience. The following sections introduce how Monte Carlo Sampling is used to estimate value functions without needing a model.</p>"},{"location":"course_notes/value-based/#32-introduction-to-monte-carlo","title":"3.2. Introduction to Monte Carlo","text":"<p>Monte Carlo methods use random sampling to estimate numerical results, especially when direct computation is infeasible or the underlying distribution is unknown. These methods are widely applied in physics, finance, optimization, and reinforcement learning.</p> <p>Monte Carlo estimates an expectation:</p> \\[ I = \\mathbb{E}[f(X)] = \\int f(x) p(x) dx \\] <p>using sample averaging:</p> \\[ \\hat{I}_N = \\frac{1}{N} \\sum_{i=1}^{N} f(x_i), \\] <p>where $ x_i $ are independent samples drawn from $ p(x) $. The Law of Large Numbers (LLN) ensures that as \\(N \\to \\infty\\):</p> \\[ \\hat{I}_N \\to I. \\] <p>This guarantees convergence, but the speed of convergence depends on the variance of the samples.</p> <p>Monte Carlo estimates become more accurate as \\(N\\) increases, but independent samples are crucial for unbiased estimation.</p> <p>By the Central Limit Theorem (CLT), for large \\(N\\), the Monte Carlo estimate follows a normal distribution:</p> \\[ \\hat{I}_N \\approx \\mathcal{N} \\left(I, \\frac{\\sigma^2}{N} \\right). \\] <p>This shows that the variance decreases at a rate of \\(O(1/N)\\), meaning that as the number of independent samples increases, the estimate becomes more stable. However, this reduction is slow, requiring a large number of samples to achieve high precision.</p>"},{"location":"course_notes/value-based/#example-estimating-pi","title":"Example: Estimating \\(\\pi\\)","text":"<p>Monte Carlo methods can estimate \\(\\pi\\) by randomly sampling points and analyzing their distribution relative to a known geometric shape.</p>"},{"location":"course_notes/value-based/#steps","title":"Steps:","text":"<ol> <li>Generate \\(N\\) random points \\((x, y)\\) where \\(x, y \\sim U(-1,1)\\), meaning they are uniformly sampled in the square \\([-1,1] \\times [-1,1]\\).</li> <li>Define an indicator function \\(I(x, y)\\) that takes the value:</li> </ol> \\[ I(x, y) = \\begin{cases} 1, &amp; \\text{if } x^2 + y^2 \\leq 1 \\quad \\text{(inside the circle)} \\\\ 0, &amp; \\text{otherwise}. \\end{cases} \\] <p>Since each point is either inside or outside the circle, the variable \\(I(x, y)\\) follows a Bernoulli distribution with probability \\(p = \\frac{\\pi}{4}\\).</p> <ol> <li>Compute the proportion of points inside the circle. The expectation of \\(I(x, y)\\) gives:</li> </ol> \\[ \\mathbb{E}[I] = P(I = 1) = \\frac{\\pi}{4}. \\] <p>By the Law of Large Numbers (LLN), the sample mean of \\(I(x, y)\\) over \\(N\\) points converges to this expected value:</p> \\[ \\frac{\\text{Points inside the circle}}{\\text{Total points}} \\approx \\frac{\\pi}{4}. \\]"},{"location":"course_notes/value-based/#example-integration","title":"Example: Integration","text":"<p>Monte Carlo methods can also estimate definite integrals using random sampling. Given an integral:</p> \\[ I = \\int_a^b f(x) dx, \\] <p>we approximate it using Monte Carlo sampling:</p> \\[ \\hat{I}_N = \\frac{b-a}{N} \\sum_{i=1}^{N} f(x_i), \\] <p>where \\(x_i\\) are sampled uniformly from \\([a, b]\\). By the LLN, as \\(N \\to \\infty\\), the estimate \\(\\hat{I}_N\\) converges to the true integral.</p> <p>Source</p> <p>Watch on YouTube</p>"},{"location":"course_notes/value-based/#33-monte-carlo-prediction","title":"3.3. Monte Carlo Prediction","text":"<p>In reinforcement learning, an episode is a sequence of states, actions, and rewards that starts from an initial state and ends in a terminal state. Each episode represents a complete trajectory of the agent\u2019s interaction with the environment.</p> <p>The return for a time step \\(t\\) in an episode is the cumulative discounted reward:</p> \\[ G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\]"},{"location":"course_notes/value-based/#estimating-vpis","title":"Estimating \\(V^\\pi(s)\\)","text":"<p>Monte Carlo methods estimate the state value function \\(V^\\pi(s)\\) by averaging the returns observed after visiting state \\(s\\) in multiple episodes.</p> <p>The estimate of \\(V^\\pi(s)\\) is:</p> \\[ V^\\pi(s) = \\frac{1}{N(s)} \\sum_{i=1}^{N(s)} G_i \\] <p>where: - \\(N(s)\\) is the number of times state \\(s\\) has been visited. - \\(G_i\\) is the return observed from the \\(i\\)-th visit to state \\(s\\).</p> <p>Since Monte Carlo methods rely entirely on sampled episodes, they do not require knowledge of transition probabilities or rewards so they learn directly from experience.</p>"},{"location":"course_notes/value-based/#34-monte-carlo-control","title":"3.4. Monte Carlo Control","text":"<p>In Monte Carlo Control, the goal is to improve the policy \\(\\pi\\) by optimizing it based on the action-value function \\(Q^\\pi(s, a)\\). </p>"},{"location":"course_notes/value-based/#algorithm","title":"Algorithm:","text":"<ol> <li>Generate Episodes: Generate episodes by interacting with the environment under the current policy \\(\\pi\\).</li> <li>Compute Returns: For each state-action pair \\((s, a)\\) in the episode, compute the return \\(G_t\\) from that time step onward.</li> <li>Update Action-Value Function: For each state-action pair, update the action-value function as:</li> </ol> \\[  Q^\\pi(s, a) = \\frac{1}{N(s, a)} \\sum_{i=1}^{N(s, a)} G_i \\] <p>Where \\(N(s, a)\\) is the number of times the state-action pair \\((s, a)\\) has been visited. 4. Policy Improvement: After updating the action-value function, improve the policy by selecting the action that maximizes \\(Q^\\pi(s, a)\\) for each state \\(s\\):</p> \\[  \\pi'(s) = \\arg\\max_a Q^\\pi(s, a)  \\] <p>This method is used to optimize the policy iteratively, improving it by making the policy greedy with respect to the current action-value function.</p>"},{"location":"course_notes/value-based/#35-first-visit-vs-every-visit-monte-carlo","title":"3.5. First-Visit vs. Every-Visit Monte Carlo","text":"<p>There are two main variations of Monte Carlo methods for estimating value functions: First-Visit Monte Carlo and Every-Visit Monte Carlo.</p>"},{"location":"course_notes/value-based/#first-visit-monte-carlo","title":"First-Visit Monte Carlo:","text":"<p>In First-Visit Monte Carlo, the return for a state is only updated the first time it is visited in an episode. This approach helps avoid over-counting and ensures that the value estimate for each state is updated only once per episode.</p>"},{"location":"course_notes/value-based/#algorithm_1","title":"Algorithm:","text":"<ol> <li>Initialize \\(N(s) = 0\\) and \\(G(s) = 0\\) for all states.</li> <li>For each episode, visit each state \\(s\\) for the first time, and when it is first visited, add the return \\(G_t\\) to \\(G(s)\\) and increment \\(N(s)\\).</li> <li>After the episode, update the value estimate for each state as:  </li> </ol> \\[ V^\\pi(s) = \\frac{G(s)}{N(s)} \\]"},{"location":"course_notes/value-based/#every-visit-monte-carlo","title":"Every-Visit Monte Carlo:","text":"<p>In Every-Visit Monte Carlo, the return for each state is updated every time it is visited in an episode. This approach uses all occurrences of a state to update its value function, which can sometimes lead to more stable estimates.</p>"},{"location":"course_notes/value-based/#algorithm_2","title":"Algorithm:","text":"<ol> <li>Initialize \\(N(s) = 0\\) and \\(G(s) = 0\\) for all states.</li> <li>For each episode, visit each state \\(s\\), and every time it is visited, add the return \\(G_t\\) to \\(G(s)\\) and increment \\(N(s)\\).</li> <li>After the episode, update the value estimate for each state as:</li> </ol> \\[ V^\\pi(s) = \\frac{G(s)}{N(s)} \\]"},{"location":"course_notes/value-based/#comparison","title":"Comparison:","text":"<p>First-Visit Monte Carlo updates the value function only the first time a state is encountered in an episode, ensuring an unbiased estimate but using fewer samples, which can result in higher variance and slower learning. In contrast, Every-Visit Monte Carlo updates the value function on all occurrences of a state within an episode, reducing variance and improving sample efficiency by utilizing more data. Although it may introduce bias, it often converges faster, making it more practical in many applications.</p>"},{"location":"course_notes/value-based/#36-incremental-monte-carlo-policy","title":"3.6. Incremental Monte Carlo Policy","text":"<p>In addition to First-Visit and Every-Visit Monte Carlo, an alternative approach is the Incremental Monte Carlo Policy, which updates the value function incrementally after each visit instead of computing an average over all episodes. This method is more memory-efficient and allows real-time updates without storing past returns.</p> <p>Given the return \\(G_{i,t}\\) observed for state \\(s\\) at time \\(t\\) in episode \\(i\\), we update the value function as:</p> \\[ V^\\pi(s) = V^\\pi(s) \\frac{N(s) - 1}{N(s)} + \\frac{G_{i,t}}{N(s)} \\] <p>which can be rewritten as:</p> \\[ V^\\pi(s) = V^\\pi(s) + \\frac{1}{N(s)} (G_{i,t} - V^\\pi(s)) \\] <ul> <li>The update formula behaves like a running average, gradually incorporating new information.</li> </ul> <p>This approach ensures smooth updates, avoids storing all past returns, and is more computationally efficient, especially in long episodes or large state spaces.</p>"},{"location":"course_notes/value-based/#4-temporal-difference-td-learning","title":"4. Temporal Difference (TD) Learning","text":"<p>Temporal Difference (TD) Learning is a method for estimating value functions in reinforcement learning. </p>"},{"location":"course_notes/value-based/#41-td-prediction","title":"4.1. TD Prediction","text":"<p>TD Learning updates the value function using the Bellman equation. It differs from Monte Carlo methods in that it updates after each step rather than waiting for the entire episode to finish. The general TD update rule for state-value function \\(V^\\pi(s_t)\\) is:</p> \\[ V^\\pi(s_t) = V^\\pi(s_t) + \\alpha \\left[ R_{t+1} + \\gamma V^\\pi(s_{t+1}) - V^\\pi(s_t) \\right] \\] <p>This approach is called bootstrapping since it estimates future rewards based on the current value function rather than waiting for the full return.</p> <p>Just like for state-value functions, we can extend TD Learning to action-value functions (Q-values). The TD update rule for \\(Q(s_t, a_t)\\) is:</p> \\[ Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\left[ R_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right] \\] <p>This allows TD learning to be applied to control tasks, where the agent needs to improve its policy while learning. </p>"},{"location":"course_notes/value-based/#43-on-policy-vs-off-policy-td-learning","title":"4.3. On-Policy vs. Off-Policy TD Learning","text":"<p>TD methods can be used for both on-policy and off-policy learning:</p> <ul> <li>SARSA (On-Policy TD Control): Updates the action-value function based on the agent\u2019s actual policy.</li> <li>Q-Learning (Off-Policy TD Control): Updates based on the optimal action, regardless of the agent\u2019s current policy.</li> </ul>"},{"location":"course_notes/value-based/#sarsa-algorithm-on-policy","title":"SARSA Algorithm (On-Policy)","text":"<p>In SARSA, the agent chooses the next action \\(a_{t+1}\\) according to its current policy and updates the Q-value based on the immediate reward and the Q-value for the next state-action pair.</p> <pre><code>Initialize Q(s, a) arbitrarily, for all s \u2208 S, a \u2208 A(s), and Q(terminal-state, \u00b7) = 0\nRepeat (for each episode):\n    Initialize S\n    Choose A from S using policy derived from Q (e.g., \u025b-greedy)\n    Repeat (for each step of episode):\n         Take action A, observe R and next state S'\n         Choose A' from S' using policy derived from Q (e.g., \u025b-greedy)\n\n         Update Q-value:\n         Q(S, A) \u2190 Q(S, A) + \u03b1[R + \u03b3 * Q(S', A') - Q(S, A)]\n\n         Set S \u2190 S', A \u2190 A'\n\n    until S is terminal\n</code></pre>"},{"location":"course_notes/value-based/#q-learning-algorithm-off-policy","title":"Q-Learning Algorithm (Off-Policy)","text":"<p>Q-learning is an off-policy algorithm that learns the best action-value function, no matter what behavior policy the agent used to gather the data.</p> <pre><code>Initialize Q(s, a) arbitrarily, for all s \u2208 S, a \u2208 A(s), and Q(terminal-state, \u00b7) = 0\nRepeat (for each episode):\n    Initialize S\n\n    Repeat (for each step of episode):\n         Choose A from S using policy derived from Q (e.g., \u025b-greedy)\n\n         Take action A, observe R and next state S'\n\n         Update Q-value:\n         Q(S, A) \u2190 Q(S, A) + \u03b1[R + \u03b3 * max_a Q(S', A') - Q(S, A)]\n\n         Set S \u2190 S'\n\n    until S is terminal\n</code></pre>"},{"location":"course_notes/value-based/#44-exploitation-vs-exploration","title":"4.4. Exploitation vs Exploration","text":""},{"location":"course_notes/value-based/#balancing-exploration-and-exploitation","title":"Balancing Exploration and Exploitation","text":"<p>In reinforcement learning, an agent needs to balance exploration (trying new actions) and exploitation (using known actions that give good rewards). To do this, we use an \\(\\epsilon\\)-greedy policy:</p> \\[ \\pi(a_t | s_t) =  \\begin{cases}  \\arg\\max_a Q(s_t, a) &amp; \\text{with probability } 1 - \\epsilon \\\\ \\text{random action} &amp; \\text{with probability } \\epsilon \\end{cases} \\] <p>At the start of learning, \\(\\epsilon\\) is high to encourage exploration. As the agent learns more about the environment, \\(\\epsilon\\) decreases, allowing the agent to focus more on exploiting the best actions it has learned. This process is called epsilon decay.</p> <p>Common ways to decay \\(\\epsilon\\) include:</p> <ul> <li>Linear Decay: </li> </ul> \\[ \\epsilon_t = \\frac{1}{t} \\] <p>where \\(t\\) is the time step.</p> <ul> <li>Exponential Decay:</li> </ul> \\[ \\epsilon_t = \\epsilon_0 \\cdot \\text{decay_rate}^t \\] <p>Watch on YouTube</p>"},{"location":"course_notes/value-based/#5-summary-of-key-concepts-and-methods","title":"5. Summary of Key Concepts and Methods","text":"<p>In reinforcement learning, various methods are used to estimate value functions and find optimal policies. These methods can be broadly categorized into Model-Based and Model-Free learning, as well as On-Policy and Off-Policy learning. Below is a concise summary of these key concepts and a comparison of different approaches.</p>"},{"location":"course_notes/value-based/#51-model-based-vs-model-free-learning","title":"5.1. Model-Based vs. Model-Free Learning","text":""},{"location":"course_notes/value-based/#model-based-learning","title":"Model-Based Learning:","text":"<ul> <li>Definition: The agent uses a model of the environment to predict future states and rewards. The model allows the agent to simulate actions and outcomes.</li> <li>Example: Dynamic Programming (DP) relies on a complete model of the environment.</li> <li>Advantages: Efficient when the model is available and provides exact solutions when the environment is known.</li> </ul>"},{"location":"course_notes/value-based/#model-free-learning","title":"Model-Free Learning:","text":"<ul> <li>Definition: The agent learns directly from interactions with the environment by estimating value functions based on observed rewards, without needing a model.</li> <li>Examples: Monte Carlo (MC), Temporal Difference (TD).</li> <li>Advantages: More flexible, applicable when the model is unknown or too complex to compute.</li> </ul>"},{"location":"course_notes/value-based/#52-on-policy-vs-off-policy-learning","title":"5.2. On-Policy vs. Off-Policy Learning","text":""},{"location":"course_notes/value-based/#on-policy-learning","title":"On-Policy Learning:","text":"<ul> <li>Definition: The agent learns about and improves the policy it is currently following. The policy that generates the data is the same as the one being evaluated and improved.</li> <li>Example: SARSA updates based on actions taken under the current policy.</li> <li>Advantages: Simpler and guarantees that the agent learns from its own actions.</li> </ul>"},{"location":"course_notes/value-based/#off-policy-learning","title":"Off-Policy Learning:","text":"<ul> <li>Definition: The agent learns about an optimal policy while following a different behavior policy. The target policy is updated while the agent explores using a behavior policy.</li> <li>Example: Q-Learning updates based on the optimal action, independent of the behavior policy.</li> <li>Advantages: More flexible, allows for learning from past experiences and using different exploration strategies.</li> </ul>"},{"location":"course_notes/value-based/#53-comparison","title":"5.3. Comparison","text":"Feature TD (Temporal Difference) Monte Carlo (MC) Dynamic Programming (DP) Model Requirement No model required (model-free) No model required (model-free) Requires a full model of the environment Learning Method Updates based on current estimates (bootstrapping) Updates after complete episode (no bootstrapping) Updates based on exact model (transition probabilities) Update Frequency After each step After each episode After each step or full sweep over states Efficiency More sample efficient (incremental learning) Less efficient (requires full episodes) Very efficient, but needs a model Convergence Converges with sufficient exploration Converges with sufficient exploration Converges to optimal policy with a known model Suitability Works well in ongoing tasks Works well for episodic tasks Works well in fully known environments <p>The choice of method depends on the environment, the availability of a model, and the trade-off between exploration and exploitation.</p>"},{"location":"course_notes/value-based/#references","title":"References","text":"<ul> <li>Sutton, R.S., &amp; Barto, A.G. (2018). Reinforcement Learning: An Introduction (2nd ed.). MIT Press.</li> <li>monte-carlo for integration</li> </ul>"},{"location":"course_notes/value-based2/","title":"Week 7: Value-based Methods","text":""},{"location":"course_notes/value-based2/#1-bellman-equations-and-value-functions","title":"1. Bellman Equations and Value Functions","text":""},{"location":"course_notes/value-based2/#11-state-value-function-vs","title":"1.1. State Value Function \\(V(s)\\)","text":""},{"location":"course_notes/value-based2/#definition","title":"Definition:","text":"<p>The state value function \\(V^\\pi(s)\\) measures the expected return when an agent starts in state \\(s\\) and follows a policy \\(\\pi\\). It provides a scalar value for each state that reflects the desirability of that state under the given policy. Formally, it is defined as:</p> \\[ V^\\pi(s) = \\mathbb{E} \\left[ G_t \\mid s_t = s \\right] \\] <p>Where \\(G_t\\) represents the return (total reward) from time step \\(t\\) onwards:</p> \\[ G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\] <ul> <li>\\(R_t\\) is the reward received at time step \\(t\\).</li> <li>\\(\\gamma\\) is the discount factor (\\(0 \\leq \\gamma \\leq 1\\)), controlling how much future rewards are valued compared to immediate rewards.</li> </ul>"},{"location":"course_notes/value-based2/#bellman-expectation-equation-for-vpis","title":"Bellman Expectation Equation for \\(V^\\pi(s)\\):","text":"<p>The Bellman Expectation Equation for the state value function expresses the value of a state \\(s\\) in terms of the expected immediate reward and the discounted value of the next state. It is written as:</p> \\[ V^\\pi(s) = \\mathbb{E} \\left[ R_{t+1} + \\gamma V^\\pi(s_{t+1}) \\mid s_t = s \\right] \\] <p>Using the transition probabilities of the environment, this can be expanded as:</p> \\[ V^\\pi(s) = \\sum_{s'} P(s'|s, \\pi(s)) \\left[ R(s, \\pi(s), s') + \\gamma V^\\pi(s') \\right] \\] <p>Where: - \\(P(s'|s, \\pi(s))\\) is the probability of transitioning from state \\(s\\) to state \\(s'\\) when following action \\(\\pi(s)\\). - \\(R(s, \\pi(s), s')\\) is the reward for transitioning from state \\(s\\) to \\(s'\\) under action \\(\\pi(s)\\).</p> <p>This equation allows for the iterative computation of state values in a model-based setting.</p>"},{"location":"course_notes/value-based2/#12-action-value-function-qs-a","title":"1.2. Action Value Function \\(Q(s, a)\\)","text":""},{"location":"course_notes/value-based2/#definition_1","title":"Definition:","text":"<p>The action value function \\(Q^\\pi(s, a)\\) represents the expected return when an agent starts in state \\(s\\), takes action \\(a\\), and then follows policy \\(\\pi\\):</p> \\[ Q^\\pi(s, a) = \\mathbb{E} \\left[ G_t \\mid s_t = s, a_t = a \\right] \\] <p>Where \\(G_t\\) is the return starting at time \\(t\\).</p>"},{"location":"course_notes/value-based2/#bellman-expectation-equation-for-qpis-a","title":"Bellman Expectation Equation for \\(Q^\\pi(s, a)\\):","text":"<p>The Bellman Expectation Equation for the action value function is similar to the one for the state value function but includes both the action and the subsequent states and actions. It is given by:</p> \\[ Q^\\pi(s, a) = \\mathbb{E} \\left[ R_{t+1} + \\gamma Q^\\pi(s_{t+1}, a_{t+1}) \\mid s_t = s, a_t = a \\right] \\] <p>Expanding this into a sum over possible next states, we get:</p> \\[ Q^\\pi(s, a) = \\sum_{s'} P(s'|s, a) \\left[ R(s, a, s') + \\gamma \\sum_{a'} \\pi(a'|s') Q^\\pi(s', a') \\right] \\] <p>Where: - \\(P(s'|s, a)\\) is the transition probability from state \\(s\\) to state \\(s'\\) under action \\(a\\). - \\(\\pi(a'|s')\\) is the probability of taking action \\(a'\\) in state \\(s'\\) under policy \\(\\pi\\).</p>"},{"location":"course_notes/value-based2/#bellman-optimality-equation-for-qs-a","title":"Bellman Optimality Equation for \\(Q^*(s, a)\\):","text":"<p>The Bellman Optimality Equation for \\(Q^*(s, a)\\) expresses the optimal action value function. It is given by:</p> \\[ Q^*(s, a) = \\mathbb{E} \\left[ R_{t+1} + \\gamma \\max_{a'} Q^*(s_{t+1}, a') \\mid s_t = s, a_t = a \\right] \\] <p>This shows that the optimal action value at each state-action pair is the immediate reward plus the discounted maximum expected value from the next state, where the next action is chosen optimally.</p>"},{"location":"course_notes/value-based2/#2-dynamic-programming","title":"2. Dynamic Programming","text":"<p>Dynamic Programming (DP) is a powerful technique used to solve reinforcement learning problems where the environment is fully known (i.e., the model is available). DP algorithms compute the optimal policy and value functions by iteratively updating estimates based on a model of the environment. </p>"},{"location":"course_notes/value-based2/#21-value-iteration","title":"2.1. Value Iteration","text":""},{"location":"course_notes/value-based2/#bellman-optimality-equation","title":"Bellman Optimality Equation:","text":"<p>The Bellman Optimality Equation for the value function is:</p> \\[ V_{k+1}(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V_k(s') \\right] \\] <p>Where: - \\(\\max_a\\) selects the action \\(a\\) that maximizes the expected return from state \\(s\\).</p>"},{"location":"course_notes/value-based2/#value-iteration-algorithm","title":"Value Iteration Algorithm:","text":"<ol> <li>Initialize the value function \\(V_0(s)\\) arbitrarily.</li> <li> <p>Repeat until convergence:</p> <ul> <li>For each state \\(s\\), update the value function:</li> </ul> \\[ V_{k+1}(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V_k(s') \\right] \\] </li> <li> <p>Once the value function converges, the optimal policy \\(\\pi^*(s)\\) can be derived by selecting the action that maximizes the expected return:</p> </li> </ol> \\[ \\pi^*(s) = \\arg\\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V^*(s') \\right] \\]"},{"location":"course_notes/value-based2/#convergence","title":"Convergence:","text":"<p>Value Iteration is guaranteed to converge to the optimal value function and policy. The number of iterations required depends on the problem's dynamics, but it typically converges faster than Policy Iteration in terms of the number of iterations, though it may require more computation per iteration.</p>"},{"location":"course_notes/value-based2/#22-policy-evaluation","title":"2.2. Policy Evaluation","text":"<p>Policy Evaluation calculates the state value function \\(V^\\pi(s)\\) for a given policy \\(\\pi\\) by iteratively updating the value function using the Bellman Expectation Equation:</p> \\[ V^\\pi(s) = R(s, \\pi(s)) + \\gamma \\sum_{s'} P(s'|s, \\pi(s)) V^\\pi(s') \\] <p>This process is repeated until \\(V^\\pi(s)\\) converges to a fixed point for all s.</p>"},{"location":"course_notes/value-based2/#23-policy-improvement","title":"2.3. Policy Improvement","text":"<p>Policy Improvement refines a policy \\(\\pi\\) by making it greedy with respect to the current value function:</p> \\[ \\pi'(s) = \\arg\\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V^\\pi(s') \\right] \\] <p>It is proven that the new policy\u2019s value function is at least as good as the previous one:</p> \\[ V^{\\pi'}(s) \\geq V^\\pi(s), \\quad \\forall s. \\] <p>By repeating policy evaluation and improvement, the policy converges to the optimal one.</p> <ol> <li> <p>Single-Step Improvement: </p> <ul> <li>Modify the policy at only \\(t = 0\\), keeping the rest unchanged.  </li> <li>This new policy achieves a higher or equal value:  </li> </ul> \\[ V^{\\pi_{(k+1)}^{(1)}}(s) \\geq V^{\\pi_k}(s), \\quad \\forall s. \\] </li> <li> <p>Extending to Multiple Steps: </p> <ul> <li>Modify the policy at \\(t = 0\\) and \\(t = 1\\), keeping the rest unchanged.  </li> <li>Again, the value function improves: </li> </ul> \\[ V^{\\pi_{(k+1)}^{(2)}}(s) \\geq V^{\\pi_{(k+1)}^{(1)}}(s) \\geq V^{\\pi_k}(s). \\] </li> <li> <p>Repeating for All Steps: </p> <ul> <li>After applying this to all time steps, the final policy matches the fully improved one:  </li> </ul> \\[ \\pi_{(k+1)}^{(\\infty)}(s) = \\pi_{k+1}(s). \\] <ul> <li>This ensures:  </li> </ul> \\[ V^{\\pi_{k+1}}(s) \\geq V^{\\pi_k}(s), \\quad \\forall s. \\] </li> </ol> <p>The value function never decreases with each update.  </p>"},{"location":"course_notes/value-based2/#24-policy-iteration","title":"2.4. Policy Iteration","text":"<p>Policy Iteration alternates between policy evaluation and policy improvement to compute the optimal policy.</p> <ol> <li>Initialize policy \\(\\pi_0\\) randomly.</li> <li>Policy Evaluation: Compute the value function \\(V^{\\pi_k}(s)\\) for the current policy \\(\\pi_k\\) using the Bellman Expectation Equation.</li> <li>Policy Improvement: Update the policy \\(\\pi_{k+1}(s)\\) by making it greedy with respect to the current value function:</li> </ol> \\[  \\pi_{k+1}(s) = \\arg\\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V^{\\pi_k}(s') \\right]  \\] <ol> <li>Repeat the above steps until the policy converges (i.e., \\(\\pi_k = \\pi_{k+1}\\)).</li> </ol>"},{"location":"course_notes/value-based2/#convergence_1","title":"Convergence:","text":"<p>Each policy update ensures that the value function does not decrease.</p> <p>Since there are only a finite number of deterministic policies in a finite Markov Decision Process (MDP), the sequence of improving policies must eventually reach an policy \\(\\pi^*\\), where further improvement do not change it.</p> <p>The value function of the fixed point \\(\\pi^*\\) satisfy the Bellman Optimality Equation:</p> \\[ V^*(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s'|s, a) V^*(s') \\right] \\] <p>This confirms that the final policy \\(\\pi^*\\) is optimal.</p>"},{"location":"course_notes/value-based2/#25-comparison-of-policy-iteration-and-value-iteration","title":"2.5. Comparison of Policy Iteration and Value Iteration","text":"<p>Policy Iteration and Value Iteration are two dynamic programming methods for finding the optimal policy in an MDP. Both rely on iterative updates but differ in efficiency and computation.</p> Feature Policy Iteration Value Iteration Update Method Alternates between policy evaluation and improvement Updates value function directly Computational Cost Per Iteration \\(O(\\|S\\|^3)\\) (solving linear equations) \\(O(\\|S\\| \\cdot \\|A\\|)\\) (maximization over actions) Number of Iterations Fewer iterations, but each is expensive More iterations, but each is cheaper Best for Small state spaces, deterministic transitions Large state spaces, stochastic transitions <p>Policy Iteration explicitly computes the value function for a given policy, requiring solving a system of equations. Each iteration is computationally expensive but results in a significant improvement, leading to faster convergence in terms of iterations.</p> <p>Value Iteration avoids solving a system of equations by updating the value function incrementally. Each iteration is computationally cheaper, but because the value function is updated gradually, more iterations are needed for convergence.</p> <p>Thus, Policy Iteration takes fewer iterations but is computationally heavy per step, while Value Iteration takes more iterations but is computationally lighter per step.</p> <p>Watch on YouTube</p>"},{"location":"course_notes/value-based2/#3-monte-carlo-methods","title":"3. Monte Carlo Methods","text":""},{"location":"course_notes/value-based2/#31-planning-vs-learning-in-rl","title":"3.1. Planning vs. Learning in RL","text":"<p>Reinforcement learning can be approached in two ways: planning and learning. The main difference is that planning relies on a model of the environment, while learning uses real-world interactions to improve decision-making.</p>"},{"location":"course_notes/value-based2/#planning-model-based-rl","title":"Planning (Model-Based RL)","text":"<ul> <li>Uses a model to predict state transitions and rewards.</li> <li>The agent can simulate future actions without interacting with the environment.</li> <li>Examples: Dynamic Programming (DP), Monte Carlo Tree Search (MCTS).</li> </ul>"},{"location":"course_notes/value-based2/#learning-model-free-rl","title":"Learning (Model-Free RL)","text":"<ul> <li>No access to a model; the agent learns by interacting with the environment.</li> <li>The agent updates value estimates based on observed rewards.</li> <li>Examples: Monte Carlo, Temporal Difference (TD), Q-Learning.</li> </ul> <p>Planning is efficient when a reliable model is available, but learning is necessary when the model is unknown or too complex to compute.</p> <p>Monte Carlo methods fall under Model-Free RL, where the agent improves through experience. The following sections introduce how Monte Carlo Sampling is used to estimate value functions without needing a model.</p>"},{"location":"course_notes/value-based2/#32-introduction-to-monte-carlo","title":"3.2. Introduction to Monte Carlo","text":"<p>Monte Carlo methods use random sampling to estimate numerical results, especially when direct computation is infeasible or the underlying distribution is unknown. These methods are widely applied in physics, finance, optimization, and reinforcement learning.</p> <p>Monte Carlo estimates an expectation:</p> \\[ I = \\mathbb{E}[f(X)] = \\int f(x) p(x) dx \\] <p>using sample averaging:</p> \\[ \\hat{I}_N = \\frac{1}{N} \\sum_{i=1}^{N} f(x_i), \\] <p>where $ x_i $ are independent samples drawn from $ p(x) $. The Law of Large Numbers (LLN) ensures that as \\(N \\to \\infty\\):</p> \\[ \\hat{I}_N \\to I. \\] <p>This guarantees convergence, but the speed of convergence depends on the variance of the samples.</p> <p>Monte Carlo estimates become more accurate as \\(N\\) increases, but independent samples are crucial for unbiased estimation.</p> <p>By the Central Limit Theorem (CLT), for large \\(N\\), the Monte Carlo estimate follows a normal distribution:</p> \\[ \\hat{I}_N \\approx \\mathcal{N} \\left(I, \\frac{\\sigma^2}{N} \\right). \\] <p>This shows that the variance decreases at a rate of \\(O(1/N)\\), meaning that as the number of independent samples increases, the estimate becomes more stable. However, this reduction is slow, requiring a large number of samples to achieve high precision.</p>"},{"location":"course_notes/value-based2/#example-estimating-pi","title":"Example: Estimating \\(\\pi\\)","text":"<p>Monte Carlo methods can estimate \\(\\pi\\) by randomly sampling points and analyzing their distribution relative to a known geometric shape.</p>"},{"location":"course_notes/value-based2/#steps","title":"Steps:","text":"<ol> <li>Generate \\(N\\) random points \\((x, y)\\) where \\(x, y \\sim U(-1,1)\\), meaning they are uniformly sampled in the square \\([-1,1] \\times [-1,1]\\).</li> <li>Define an indicator function \\(I(x, y)\\) that takes the value:</li> </ol> \\[ I(x, y) = \\begin{cases} 1, &amp; \\text{if } x^2 + y^2 \\leq 1 \\quad \\text{(inside the circle)} \\\\ 0, &amp; \\text{otherwise}. \\end{cases} \\] <p>Since each point is either inside or outside the circle, the variable \\(I(x, y)\\) follows a Bernoulli distribution with probability \\(p = \\frac{\\pi}{4}\\).</p> <ol> <li>Compute the proportion of points inside the circle. The expectation of \\(I(x, y)\\) gives:</li> </ol> \\[ \\mathbb{E}[I] = P(I = 1) = \\frac{\\pi}{4}. \\] <p>By the Law of Large Numbers (LLN), the sample mean of \\(I(x, y)\\) over \\(N\\) points converges to this expected value:</p> \\[ \\frac{\\text{Points inside the circle}}{\\text{Total points}} \\approx \\frac{\\pi}{4}. \\]"},{"location":"course_notes/value-based2/#example-integration","title":"Example: Integration","text":"<p>Monte Carlo methods can also estimate definite integrals using random sampling. Given an integral:</p> \\[ I = \\int_a^b f(x) dx, \\] <p>we approximate it using Monte Carlo sampling:</p> \\[ \\hat{I}_N = \\frac{b-a}{N} \\sum_{i=1}^{N} f(x_i), \\] <p>where \\(x_i\\) are sampled uniformly from \\([a, b]\\). By the LLN, as \\(N \\to \\infty\\), the estimate \\(\\hat{I}_N\\) converges to the true integral.</p> <p>Source</p> <p>Watch on YouTube</p>"},{"location":"course_notes/value-based2/#33-monte-carlo-prediction","title":"3.3. Monte Carlo Prediction","text":"<p>In reinforcement learning, an episode is a sequence of states, actions, and rewards that starts from an initial state and ends in a terminal state. Each episode represents a complete trajectory of the agent\u2019s interaction with the environment.</p> <p>The return for a time step \\(t\\) in an episode is the cumulative discounted reward:</p> \\[ G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\]"},{"location":"course_notes/value-based2/#estimating-vpis","title":"Estimating \\(V^\\pi(s)\\)","text":"<p>Monte Carlo methods estimate the state value function \\(V^\\pi(s)\\) by averaging the returns observed after visiting state \\(s\\) in multiple episodes.</p> <p>The estimate of \\(V^\\pi(s)\\) is:</p> \\[ V^\\pi(s) = \\frac{1}{N(s)} \\sum_{i=1}^{N(s)} G_i \\] <p>where: - \\(N(s)\\) is the number of times state \\(s\\) has been visited. - \\(G_i\\) is the return observed from the \\(i\\)-th visit to state \\(s\\).</p> <p>Since Monte Carlo methods rely entirely on sampled episodes, they do not require knowledge of transition probabilities or rewards so they learn directly from experience.</p>"},{"location":"course_notes/value-based2/#34-monte-carlo-control","title":"3.4. Monte Carlo Control","text":"<p>In Monte Carlo Control, the goal is to improve the policy \\(\\pi\\) by optimizing it based on the action-value function \\(Q^\\pi(s, a)\\). </p>"},{"location":"course_notes/value-based2/#algorithm","title":"Algorithm:","text":"<ol> <li>Generate Episodes: Generate episodes by interacting with the environment under the current policy \\(\\pi\\).</li> <li>Compute Returns: For each state-action pair \\((s, a)\\) in the episode, compute the return \\(G_t\\) from that time step onward.</li> <li>Update Action-Value Function: For each state-action pair, update the action-value function as:</li> </ol> \\[  Q^\\pi(s, a) = \\frac{1}{N(s, a)} \\sum_{i=1}^{N(s, a)} G_i \\] <p>Where \\(N(s, a)\\) is the number of times the state-action pair \\((s, a)\\) has been visited. 4. Policy Improvement: After updating the action-value function, improve the policy by selecting the action that maximizes \\(Q^\\pi(s, a)\\) for each state \\(s\\):</p> \\[  \\pi'(s) = \\arg\\max_a Q^\\pi(s, a)  \\] <p>This method is used to optimize the policy iteratively, improving it by making the policy greedy with respect to the current action-value function.</p>"},{"location":"course_notes/value-based2/#35-first-visit-vs-every-visit-monte-carlo","title":"3.5. First-Visit vs. Every-Visit Monte Carlo","text":"<p>There are two main variations of Monte Carlo methods for estimating value functions: First-Visit Monte Carlo and Every-Visit Monte Carlo.</p>"},{"location":"course_notes/value-based2/#first-visit-monte-carlo","title":"First-Visit Monte Carlo:","text":"<p>In First-Visit Monte Carlo, the return for a state is only updated the first time it is visited in an episode. This approach helps avoid over-counting and ensures that the value estimate for each state is updated only once per episode.</p>"},{"location":"course_notes/value-based2/#algorithm_1","title":"Algorithm:","text":"<ol> <li>Initialize \\(N(s) = 0\\) and \\(G(s) = 0\\) for all states.</li> <li>For each episode, visit each state \\(s\\) for the first time, and when it is first visited, add the return \\(G_t\\) to \\(G(s)\\) and increment \\(N(s)\\).</li> <li>After the episode, update the value estimate for each state as:  </li> </ol> \\[ V^\\pi(s) = \\frac{G(s)}{N(s)} \\]"},{"location":"course_notes/value-based2/#every-visit-monte-carlo","title":"Every-Visit Monte Carlo:","text":"<p>In Every-Visit Monte Carlo, the return for each state is updated every time it is visited in an episode. This approach uses all occurrences of a state to update its value function, which can sometimes lead to more stable estimates.</p>"},{"location":"course_notes/value-based2/#algorithm_2","title":"Algorithm:","text":"<ol> <li>Initialize \\(N(s) = 0\\) and \\(G(s) = 0\\) for all states.</li> <li>For each episode, visit each state \\(s\\), and every time it is visited, add the return \\(G_t\\) to \\(G(s)\\) and increment \\(N(s)\\).</li> <li>After the episode, update the value estimate for each state as:</li> </ol> \\[ V^\\pi(s) = \\frac{G(s)}{N(s)} \\]"},{"location":"course_notes/value-based2/#comparison","title":"Comparison:","text":"<p>First-Visit Monte Carlo updates the value function only the first time a state is encountered in an episode, ensuring an unbiased estimate but using fewer samples, which can result in higher variance and slower learning. In contrast, Every-Visit Monte Carlo updates the value function on all occurrences of a state within an episode, reducing variance and improving sample efficiency by utilizing more data. Although it may introduce bias, it often converges faster, making it more practical in many applications.</p>"},{"location":"course_notes/value-based2/#36-incremental-monte-carlo-policy","title":"3.6. Incremental Monte Carlo Policy","text":"<p>In addition to First-Visit and Every-Visit Monte Carlo, an alternative approach is the Incremental Monte Carlo Policy, which updates the value function incrementally after each visit instead of computing an average over all episodes. This method is more memory-efficient and allows real-time updates without storing past returns.</p> <p>Given the return \\(G_{i,t}\\) observed for state \\(s\\) at time \\(t\\) in episode \\(i\\), we update the value function as:</p> \\[ V^\\pi(s) = V^\\pi(s) \\frac{N(s) - 1}{N(s)} + \\frac{G_{i,t}}{N(s)} \\] <p>which can be rewritten as:</p> \\[ V^\\pi(s) = V^\\pi(s) + \\frac{1}{N(s)} (G_{i,t} - V^\\pi(s)) \\] <ul> <li>The update formula behaves like a running average, gradually incorporating new information.</li> </ul> <p>This approach ensures smooth updates, avoids storing all past returns, and is more computationally efficient, especially in long episodes or large state spaces.</p>"},{"location":"course_notes/value-based2/#4-temporal-difference-td-learning","title":"4. Temporal Difference (TD) Learning","text":"<p>Temporal Difference (TD) Learning is a method for estimating value functions in reinforcement learning. </p>"},{"location":"course_notes/value-based2/#41-td-prediction","title":"4.1. TD Prediction","text":"<p>TD Learning updates the value function using the Bellman equation. It differs from Monte Carlo methods in that it updates after each step rather than waiting for the entire episode to finish. The general TD update rule for state-value function \\(V^\\pi(s_t)\\) is:</p> \\[ V^\\pi(s_t) = V^\\pi(s_t) + \\alpha \\left[ R_{t+1} + \\gamma V^\\pi(s_{t+1}) - V^\\pi(s_t) \\right] \\] <p>This approach is called bootstrapping since it estimates future rewards based on the current value function rather than waiting for the full return.</p> <p>Just like for state-value functions, we can extend TD Learning to action-value functions (Q-values). The TD update rule for \\(Q(s_t, a_t)\\) is:</p> \\[ Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\left[ R_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right] \\] <p>This allows TD learning to be applied to control tasks, where the agent needs to improve its policy while learning. </p>"},{"location":"course_notes/value-based2/#43-on-policy-vs-off-policy-td-learning","title":"4.3. On-Policy vs. Off-Policy TD Learning","text":"<p>TD methods can be used for both on-policy and off-policy learning:</p> <ul> <li>SARSA (On-Policy TD Control): Updates the action-value function based on the agent\u2019s actual policy.</li> <li>Q-Learning (Off-Policy TD Control): Updates based on the optimal action, regardless of the agent\u2019s current policy.</li> </ul>"},{"location":"course_notes/value-based2/#sarsa-algorithm-on-policy","title":"SARSA Algorithm (On-Policy)","text":"<p>In SARSA, the agent chooses the next action \\(a_{t+1}\\) according to its current policy and updates the Q-value based on the immediate reward and the Q-value for the next state-action pair.</p> <pre><code>Initialize Q(s, a) arbitrarily, for all s \u2208 S, a \u2208 A(s), and Q(terminal-state, \u00b7) = 0\nRepeat (for each episode):\n    Initialize S\n    Choose A from S using policy derived from Q (e.g., \u025b-greedy)\n    Repeat (for each step of episode):\n         Take action A, observe R and next state S'\n         Choose A' from S' using policy derived from Q (e.g., \u025b-greedy)\n\n         Update Q-value:\n         Q(S, A) \u2190 Q(S, A) + \u03b1[R + \u03b3 * Q(S', A') - Q(S, A)]\n\n         Set S \u2190 S', A \u2190 A'\n\n    until S is terminal\n</code></pre>"},{"location":"course_notes/value-based2/#q-learning-algorithm-off-policy","title":"Q-Learning Algorithm (Off-Policy)","text":"<p>Q-learning is an off-policy algorithm that learns the best action-value function, no matter what behavior policy the agent used to gather the data.</p> <pre><code>Initialize Q(s, a) arbitrarily, for all s \u2208 S, a \u2208 A(s), and Q(terminal-state, \u00b7) = 0\nRepeat (for each episode):\n    Initialize S\n\n    Repeat (for each step of episode):\n         Choose A from S using policy derived from Q (e.g., \u025b-greedy)\n\n         Take action A, observe R and next state S'\n\n         Update Q-value:\n         Q(S, A) \u2190 Q(S, A) + \u03b1[R + \u03b3 * max_a Q(S', A') - Q(S, A)]\n\n         Set S \u2190 S'\n\n    until S is terminal\n</code></pre>"},{"location":"course_notes/value-based2/#44-exploitation-vs-exploration","title":"4.4. Exploitation vs Exploration","text":""},{"location":"course_notes/value-based2/#balancing-exploration-and-exploitation","title":"Balancing Exploration and Exploitation","text":"<p>In reinforcement learning, an agent needs to balance exploration (trying new actions) and exploitation (using known actions that give good rewards). To do this, we use an \\(\\epsilon\\)-greedy policy:</p> \\[ \\pi(a_t | s_t) =  \\begin{cases}  \\arg\\max_a Q(s_t, a) &amp; \\text{with probability } 1 - \\epsilon \\\\ \\text{random action} &amp; \\text{with probability } \\epsilon \\end{cases} \\] <p>At the start of learning, \\(\\epsilon\\) is high to encourage exploration. As the agent learns more about the environment, \\(\\epsilon\\) decreases, allowing the agent to focus more on exploiting the best actions it has learned. This process is called epsilon decay.</p> <p>Common ways to decay \\(\\epsilon\\) include:</p> <ul> <li>Linear Decay: </li> </ul> \\[ \\epsilon_t = \\frac{1}{t} \\] <p>where \\(t\\) is the time step.</p> <ul> <li>Exponential Decay:</li> </ul> \\[ \\epsilon_t = \\epsilon_0 \\cdot \\text{decay_rate}^t \\] <p>Watch on YouTube</p>"},{"location":"course_notes/value-based2/#5-summary-of-key-concepts-and-methods","title":"5. Summary of Key Concepts and Methods","text":"<p>In reinforcement learning, various methods are used to estimate value functions and find optimal policies. These methods can be broadly categorized into Model-Based and Model-Free learning, as well as On-Policy and Off-Policy learning. Below is a concise summary of these key concepts and a comparison of different approaches.</p>"},{"location":"course_notes/value-based2/#51-model-based-vs-model-free-learning","title":"5.1. Model-Based vs. Model-Free Learning","text":""},{"location":"course_notes/value-based2/#model-based-learning","title":"Model-Based Learning:","text":"<ul> <li>Definition: The agent uses a model of the environment to predict future states and rewards. The model allows the agent to simulate actions and outcomes.</li> <li>Example: Dynamic Programming (DP) relies on a complete model of the environment.</li> <li>Advantages: Efficient when the model is available and provides exact solutions when the environment is known.</li> </ul>"},{"location":"course_notes/value-based2/#model-free-learning","title":"Model-Free Learning:","text":"<ul> <li>Definition: The agent learns directly from interactions with the environment by estimating value functions based on observed rewards, without needing a model.</li> <li>Examples: Monte Carlo (MC), Temporal Difference (TD).</li> <li>Advantages: More flexible, applicable when the model is unknown or too complex to compute.</li> </ul>"},{"location":"course_notes/value-based2/#52-on-policy-vs-off-policy-learning","title":"5.2. On-Policy vs. Off-Policy Learning","text":""},{"location":"course_notes/value-based2/#on-policy-learning","title":"On-Policy Learning:","text":"<ul> <li>Definition: The agent learns about and improves the policy it is currently following. The policy that generates the data is the same as the one being evaluated and improved.</li> <li>Example: SARSA updates based on actions taken under the current policy.</li> <li>Advantages: Simpler and guarantees that the agent learns from its own actions.</li> </ul>"},{"location":"course_notes/value-based2/#off-policy-learning","title":"Off-Policy Learning:","text":"<ul> <li>Definition: The agent learns about an optimal policy while following a different behavior policy. The target policy is updated while the agent explores using a behavior policy.</li> <li>Example: Q-Learning updates based on the optimal action, independent of the behavior policy.</li> <li>Advantages: More flexible, allows for learning from past experiences and using different exploration strategies.</li> </ul>"},{"location":"course_notes/value-based2/#53-comparison","title":"5.3. Comparison","text":"Feature TD (Temporal Difference) Monte Carlo (MC) Dynamic Programming (DP) Model Requirement No model required (model-free) No model required (model-free) Requires a full model of the environment Learning Method Updates based on current estimates (bootstrapping) Updates after complete episode (no bootstrapping) Updates based on exact model (transition probabilities) Update Frequency After each step After each episode After each step or full sweep over states Efficiency More sample efficient (incremental learning) Less efficient (requires full episodes) Very efficient, but needs a model Convergence Converges with sufficient exploration Converges with sufficient exploration Converges to optimal policy with a known model Suitability Works well in ongoing tasks Works well for episodic tasks Works well in fully known environments <p>The choice of method depends on the environment, the availability of a model, and the trade-off between exploration and exploitation.</p>"},{"location":"course_notes/value-based2/#references","title":"References","text":"<ul> <li>Sutton, R.S., &amp; Barto, A.G. (2018). Reinforcement Learning: An Introduction (2nd ed.). MIT Press.</li> <li>monte-carlo for integration</li> </ul>"},{"location":"exams/","title":"Previous Semesters","text":""},{"location":"exams/#spring-2025-exams","title":"Spring 2025 Exams","text":"Exam Download Midterm Midterm , Video Final_1 Final_1 Final_2 Final_2"},{"location":"exams/#spring-2024-exams","title":"Spring 2024 Exams","text":"Exam Download Quizzes Quizzes Midterm Midterm Final Final"},{"location":"exams/#spring-2023-exams","title":"Spring 2023 Exams","text":"Exam Download Quizzes Quizzes Midterm Midterm Final Final"},{"location":"exams/final/","title":"Spring 2025 Final","text":""},{"location":"exams/final/#final-exam-1","title":"Final Exam 1","text":"<p>Download Exam</p>"},{"location":"exams/final/#final-exam-2","title":"Final Exam 2","text":"<p>Download Exam</p>"},{"location":"exams/midterm/","title":"Spring 2025 Midterm","text":""},{"location":"exams/midterm/#midterm-exam","title":"Midterm Exam","text":"<p>Download Exam</p>"},{"location":"exams/midterm/#midterm-qa","title":"Midterm QA","text":""},{"location":"exams/midterm/#screen-record","title":"Screen Record","text":""},{"location":"guests/abhishek_gupta/","title":"Abhishek Gupta","text":""},{"location":"guests/abhishek_gupta/#about","title":"About","text":"<p>Abhishek Gupta is an Assistant Professor in the Paul G. Allen School of Computer Science and Engineering at the University of Washington, where he leads the Washington Embodied Intelligence and Robotics Development (WEIRD) Lab. His research focuses on enabling robotic systems to learn complex tasks in unstructured, human-centric environments such as homes and offices. Gupta works on real-world reinforcement learning with themes like deployment-time adaptation, human-in-the-loop learning, and leveraging off-domain data from videos, simulations, and generative models. He earned his Ph.D. in machine learning and robotics at UC Berkeley, advised by Sergey Levine and Pieter Abbeel, and later held a postdoctoral position at MIT, collaborating with Russ Tedrake and Pulkit Agrawal. His broader research interests include offline RL, dexterous manipulation, meta-learning, safe and scalable adaptation, and the development of foundation models for embodied AI. Read more</p>"},{"location":"guests/abhishek_gupta/#lecture","title":"Lecture","text":""},{"location":"guests/abhishek_gupta/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/adam_white/","title":"Adam White","text":""},{"location":"guests/adam_white/#about","title":"About","text":"<p>Adam White is an Assistant Professor at the University of Alberta in the Department of Computing Science and a Canada CIFAR AI Chair as well as Principal Investigator of the Reinforcement Learning &amp; Artificial Intelligence Lab (RLAI), and also serves as a Senior Research Scientist at DeepMind. His research focuses on the theoretical and empirical foundations of reinforcement learning\u2014particularly continual learning, knowledge representation, intrinsic motivation, and deployment in real-world control systems\u2014blending work in simulated environments, robotics, and industrial applications. He co-created influential tools and architectures like RL-Glue and Horde for scalable, real-time RL experimentation, co-developed the acclaimed Reinforcement Learning Specialization on Coursera used by tens of thousands, and has received accolades including \u201cPaper of Distinction\u201d and best paper awards in robotics conferences. Read more</p>"},{"location":"guests/adam_white/#lecture","title":"Lecture","text":""},{"location":"guests/adam_white/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/amy_zhang/","title":"Amy Zhang","text":""},{"location":"guests/amy_zhang/#about","title":"About","text":"<p>Amy Zhang is an Assistant Professor of Electrical &amp; Computer Engineering at UT\u202fAustin and a Texas Instruments/Kilby Fellow, studying reinforcement learning with an emphasis on sample efficiency, generalization, and state abstraction in both simulation and real-world robotics. Her work bridges theory and practice to develop robust RL algorithms for sequential decision-making; she also leads UT\u2019s MIDI lab and recently secured an Army Research Office award for integrating combinatorial generalization with RL methods. Read more</p>"},{"location":"guests/amy_zhang/#lecture","title":"Lecture","text":""},{"location":"guests/amy_zhang/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/anne_collins/","title":"Anne Collins","text":""},{"location":"guests/anne_collins/#about","title":"About","text":"<p>Anne\u202fG.\u202fE.\u202fCollins is an Associate Professor in the Department of Psychology at UC\u202fBerkeley and a faculty member in the Helen Wills Neuroscience Institute, where she leads the Computational Cognitive Neuroscience (CCN) Lab. Her research develops and tests computational models of human learning, decision-making, and executive functions\u2014especially how reinforcement learning, working memory, and neural signals interact\u2014to understand flexible, adaptive cognition. Using behavioral experiments, EEG, imaging, and sophisticated modeling (e.g., Bayesian inference, neural networks), her work has revealed how fast working memory systems can interfere with slower but more durable reinforcement learning, with implications for AI and psychiatry.</p>"},{"location":"guests/anne_collins/#lecture","title":"Lecture","text":""},{"location":"guests/anne_collins/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/benjamin_eysenbach/","title":"Benjamin Eysenbach","text":""},{"location":"guests/benjamin_eysenbach/#about","title":"About","text":"<p>Benjamin Eysenbach is an Assistant Professor of Computer Science at Princeton University, where he leads the Princeton Reinforcement Learning Lab. His research focuses on designing reinforcement learning (RL) algorithms that learn intelligent behaviors through self-supervised methods, eliminating the need for explicit rewards or human supervision. Eysenbach's work emphasizes simplicity, scalability, and robustness, contributing to advancements in goal-conditioned RL and contrastive learning techniques. He earned his Ph.D. in Machine Learning from Carnegie Mellon University, advised by Ruslan Salakhutdinov and Sergey Levine, and held research positions at Google Brain. His academic journey began with undergraduate studies in mathematics at MIT. At Princeton, Eysenbach teaches courses on reinforcement learning and probabilistic inference, and his lab's research has been featured in leading conferences such as ICLR and NeurIPS. Read more</p>"},{"location":"guests/benjamin_eysenbach/#lecture","title":"Lecture","text":""},{"location":"guests/benjamin_eysenbach/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/benjamin_van_roy/","title":"Benjamin Van Roy","text":""},{"location":"guests/benjamin_van_roy/#about","title":"About","text":"<p>Benjamin Van Roy is a Professor at Stanford University, where he has been on the faculty since 1998, and a leading expert in reinforcement learning, with research focused on the design and analysis of intelligent agents. He also founded and leads the Efficient Agent Team at Google DeepMind and has directed research initiatives at Morgan Stanley, Unica (acquired by IBM), and Enuvis (acquired by SiRF), which he co-founded. Van Roy holds SB, SM, and PhD degrees from MIT in Electrical Engineering and Computer Science, with his doctoral work advised by John N. Tsitsiklis. A Fellow of both INFORMS and IEEE, he has served on the editorial boards of several top journals in machine learning and operations research. His contributions have been recognized with numerous awards, including the NSF CAREER Award, the INFORMS Frederick W. Lanchester Prize, and multiple teaching honors from Stanford. He has advised dozens of PhD students who have gone on to influential roles in academia, industry, and finance. Read more</p>"},{"location":"guests/benjamin_van_roy/#lecture","title":"Lecture","text":""},{"location":"guests/benjamin_van_roy/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/chris_watkins/","title":"Chris Watkins","text":""},{"location":"guests/chris_watkins/#about","title":"About","text":"<p>Professor Christopher Watkins is a world-class authority on reinforcement learning and evolutionary theory. He is a Professor of Machine Learning at the Department of Computer Science, Royal Holloway, University of London. Renowned for his foundational contributions to artificial intelligence, Professor Watkins introduced the Q-learning algorithm, a pivotal breakthrough that laid the groundwork for modern reinforcement learning. Read more</p>"},{"location":"guests/chris_watkins/#lecture","title":"Lecture","text":""},{"location":"guests/chris_watkins/#slides","title":"Slides","text":"<p>Download Slides</p> <p>Download Slides</p>"},{"location":"guests/christopher_amato/","title":"Christopher Amato","text":""},{"location":"guests/christopher_amato/#about","title":"About","text":"<p>Christopher Amato is an Associate Professor at Northeastern University where he leads the Lab for Learning and Planning in Robotics. He has published many papers in leading artificial intelligence, machine learning and robotics conferences (including winning a best paper prize at AAMAS-14 and being nominated for the best paper at RSS-15, AAAI-19, AAMAS-21 and MRS-21). He has also successfully co-organized several tutorials on multi-agent coordination and has co-authored a book on the subject. He has also won several awards such as Amazon Research Awards and an NSF CAREER Award. His research focuses on reinforcement learning in partially observable and multi-agent/multi-robot systems. Read more</p>"},{"location":"guests/christopher_amato/#lecture","title":"Lecture","text":""},{"location":"guests/christopher_amato/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/ian_osband/","title":"Ian Osband","text":""},{"location":"guests/ian_osband/#about","title":"About","text":"<p>Ian Osband is a researcher in artificial intelligence, focusing on decision-making under uncertainty, particularly in reinforcement learning (RL). He is known for his work on efficient exploration strategies, notably through the development of randomized value functions, as detailed in his Ph.D. thesis, \"Deep Exploration via Randomized Value Functions,\" which earned second place in the national Dantzig dissertation award . Osband completed his Ph.D. at Stanford University under the supervision of Benjamin Van Roy, following undergraduate studies in mathematics at Oxford University. He has held research positions at DeepMind and OpenAI, where he contributed to advancements in RL algorithms and their applications. His work has significantly influenced both theoretical and practical aspects of reinforcement learning, particularly in the areas of exploration and uncertainty estimation. Read more</p>"},{"location":"guests/ian_osband/#lecture","title":"Lecture","text":""},{"location":"guests/ida_momennejad/","title":"Ida Momennejad","text":""},{"location":"guests/ida_momennejad/#about","title":"About","text":"<p>Ida Momennejad is a Principal Researcher at Microsoft Research NYC, where she develops and evaluates generative AI systems inspired by her work in cognitive neuroscience, reinforcement learning, and NeuroAI, with a focus on how humans and machines build internal world models for memory, exploration, and planning. She designs brain- and behavior-inspired algorithms\u2014combining reinforcement learning, neural networks, large language models, machine learning, behavioral experiments, fMRI, and electrophysiology\u2014with applications ranging from AI for Xbox gaming to foundational advances in AI reasoning. Beyond her research, she co-hosts The Learning Salon every Friday at 4 PM ET, mentors creative scientists at the New Museum\u2019s New Inc incubator, and shares her insights on the Microsoft Research AI Frontiers podcast and other outlets. Academically, she earned a BSc in software engineering in Tehran, an MSc in Philosophy of Science in Utrecht, and a PhD in psychology from the Bernstein Center for Computational Neuroscience in Berlin, followed by postdoctoral research at Princeton University and earlier work at Columbia University\u2019s Electrophysiology, Memory, and Navigation Lab. Read more</p>"},{"location":"guests/ida_momennejad/#lecture","title":"Lecture","text":""},{"location":"guests/jakob_foerster/","title":"Jakob Foerster","text":""},{"location":"guests/jakob_foerster/#about","title":"About","text":"<p>Jakob Foerster is an Associate Professor in the Department of Engineering Science at the University of Oxford and a Research Scientist at FAIR, Meta AI. He is also a Supernumerary Fellow at St Anne's College, Oxford. Foerster leads the FLAIR (Foundations of Learning, AI, and Robotics) research group, focusing on multi-agent reinforcement learning, human-AI coordination, and the broader impact of AI on society and science. His work has influenced both academia and industry, with applications spanning finance, bioengineering, and large-scale AI systems. Read more</p>"},{"location":"guests/jakob_foerster/#lecture","title":"Lecture","text":""},{"location":"guests/jakob_foerster/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/jeff_clune/","title":"Jeff Clune","text":""},{"location":"guests/jeff_clune/#about","title":"About","text":"<p>Jeff Clune is a Professor of Computer Science at the University of British Columbia, a Canada CIFAR AI Chair at the Vector Institute, and a Senior Research Advisor at DeepMind. His research centers on deep learning and deep reinforcement learning, with a particular focus on open-ended learning and the evolution of intelligence. Previously, he held research leadership roles at OpenAI and Uber AI Labs\u2014where he was a founding member\u2014and was a faculty member at the University of Wyoming and a research scientist at Cornell. Clune earned his Ph.D. and master\u2019s degrees from Michigan State University and his bachelor\u2019s from the University of Michigan. He has received numerous prestigious honors, including the Presidential Early Career Award for Scientists and Engineers, an NSF CAREER award, and multiple best paper and Test of Time awards at top conferences like NeurIPS, CVPR, ICLR, and ICML. His work has been featured in premier journals such as Nature, Science, and PNAS, and widely covered by major media outlets including The New York Times, BBC, and National Geographic. Read more</p>"},{"location":"guests/jeff_clune/#lecture","title":"Lecture","text":""},{"location":"guests/jeff_clune/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/karl_friston/","title":"Karl Friston","text":""},{"location":"guests/karl_friston/#about","title":"About","text":"<p>Karl Friston is a theoretical neuroscientist and authority on brain imaging. He invented statistical parametric mapping (SPM), voxel-based morphometry (VBM) and dynamic causal modelling (DCM). These contributions were motivated by schizophrenia research and theoretical studies of value-learning, formulated as the dysconnection hypothesis of schizophrenia. Mathematical contributions include variational Laplacian procedures and generalized filtering for hierarchical Bayesian model inversion. Friston currently works on models of functional integration in the human brain and the principles that underlie neuronal interactions. His main contribution to theoretical neurobiology is a free-energy principle for action and perception (active inference). Friston received the first Young Investigators Award in Human Brain Mapping (1996) and was elected a Fellow of the Academy of Medical Sciences (1999). In 2000 he was President of the international Organization of Human Brain Mapping. In 2003 he was awarded the Minerva Golden Brain Award and was elected a Fellow of the Royal Society in 2006. In 2008 he received a Medal, College de France and an Honorary Doctorate from the University of York in 2011. He became of Fellow of the Royal Society of Biology in 2012, received the Weldon Memorial prize and Medal in 2013 for contributions to mathematical biology and was elected as a member of EMBO (excellence in the life sciences) in 2014 and the Academia Europaea in (2015). He was the 2016 recipient of the Charles Branch Award for unparalleled breakthroughs in Brain Research and the Glass Brain Award, a lifetime achievement award in the field of human brain mapping. He holds Honorary Doctorates from the Universities of York, Zurich and Radboud University. Read more</p>"},{"location":"guests/karl_friston/#lecture","title":"Lecture","text":""},{"location":"guests/karl_friston/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/luis_serrano/","title":"Luis Serrano","text":""},{"location":"guests/luis_serrano/#about","title":"About","text":"<p>Luis Serrano is the founder of Serrano Academy\u2014a popular educational YouTube channel with over 150K subscribers\u2014where he makes complex math and machine learning intuitively accessible. He authored the highly praised Grokking Machine Learning (Manning, 2021), designed to teach core ML concepts using only high-school math and intuitive examples. Previously, he led AI education efforts as a machine learning engineer at Google (working on YouTube recommendations), Head of AI content at Udacity, and Lead AI Educator at Apple. Serrano holds a PhD in mathematics from the University of Michigan, an MSc and BSc from the University of Waterloo, and completed a postdoc in quantum machine learning at Universit\u00e9 du Qu\u00e9bec \u00e0 Montr\u00e9al. Read more</p>"},{"location":"guests/luis_serrano/#lecture","title":"Lecture","text":""},{"location":"guests/mark_ho/","title":"Mark Ho","text":""},{"location":"guests/mark_ho/#about","title":"About","text":"<p>Mark Ho is an Assistant Professor of Psychology at New York University and an affiliated faculty member at NYU\u2019s Center for Data Science. His research integrates cognitive science, social psychology, neuroscience, and computer science to develop computational theories of how people\u2019s goals, values, and motivations shape their thoughts, decisions, and social interactions. Ho's work focuses on understanding the principles underlying human problem-solving and social cognition, aiming to inform the design of intelligent systems that interact effectively with humans. He earned his Ph.D. in Cognitive Science and M.S. in Computer Science from Brown University, followed by postdoctoral research at Princeton University. His research has been published in leading journals, including Nature, where he explored how people construct simplified mental representations to plan. Read more</p>"},{"location":"guests/mark_ho/#lecture","title":"Lecture","text":""},{"location":"guests/mark_ho/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/marlos_machado/","title":"Marlos C. Machado","text":""},{"location":"guests/marlos_machado/#about","title":"About","text":"<p>Marlos\u202fC. Machado is an Assistant Professor in the Department of Computing Science at the University of Alberta, a Canada CIFAR AI Chair and Amii Fellow, and a Principal Investigator in the Reinforcement Learning &amp; Artificial Intelligence (RLAI) group. His research focuses on deep reinforcement learning, representation learning, continual learning, and real-world applications\u2014emphasizing algorithms that autonomously discover temporal abstractions (\u201ceigenoptions\u201d) via the successor representation. During his Ph.D., he introduced stochasticity and game modes in the Arcade Learning Environment and popularized eigenoptions; post-Ph.D., he spent four years at DeepMind and Google Brain, contributing to real-world systems like stratospheric balloon control. His work, featured in top venues such as Nature, JMLR, NeurIPS, ICML, and ICLR, as well as media outlets like BBC, Bloomberg, The Verge, and Wired, continues to influence foundational advances in exploration and abstraction in RL. Read more</p>"},{"location":"guests/marlos_machado/#lecture","title":"Lecture","text":""},{"location":"guests/marlos_machado/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/martha_white/","title":"Martha White","text":""},{"location":"guests/martha_white/#about","title":"About","text":"<p>Martha White is an Associate Professor of Computing Science at the University of Alberta, a Canada CIFAR AI Chair, and a Fellow at Amii, where she leads research on adaptive reinforcement learning agents capable of continual learning in real-world environments. Her work focuses on representation learning\u2014employing sparse and recurrent neural architectures\u2014and off-policy methods that allow agents to learn from streams of data, supporting applications such as control of industrial processes like water treatment systems. She also plays a key role in co-founding RL CORE with Adam White, has delivered invited talks at top venues like ICLR, ICML, and NeurIPS, and has earned notable distinctions including IEEE\u2019s \u201cAI\u2019s 10 to Watch\u201d award. Read more</p>"},{"location":"guests/martha_white/#lecture","title":"Lecture","text":""},{"location":"guests/martha_white/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/michael_littman/","title":"Michael Littman","text":""},{"location":"guests/michael_littman/#about","title":"About","text":"<p>Michael L. Littman is a University Professor of Computer Science at Brown University, studying machine learning and decision making under uncertainty. He has earned multiple university-level awards for teaching and his research on reinforcement learning, probabilistic planning, and automated crossword-puzzle solving has been recognized with three best-paper awards and three influential paper awards. Littman is co-director of Brown's Humanity Centered Robotics Initiative and a Fellow of the Association for the Advancement of Artificial Intelligence and the Association for Computing Machinery. He is also a Fellow of the American Association for the Advancement of Science Leshner Leadership Institute for Public Engagement with Science, focusing on Artificial Intelligence. Read more</p>"},{"location":"guests/michael_littman/#lecture","title":"Lecture","text":""},{"location":"guests/michael_littman/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/nan_jiang/","title":"Nan Jiang","text":""},{"location":"guests/nan_jiang/#about","title":"About","text":"<p>Nan Jiang (\u59dc\u6960) is an Associate Professor in the Department of Computer Science at the University of Illinois at Urbana-Champaign (UIUC). His research focuses on building the theoretical foundations of reinforcement learning (RL), particularly in the function-approximation setting, with an emphasis on developing sample-efficient algorithms by drawing from statistical learning theory. He earned his Ph.D. in Computer Science and Engineering from the University of Michigan and was a postdoctoral researcher at Microsoft Research New York City before joining UIUC. Jiang's contributions have been recognized with several honors, including the NSF CAREER Award, a Sloan Research Fellowship, and a Google Research Scholar Award. He also serves as an action editor for the Journal of Machine Learning Research and an editor for Foundations and Trends in Machine Learning. Beyond research, he is deeply committed to education and mentorship, having received multiple teaching awards at UIUC, and his work continues to shape both theoretical and applied aspects of reinforcement learning. Read more</p>"},{"location":"guests/nan_jiang/#lecture","title":"Lecture","text":""},{"location":"guests/nan_jiang/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/pascal_poupart/","title":"Pascal Poupart","text":""},{"location":"guests/pascal_poupart/#about","title":"About","text":"<p>Pascal Poupart is a Professor in the David R. Cheriton School of Computer Science at the University of Waterloo (Canada). He is also a Canada CIFAR AI Chair at the Vector Institute and a member of the Waterloo AI Institute. He serves on the advisory board of the NSF AI Institute for Advances in Optimization (2022-present) at Georgia Tech, UC Berkeley and University of Southern California. He served as Research Director and Principal Research Scientist at the Waterloo Borealis AI Research Lab at the Royal Bank of Canada (2018-2020). He also served as scientific advisor for ProNavigator (2017-2019), ElementAI (2017-2018) and DialPad (2017-2018). His research focuses on the development of algorithms for Machine Learning with application to Natural Language Processing and Material Discovery. He is most well-known for his contributions to the development of Reinforcement Learning algorithms. Notable projects that his research team is currently working on include democratizing large language models, inverse constraint learning, mean field RL, RL foundation models, Bayesian federated learning, uncertainty quantification, probabilistic deep learning, conversational agents, transcription error correction, sport analytics, and material discovery for CO2 recycling. Read more</p>"},{"location":"guests/pascal_poupart/#lecture","title":"Lecture","text":""},{"location":"guests/pascal_poupart/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/peter_dayan/","title":"Peter Dayan","text":""},{"location":"guests/peter_dayan/#about","title":"About","text":"<p>Peter Dayan is a Director at the Max Planck Institute for Biological Cybernetics in T\u00fcbingen and a leading figure in theoretical neuroscience and artificial intelligence. He studied mathematics at Cambridge University and earned his Ph.D. from the University of Edinburgh, followed by postdoctoral work at the Salk Institute and the University of Toronto. After serving as an assistant professor at MIT, he co-founded the renowned Gatsby Computational Neuroscience Unit in London and led it as Director from 2002 to 2017. His research explores decision-making in the brain, the roles of neuromodulators, and computational models of psychiatric disorders, placing him at the intersection of neuroscience and AI. Dayan has received numerous accolades, including the Rumelhart Prize, the Brain Prize, and Germany\u2019s prestigious Alexander von Humboldt Professorship. He is a Fellow of both the Royal Society and the AAAS, and continues to shape our understanding of how brains and machines learn and make decisions. Read more</p>"},{"location":"guests/peter_dayan/#lecture","title":"Lecture","text":""},{"location":"guests/peter_dayan/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/peter_norvig/","title":"Peter Norvig","text":""},{"location":"guests/peter_norvig/#about","title":"About","text":"<p>Peter Norvig (born December\u202f14,\u202f1956) is an American computer scientist, Distinguished Education Fellow at Stanford\u2019s Human\u2011Centered AI Institute, and Director of Research at Google, previously overseeing Google\u2019s core search algorithms and broader research initiatives. He co-authored Artificial Intelligence: A Modern Approach\u2014the leading AI textbook used at over 1,500 universities worldwide\u2014and Paradigms of AI Programming, and authored influential essays such as \u201cThe Unreasonable Effectiveness of Data\u201d. Before Google, he led the Computational Sciences Division at NASA Ames, earning the NASA Exceptional Achievement Award in 2001, and held roles at Junglee, Sun Microsystems Labs, USC, and UC\u202fBerkeley. A Fellow of AAAI, ACM, the California Academy of Sciences, and the American Academy of Arts &amp; Sciences, Norvig also pioneered AI education by teaching a 160,000\u2011student online AI class, helping launch MOOCs via Udacity and edX. Read more</p>"},{"location":"guests/peter_norvig/#lecture","title":"Lecture","text":""},{"location":"guests/peter_stone/","title":"Peter Stone","text":""},{"location":"guests/peter_stone/#about","title":"About","text":"<p>Peter Stone is an American computer scientist who holds the Truchard Foundation Chair of Computer Science at The University of Texas at Austin. He is also Chief Scientist of Sony AI, an Alfred P. Sloan Research Fellow, Guggenheim Fellow, AAAI Fellow, IEEE Fellow, AAAS Fellow, ACM Fellow, and Fulbright Scholar. Read more</p>"},{"location":"guests/peter_stone/#lecture","title":"Lecture","text":""},{"location":"guests/richard_sutton/","title":"Richard Sutton","text":""},{"location":"guests/richard_sutton/#about","title":"About","text":"<p>Richard S. Sutton is a Canadian computer scientist. He is a professor of computing science at the University of Alberta and a research scientist at Keen Technologies. Sutton is considered one of the founders of modern computational reinforcement learning, having several significant contributions to the field, including temporal difference learning and policy gradient methods. Read more on Wikipedia</p>"},{"location":"guests/richard_sutton/#lecture","title":"Lecture","text":""},{"location":"guests/richard_sutton/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"guests/wolfram_schultz/","title":"Wolfram Schultz","text":""},{"location":"guests/wolfram_schultz/#about","title":"About","text":"<p>Wolfram Schultz FRS is a Professor of Neuroscience at the University of Cambridge and a Wellcome Principal Research Fellow, renowned for his groundbreaking discovery that dopamine neurons encode reward prediction errors\u2014a foundational insight linking neuroscience with reinforcement learning and economic decision theory. After earning his medical degree from the University of Heidelberg and a PhD in physiology from the University of Fribourg, he conducted postdoctoral research in Germany, the USA, and Sweden. Schultz's research focuses on how the brain processes reward, utility, risk, and decision-making, with particular attention to dopamine neurons, the striatum, orbitofrontal cortex, and amygdala. His work has significantly advanced the fields of neuroeconomics and computational neuroscience, earning him prestigious honors such as the Brain Prize, the Gruber Prize in Neuroscience, and election as a Fellow of the Royal Society. Read more</p>"},{"location":"guests/wolfram_schultz/#lecture","title":"Lecture","text":""},{"location":"guests/wolfram_schultz/#slides","title":"Slides","text":"<p>Download Slides</p>"},{"location":"home/calender/","title":"Calender","text":"<p>This Google Calendar outlines the schedule for the Deep Reinforcement Learning course, including weekly lecture times, assignment deadlines, and other important events.</p>"},{"location":"homeworks/","title":"Previous Semesters","text":""},{"location":"homeworks/#spring-2024-homeworks","title":"Spring 2024 Homeworks","text":"Homework # Problems Solutions HW1 HW1 Problems HW1 Solutions HW2 HW2 Problems HW2 Solutions HW3 HW3 Problems HW3 Solutions HW4 HW4 Problems HW4 Solutions"},{"location":"homeworks/#spring-2023-homeworks","title":"Spring 2023 Homeworks","text":"Homework # Problems Solutions HW0 HW0 Problems HW0 Solutions HW1 HW1 Problems HW1 Solutions HW2 HW2 Problems HW2 Solutions HW3 HW3 Problems HW3 Solutions HW4 HW4 Problems HW4 Solutions"},{"location":"homeworks/week1/","title":"HW1: Introduction to RL","text":"<p>Welcome to the first homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures and workshops.</p>"},{"location":"homeworks/week1/#homework","title":"Homework","text":"<p>You can download the homework questions and the notebook from the following links:</p> <p>HW1 Questions</p> <p>HW1 Notebook</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week1/#explanation","title":"Explanation","text":""},{"location":"homeworks/week1/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f2\u06f8 \u0628\u0647\u0645\u0646 (February 16) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"homeworks/week1/#solution","title":"Solution","text":"<p>HW1 Solution Notebook HW1 Solution Slides</p>"},{"location":"homeworks/week10/","title":"HW10: Exploration Methods","text":"<p>Welcome to the 10th homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures, recitation, and workshop.</p>"},{"location":"homeworks/week10/#homework","title":"Homework","text":"<p>You can download the homework questions from the following link:</p> <p>HW10 Questions</p> <p>HW10 Notebook 1 HW10 Notebook 2</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week10/#explanation","title":"Explanation","text":""},{"location":"homeworks/week10/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f1\u06f6 \u062e\u0631\u062f\u0627\u062f (June 6) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"homeworks/week11/","title":"HW11: Imitation &amp; Inverse RL","text":"<p>Welcome to the 11th homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures, recitation, and workshop.</p>"},{"location":"homeworks/week11/#homework","title":"Homework","text":"<p>You can download the homework questions from the following link:</p> <p>HW11 Questions</p> <p>HW11 Notebook</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week11/#explanation","title":"Explanation","text":""},{"location":"homeworks/week11/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f1\u06f3 \u062a\u06cc\u0631 (July 4) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"homeworks/week12/","title":"HW12: Offline Methods","text":"<p>Welcome to the 12th homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures, recitation, and workshop.</p>"},{"location":"homeworks/week12/#homework","title":"Homework","text":"<p>You can download the homework questions from the following link:</p> <p>HW12 Questions</p> <p>HW12 Notebook</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week12/#explanation","title":"Explanation","text":""},{"location":"homeworks/week12/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f1\u06f3 \u062a\u06cc\u0631 (July 4) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"homeworks/week13/","title":"HW13: Multi-Agent Methods","text":"<p>Welcome to the 13th homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures, recitation, and workshop.</p>"},{"location":"homeworks/week13/#homework","title":"Homework","text":"<p>You can download the homework questions from the following link:</p> <p>HW13 Questions</p> <p>HW13 Notebook 1 HW13 Notebook 2</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week13/#explanation","title":"Explanation","text":""},{"location":"homeworks/week13/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f2\u06f0 \u0634\u0647\u0631\u06cc\u0648\u0631 (September 11) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"homeworks/week14/","title":"HW14: Hierarchical &amp; Meta RL","text":"<p>Welcome to the 14th homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures, recitation, and workshop.</p>"},{"location":"homeworks/week14/#homework","title":"Homework","text":"<p>You can download the homework questions from the following link:</p> <p>HW14 Questions</p> <p>HW14 Notebook</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week14/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f2\u06f0 \u0634\u0647\u0631\u06cc\u0648\u0631 (September 11) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"homeworks/week2/","title":"HW2: Value-Based Methods","text":"<p>Welcome to the second homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures and workshops.</p>"},{"location":"homeworks/week2/#homework","title":"Homework","text":"<p>You can download the homework questions and the notebook from the following links:</p> <p>HW2 Questions</p> <p>HW2 Notebook 1 HW2 Notebook 2</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week2/#explanation","title":"Explanation","text":""},{"location":"homeworks/week2/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f1\u06f2 \u0627\u0633\u0641\u0646\u062f (March 2) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"homeworks/week2/#solution","title":"Solution","text":"<p>HW2 Solution to Questions</p> <p>HW2 Solution Notebook | Part 1: SARSA(n) and Q-Learning(n)</p> <p>HW2 Solution Notebook | Part 2: DQN vs DDQN</p>"},{"location":"homeworks/week3/","title":"HW3: Policy-Based Methods","text":"<p>Welcome to the third homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures and workshops.</p>"},{"location":"homeworks/week3/#homework","title":"Homework","text":"<p>You can download the homework questions and the notebook from the following links:</p> <p>HW3 Questions</p> <p>HW3 Notebook 1 HW3 Notebook 2 HW3 Notebook 3 HW3 Notebook 4</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week3/#explanation","title":"Explanation","text":""},{"location":"homeworks/week3/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f1\u06f2 \u0627\u0633\u0641\u0646\u062f (March 2) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"homeworks/week3/#solution","title":"Solution","text":"<p>HW3 Solution to Questions</p> <p>HW3 Solution Notebook | Part 1: REINFORCE vs GA</p> <p>HW3 Solution Notebook | Part 2: REINFORCE in CartPole</p> <p>HW3 Solution Notebook | Part 3: REINFORCE in Mountain Car</p> <p>HW3 Solution Notebook | Part 4: REINFORCE vs DQN</p>"},{"location":"homeworks/week4/","title":"HW4: Advanced Methods","text":"<p>Welcome to the forth homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures and workshops.</p>"},{"location":"homeworks/week4/#homework","title":"Homework","text":"<p>You can download the homework questions and the notebook from the following links:</p> <p>HW4 Questions</p> <p>HW4 Notebook 1 HW4 Notebook 2</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week4/#explanation","title":"Explanation","text":""},{"location":"homeworks/week4/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f2\u06f4 \u0627\u0633\u0641\u0646\u062f (March 14) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"homeworks/week4/#solution","title":"Solution","text":"<p>HW4 Solution to Questions</p> <p>HW4 Solution Notebook | Part 1: PPO Continuous</p> <p>HW4 Solution Notebook | Part 2: SAC &amp; DDPG Continuous</p>"},{"location":"homeworks/week5/","title":"HW5: Model-Based Methods","text":"<p>Welcome to the fifth homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures and workshops.</p>"},{"location":"homeworks/week5/#homework","title":"Homework","text":"<p>You can download the homework questions and the notebook from the following links:</p> <p>HW5 Questions</p> <p>HW5 Notebook 1 HW5 Notebook 2 HW5 Notebook 3</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week5/#explanation","title":"Explanation","text":""},{"location":"homeworks/week5/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f8 \u0641\u0631\u0648\u0631\u062f\u06cc\u0646 (March 28) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"homeworks/week5/#solution","title":"Solution","text":"<p>HW5 Solution to Questions</p> <p>HW5 Solution Notebook | Part 1: MCTS</p> <p>HW5 Solution Notebook | Part 2: Dyna</p> <p>HW5 Solution Notebook | Part 3: MPC</p>"},{"location":"homeworks/week6/","title":"HW6: Multi-Armed Bandits","text":"<p>Welcome to the sixth homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures and workshops.</p>"},{"location":"homeworks/week6/#homework","title":"Homework","text":"<p>You can download the homework questions and the notebook from the following links:</p> <p>HW6 Questions</p> <p>HW6 Notebook</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week6/#explanation","title":"Explanation","text":""},{"location":"homeworks/week6/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f1\u06f7 \u0641\u0631\u0648\u0631\u062f\u06cc\u0646 (April 6) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"homeworks/week6/#solution","title":"Solution","text":"<p>HW6 Solution to Questions</p> <p>HW6 Solution Notebook</p>"},{"location":"homeworks/week7/","title":"HW7: Value-Based Theory","text":"<p>Welcome to the 7th homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures and recitation.</p>"},{"location":"homeworks/week7/#homework","title":"Homework","text":"<p>You can download the homework questions from the following link:</p> <p>HW7 Questions</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week7/#explanation","title":"Explanation","text":""},{"location":"homeworks/week7/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f2\u06f1 \u0627\u0631\u062f\u06cc\u0628\u0647\u0634\u062a (May 11) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"homeworks/week8/","title":"HW8: Policy-Based Theory","text":"<p>Welcome to the 8th homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures and recitation.</p>"},{"location":"homeworks/week8/#homework","title":"Homework","text":"<p>You can download the homework questions from the following link:</p> <p>HW8 Questions</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week8/#explanation","title":"Explanation","text":""},{"location":"homeworks/week8/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f2\u06f1 \u0627\u0631\u062f\u06cc\u0628\u0647\u0634\u062a (May 11) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"homeworks/week9/","title":"HW9: Advanced Theory","text":"<p>Welcome to the 9th homework assignment of the Deep Reinforcement Learning course! We are excited to see you apply the concepts learned in the lectures and recitation.</p>"},{"location":"homeworks/week9/#homework","title":"Homework","text":"<p>You can download the homework questions from the following link:</p> <p>HW9 Questions</p> <p>You can use this template for your answers.</p>"},{"location":"homeworks/week9/#explanation","title":"Explanation","text":""},{"location":"homeworks/week9/#submission","title":"Submission","text":"<p>Please submit your completed homework on Quera by the deadline:</p> <ul> <li> <p>Submission Link: Quera Course Page</p> </li> <li> <p>Deadline: \u06f1\u06f6 \u062e\u0631\u062f\u0627\u062f (June 6) at 11:59 PM</p> </li> </ul> <p>Good luck, and we look forward to your submissions!</p>"},{"location":"journal_club/","title":"Introduction","text":""},{"location":"lectures/","title":"Introduction","text":""},{"location":"lectures/lecture1/","title":"Lecture1","text":""},{"location":"lectures/lecture1/#introduction-to-rl","title":"Introduction to RL","text":""},{"location":"lectures/lecture1/#screen-record","title":"Screen Record","text":""},{"location":"lectures/lecture1/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides</p>"},{"location":"lectures/lecture2/","title":"Lecture2","text":""},{"location":"lectures/lecture2/#introduction-to-rl","title":"Introduction to RL","text":""},{"location":"lectures/lecture2/#screen-record","title":"Screen Record","text":""},{"location":"lectures/lecture2/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides</p>"},{"location":"lectures/week10/","title":"Week 10: Exploration Methods","text":""},{"location":"lectures/week10/#lecture-19","title":"Lecture 19","text":""},{"location":"lectures/week10/#screen-camera","title":"Screen + Camera","text":""},{"location":"lectures/week10/#screen-record","title":"Screen Record","text":""},{"location":"lectures/week10/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated) Download Slides (Extra)</p>"},{"location":"lectures/week10/#lecture-summary","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week10/#lecture-quiz","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week10/#lecture-20","title":"Lecture 20","text":""},{"location":"lectures/week10/#screen-camera_1","title":"Screen + Camera","text":""},{"location":"lectures/week10/#screen-record_1","title":"Screen Record","text":""},{"location":"lectures/week10/#lecture-slides_1","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated) Download Slides (Extra)</p>"},{"location":"lectures/week10/#lecture-summary_1","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week10/#lecture-quiz_1","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week11/","title":"Week 11: Imitation &amp; Inverse RL","text":""},{"location":"lectures/week11/#lecture-21","title":"Lecture 21","text":""},{"location":"lectures/week11/#screen-camera","title":"Screen + Camera","text":""},{"location":"lectures/week11/#screen-record","title":"Screen Record","text":""},{"location":"lectures/week11/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated) Download Slides (Extra)</p>"},{"location":"lectures/week11/#lecture-summary","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week11/#lecture-quiz","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week11/#lecture-22","title":"Lecture 22","text":""},{"location":"lectures/week11/#screen-record_1","title":"Screen Record","text":""},{"location":"lectures/week11/#lecture-slides_1","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week11/#lecture-summary_1","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week11/#lecture-quiz_1","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week12/","title":"Week 12: Offline Methods","text":""},{"location":"lectures/week12/#lecture-23","title":"Lecture 23","text":""},{"location":"lectures/week12/#screen-camera","title":"Screen + Camera","text":""},{"location":"lectures/week12/#screen-record","title":"Screen Record","text":""},{"location":"lectures/week12/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week12/#lecture-summary","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week12/#lecture-quiz","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week12/#lecture-24","title":"Lecture 24","text":""},{"location":"lectures/week12/#screen-camera_1","title":"Screen + Camera","text":""},{"location":"lectures/week12/#screen-record_1","title":"Screen Record","text":""},{"location":"lectures/week12/#lecture-slides_1","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated) Download Slides (Extra)</p>"},{"location":"lectures/week12/#lecture-summary_1","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week12/#lecture-quiz_1","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week13/","title":"Week 13: Multi-Agent Methods","text":""},{"location":"lectures/week13/#lecture-25","title":"Lecture 25","text":""},{"location":"lectures/week13/#screen-camera","title":"Screen + Camera","text":""},{"location":"lectures/week13/#screen-record","title":"Screen Record","text":""},{"location":"lectures/week13/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week13/#lecture-summary","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week13/#lecture-quiz","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week13/#lecture-26","title":"Lecture 26","text":""},{"location":"lectures/week13/#screen-camera_1","title":"Screen + Camera","text":""},{"location":"lectures/week13/#screen-record_1","title":"Screen Record","text":""},{"location":"lectures/week13/#lecture-slides_1","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week13/#lecture-summary_1","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week13/#lecture-quiz_1","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week14/","title":"Week 14: Hierarchical &amp; Meta RL","text":""},{"location":"lectures/week14/#lecture-27","title":"Lecture 27","text":""},{"location":"lectures/week14/#screen-camera","title":"Screen + Camera","text":""},{"location":"lectures/week14/#screen-record","title":"Screen Record","text":""},{"location":"lectures/week14/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated) Download Slides (Extra)</p>"},{"location":"lectures/week14/#lecture-summary","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week14/#lecture-quiz","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week14/#lecture-28","title":"Lecture 28","text":""},{"location":"lectures/week14/#screen-camera_1","title":"Screen + Camera","text":""},{"location":"lectures/week14/#screen-record_1","title":"Screen Record","text":""},{"location":"lectures/week14/#lecture-slides_1","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week14/#lecture-summary_1","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week14/#lecture-quiz_1","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week15/","title":"Week 15: Guest Lectures","text":""},{"location":"lectures/week15/#lecture-29","title":"Lecture 29","text":""},{"location":"lectures/week15/#lecture-30","title":"Lecture 30","text":""},{"location":"lectures/week2/","title":"Week 2: Value-Based Methods","text":""},{"location":"lectures/week2/#lecture-3","title":"Lecture 3","text":""},{"location":"lectures/week2/#screen-camera","title":"Screen + Camera","text":""},{"location":"lectures/week2/#screen-record","title":"Screen Record","text":""},{"location":"lectures/week2/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week2/#lecture-summary","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week2/#lecture-quiz","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week2/#lecture-4","title":"Lecture 4","text":""},{"location":"lectures/week2/#screen-camera_1","title":"Screen + Camera","text":""},{"location":"lectures/week2/#screen-record_1","title":"Screen Record","text":""},{"location":"lectures/week2/#lecture-slides_1","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week2/#lecture-summary_1","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week2/#lecture-quiz_1","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week3/","title":"Week 3: Policy-Based Methods","text":""},{"location":"lectures/week3/#lecture-5","title":"Lecture 5","text":""},{"location":"lectures/week3/#screen-camera","title":"Screen + Camera","text":""},{"location":"lectures/week3/#screen-record","title":"Screen Record","text":""},{"location":"lectures/week3/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week3/#lecture-summary","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week3/#lecture-quiz","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week3/#lecture-6","title":"Lecture 6","text":""},{"location":"lectures/week3/#screen-camera_1","title":"Screen + Camera","text":"<p>This lecture was held online due to the university being closed.</p>"},{"location":"lectures/week3/#screen-record_1","title":"Screen Record","text":""},{"location":"lectures/week3/#lecture-slides_1","title":"Lecture Slides","text":"<p>Download Slides</p>"},{"location":"lectures/week3/#lecture-summary_1","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week3/#lecture-quiz_1","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week4/","title":"Week 4: Advanced Methods","text":""},{"location":"lectures/week4/#lecture-7","title":"Lecture 7","text":""},{"location":"lectures/week4/#screen-camera","title":"Screen + Camera","text":""},{"location":"lectures/week4/#screen-record","title":"Screen Record","text":""},{"location":"lectures/week4/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week4/#lecture-summary","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week4/#lecture-quiz","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week4/#lecture-8","title":"Lecture 8","text":""},{"location":"lectures/week4/#screen-camera_1","title":"Screen + Camera","text":""},{"location":"lectures/week4/#screen-record_1","title":"Screen Record","text":""},{"location":"lectures/week4/#lecture-slides_1","title":"Lecture Slides","text":"<p>Download Slides</p>"},{"location":"lectures/week4/#lecture-summary_1","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week4/#lecture-quiz_1","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week5/","title":"Week 5: Model-Based Methods","text":""},{"location":"lectures/week5/#lecture-9","title":"Lecture 9","text":""},{"location":"lectures/week5/#screen-camera","title":"Screen + Camera","text":""},{"location":"lectures/week5/#screen-record","title":"Screen Record","text":""},{"location":"lectures/week5/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week5/#lecture-summary","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week5/#lecture-quiz","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week5/#lecture-10","title":"Lecture 10","text":""},{"location":"lectures/week5/#screen-camera_1","title":"Screen + Camera","text":""},{"location":"lectures/week5/#screen-record_1","title":"Screen Record","text":""},{"location":"lectures/week5/#lecture-slides_1","title":"Lecture Slides","text":"<p>Download Slides</p>"},{"location":"lectures/week5/#lecture-summary_1","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week5/#lecture-quiz_1","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week6/","title":"Week 6: Multi-Armed Bandits","text":""},{"location":"lectures/week6/#lecture-11","title":"Lecture 11","text":""},{"location":"lectures/week6/#screen-record","title":"Screen Record","text":""},{"location":"lectures/week6/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides</p>"},{"location":"lectures/week6/#lecture-summary","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week6/#lecture-quiz","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week6/#lecture-12","title":"Lecture 12","text":""},{"location":"lectures/week6/#screen-record_1","title":"Screen Record","text":""},{"location":"lectures/week6/#lecture-slides_1","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week6/#lecture-summary_1","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week6/#lecture-quiz_1","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week7/","title":"Week 7: Value-Based Theory","text":""},{"location":"lectures/week7/#lecture-13","title":"Lecture 13","text":""},{"location":"lectures/week7/#screen-camera","title":"Screen + Camera","text":""},{"location":"lectures/week7/#screen-record","title":"Screen Record","text":""},{"location":"lectures/week7/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated) Download Slides (Extra)</p>"},{"location":"lectures/week7/#lecture-summary","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week7/#lecture-quiz","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week7/#lecture-14","title":"Lecture 14","text":""},{"location":"lectures/week7/#screen-camera_1","title":"Screen + Camera","text":""},{"location":"lectures/week7/#screen-record_1","title":"Screen Record","text":""},{"location":"lectures/week7/#lecture-slides_1","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week7/#lecture-summary_1","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week8/","title":"Week 8: Policy-Based Theory","text":""},{"location":"lectures/week8/#lecture-15","title":"Lecture 15","text":""},{"location":"lectures/week8/#screen-camera","title":"Screen + Camera","text":""},{"location":"lectures/week8/#screen-record","title":"Screen Record","text":""},{"location":"lectures/week8/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides</p>"},{"location":"lectures/week8/#lecture-summary","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week8/#lecture-quiz","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week8/#lecture-16","title":"Lecture 16","text":""},{"location":"lectures/week8/#screen-camera_1","title":"Screen + Camera","text":""},{"location":"lectures/week8/#screen-record_1","title":"Screen Record","text":""},{"location":"lectures/week8/#lecture-slides_1","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated) Download Slides (Extra)</p>"},{"location":"lectures/week8/#lecture-summary_1","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week8/#lecture-quiz_1","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week9/","title":"Week 9: Advanced Theory","text":""},{"location":"lectures/week9/#lecture-17","title":"Lecture 17","text":""},{"location":"lectures/week9/#screen-camera","title":"Screen + Camera","text":""},{"location":"lectures/week9/#screen-record","title":"Screen Record","text":""},{"location":"lectures/week9/#lecture-slides","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week9/#lecture-summary","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week9/#lecture-quiz","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"lectures/week9/#lecture-18","title":"Lecture 18","text":""},{"location":"lectures/week9/#screen-camera_1","title":"Screen + Camera","text":""},{"location":"lectures/week9/#screen-record_1","title":"Screen Record","text":""},{"location":"lectures/week9/#lecture-slides_1","title":"Lecture Slides","text":"<p>Download Slides Download Slides (Annotated)</p>"},{"location":"lectures/week9/#lecture-summary_1","title":"Lecture Summary","text":"<p>Download Summary</p>"},{"location":"lectures/week9/#lecture-quiz_1","title":"Lecture Quiz","text":"<p>Download Quiz</p>"},{"location":"poster_session/","title":"Introduction","text":"<p>The purpose of the poster session is for you to dive into a reinforcement learning topic of your choice, enjoy the learning process, and share your enthusiasm with classmates and the wider community! This is your chance to explore something that excites you\u2014whether it's a cutting-edge algorithm, a unique application, or an open research question\u2014and present it in a way that sparks curiosity. So, have fun, keep experimenting, and let your passion shine through!</p>"},{"location":"poster_session/#research-topic-proposal","title":"Research Topic Proposal","text":"<p>Before you begin designing your poster, please submit a concise proposal outlining your chosen reinforcement learning topic. This ensures that all submissions maintain high-quality standards and helps you focus your research effectively.</p> <ul> <li>Template: Use the Proposal Template to structure your submission.  </li> <li>Submission: Upload your completed proposal to Quera (submit here) by June\u00a06.</li> </ul>"},{"location":"poster_session/#main-grading-components","title":"Main Grading Components","text":"<p>Your final grade will be determined by three core components: a video presentation, the poster design itself, and an in-person presentation.</p>"},{"location":"poster_session/#video-presentation-03-points","title":"Video Presentation (0.3\u00a0points)","text":"<p>Record a Full HD video (1920\u00d71080) presenting your final work to someone who has basic knowledge of reinforcement learning. Think of this as a mini-lecture or demo that summarizes your project:</p> <ul> <li>Submission: Upload your video to Google Drive and share the link on Quera (submit here) by June\u00a024.</li> <li>Objective: Provide a concise, engaging overview of your research so that someone can grasp the core ideas in under 12\u00a0minutes.</li> </ul>"},{"location":"poster_session/#full-marks-criteria","title":"Full Marks Criteria","text":"<ul> <li>Clear Motivation &amp; Context:   Start with \u201cWhy does this topic matter?\u201d Give a brief intro to the problem you tackled.</li> <li>Well-Structured Flow: <ol> <li>Introduction (30\u201360\u00a0seconds): State the problem and why it's interesting.  </li> <li>Main Idea / Methods (4\u20136\u00a0minutes): Describe your approach, algorithmic insights, or experimental setup.  </li> <li>Results &amp; Discussion (2\u20133\u00a0minutes): Highlight key findings\u2014charts, figures, or demo clips.  </li> <li>Conclusion &amp; Future Work (1\u20132\u00a0minutes): Summarize takeaways and possible next steps.</li> </ol> </li> <li>Strong Visuals: <ul> <li>Use slides or screen-sharing to show your poster, code snippets, plots, or demos.  </li> <li>Keep text minimal\u2014focus on diagrams, animations, or short bullet points.  </li> </ul> </li> <li>Time Management: <ul> <li>Aim for 8\u201312\u00a0minutes total. Practice so you don't rush or run overtime.</li> </ul> </li> <li>Engaging Delivery: <ul> <li>Be audible and clear\u2014consider using a good microphone or quiet environment.  </li> <li>Maintain an energetic pace; avoid monotone speaking.  </li> <li>Make eye contact (if on camera) and use natural gestures to keep viewers engaged.</li> </ul> </li> </ul> <p>Note: Since your final video will be featured on our YouTube channel, begin with an introductory slide that prominently displays your name, topic, and affiliation. Also, include your online contact details (e.g., website, social media profiles) in the text file containing your video link, so that we can share them under your video presentation and viewers can easily connect with you.</p>"},{"location":"poster_session/#poster-design-03-points","title":"Poster Design (0.3\u00a0points)","text":"<p>Create a Vertical A1 poster (594\u202fmm\u202f\u00d7\u202f841\u202fmm) that visually communicates your project. You can submit either: A vector file (PDF, SVG, etc.), or A high-resolution raster (PNG, TIF) at \u2265\u202f300\u202fDPI.</p> <ul> <li>Submission: Upload your final poster to Google Drive and share the link on Quera (submit here) by June\u00a024.</li> <li>Objective: Use layout, color, and concise text to guide viewers through your research question, methods, and results.</li> </ul>"},{"location":"poster_session/#full-marks-criteria_1","title":"Full Marks Criteria","text":"<ol> <li>Logical Layout: <ul> <li>Divide the poster into clear sections: Title / Authors, Introduction, Methods, Results, Discussion, References.  </li> <li>Follow a top-to-bottom or left-to-right flow so readers can scan easily.</li> </ul> </li> <li>Readable Typography</li> <li>Visual Clarity: <ul> <li>Include diagrams, charts, or screenshots that directly support your narrative.  </li> <li>Keep text boxes and figures aligned and balanced\u2014avoid overcrowding.</li> </ul> </li> <li>Clear Research Question &amp; Motivation: <ul> <li>At the very top or upper-left, state \u201cWhat question are we asking?\u201d in a few sentences.</li> </ul> </li> <li>Concise Methods &amp; Results: <ul> <li>Use bullet points, flowcharts, or algorithm diagrams to explain your approach.  </li> <li>Present key findings using labeled graphs or tables\u2014annotate them so viewers understand at a glance.</li> </ul> </li> <li>Proper Citations &amp; References: <ul> <li>Include citations in a smaller font at the bottom.  </li> <li>Use a consistent citation style (MLA).</li> </ul> </li> <li>Polish &amp; Professionalism: <ul> <li>Check for spelling/grammar errors (proofread!).  </li> <li>Choose a clean color palette (2\u20133 complementary colors).  </li> <li>Leave enough white space so nothing feels cramped.</li> </ul> </li> </ol> <p>Example Posters: You can view last year's submissions here for inspiration.</p>"},{"location":"poster_session/#in-person-presentation-04-points","title":"In-Person Presentation (0.4\u00a0points)","text":"<p>On June\u202f26 or 27, you will stand beside your poster at the poster session, discuss your work with judges and peers, and answer their questions live.</p> <ul> <li>Objective: Demonstrate your deep understanding of the topic and communicate it effectively in a face-to-face setting.</li> </ul>"},{"location":"poster_session/#full-marks-criteria_2","title":"Full Marks Criteria","text":"<ol> <li>Confident Explanation: <ul> <li>Greet viewers with a brief elevator pitch (15\u201320\u00a0seconds): \u201cHi, I'm\u00a0[Name], and this is my project on\u00a0[Topic]\u2014we explore [core idea] because\u00a0[why it matters].\u201d</li> <li>Walk through the poster, pointing at key sections. Avoid reading text verbatim.</li> </ul> </li> <li>Technical Q&amp;A: <ul> <li>Anticipate common questions:  <ul> <li>\u201cWhy did you compare these two algorithms?\u201d  </li> <li>\u201cWhat are the limitations of your approach?\u201d  </li> </ul> </li> <li>If you don't know an answer, it's OK to say, \u201cThat's a good point\u2014here's how I'd investigate it next.\u201d</li> </ul> </li> <li>Demonstrated RL Foundations: <ul> <li>Show that you understand the underlying RL concepts (e.g., Markov Decision Processes, reward design, exploration vs. exploitation).  </li> <li>Be ready to discuss alternatives or extensions.</li> </ul> </li> <li>Engaging the Audience: <ul> <li>Make eye contact, ask rhetorical questions (\u201cImagine training an agent that\u2026\u201d).  </li> <li>Show enthusiasm\u2014smile and be approachable.</li> </ul> </li> <li>Time Management: <ul> <li>Each visitor may only have a few minutes\u2014focus on the big picture first, then dive deeper if they ask.</li> <li>Keep your overall \u201ctour\u201d of the poster to about 10\u00a0minutes, then invite questions.</li> </ul> </li> </ol> <p>Note: Your final score and rank in the competition will be a weighted combination of judges' evaluations and audience feedback.</p>"},{"location":"poster_session/#bonus-grading-components","title":"Bonus Grading Components","text":"<p>If you want to go above and beyond, there are three ways to earn extra points. These are optional but highly encouraged if you enjoy pushing the boundaries of scientific communication.</p>"},{"location":"poster_session/#1-video-presentation-in-english-03-points","title":"1. Video Presentation in English (0.3\u00a0points)","text":"<p>Submit an English-language video of your presentation (same format requirements as above) to demonstrate your ability to communicate RL concepts to a global audience.</p> <ul> <li>Submission: Along with your main video, upload an English version to Google Drive and share the link on Quera by June\u00a024.</li> <li>Objective: Practice scientific English and make your work accessible to non-Persian speakers.</li> </ul>"},{"location":"poster_session/#full-marks-criteria_3","title":"Full Marks Criteria","text":"<ul> <li>Entirely in English: No switching back to another language.  </li> <li>Include Presenter Video: We want to see you speaking (face and voice) while you present your slides or poster.  </li> <li>Clear Pronunciation: Listeners should understand the technical content without major effort.  </li> <li>Fluency &amp; Flow: Avoid lengthy pauses\u2014speak at a natural pace suitable for international viewers.</li> </ul>"},{"location":"poster_session/#2-blog-post-in-rl-journal-club-04-points","title":"2. Blog Post in RL Journal Club (0.4\u00a0points)","text":"<p>Write a blog post about your project for our RL Journal Club blog.</p> <ul> <li>Objective: Publish a polished article that contributes to the broader RL community.</li> <li>Deadline: June\u00a024.</li> </ul>"},{"location":"poster_session/#brief-guideline","title":"Brief Guideline","text":"<ol> <li>Fork the Repository: <ul> <li>Go to the RLJ Club GitHub and click Fork to create your own copy.</li> </ul> </li> <li>Set Up Locally: <ul> <li>Follow instructions for Hugo and the hugo-PaperMod theme.  </li> <li>Make sure you can build the site on your machine.</li> </ul> </li> <li>Write Your Article: <ul> <li>Look at existing posts to understand style and structure.  </li> <li>Write in a clear, engaging tone\u2014imagine you're explaining to someone who is familiar with RL but not an expert in your niche.</li> </ul> </li> <li>Create a Pull Request: <ul> <li>Commit your new Markdown file, push to your fork, and open a Pull Request against the main repo.  </li> <li>Include a brief description (\u201cMy poster session blog post on\u00a0[Topic]\u201d).</li> </ul> </li> </ol>"},{"location":"poster_session/#full-marks-criteria_4","title":"Full Marks Criteria","text":"<ul> <li>Clarity &amp; Coherence: The narrative flows logically; even a non-specialist can follow.  </li> <li>Comprehensive Coverage: Motivation, methods, and key insights are all well explained.  </li> <li>Visuals: At least one figure (poster snippet, plot, or diagram) to illustrate your points.  </li> <li>Professional Polish: Proofread for grammar, spelling, and formatting. Use consistent Markdown conventions.</li> </ul>"},{"location":"poster_session/#3-extra-bonus-competition-03-points","title":"3. Extra Bonus &amp; Competition (\u2265\u202f0.3\u00a0points)","text":"<p>Surprise us! If you find a creative way to elevate your project beyond the requirements, you could earn up to 0.3 extra points (or more) depending on impact and originality.</p> <ul> <li>Objective: Reward exceptional creativity, depth, or extra initiative.</li> </ul>"},{"location":"poster_session/#possible-examples","title":"Possible Examples:","text":"<ul> <li>Top Presentation Award: Judges may select the 3 best presentations in the event\u2014earn additional points if you rank among them.  </li> <li>Novel Experiments / Demos: Implement a brand-new experiment, create a live demo environment, or show off an impressive visualization.  </li> <li>Broad Comparisons: Compare multiple RL papers, algorithms, or frameworks in the same study\u2014demonstrate deeper analysis.  </li> <li>Custom Environment / Benchmark: Design an original environment (e.g., a small game or simulation) and benchmark different RL agents on it.  </li> <li>Interactive Poster: Embed QR codes, short videos, or interactive web demos that viewers can explore on their phones.  </li> <li>Real-World Application: Showcase how DRL can solve a real problem\u2014robotics, finance, games, or any practical domain.  </li> <li>Educational Material: Develop a mini-tutorial, record a short lecture series, or create a well-documented GitHub repo with code and clear README.</li> </ul> <p>Challenge yourself\u2014we love being surprised! If you're unsure whether an idea qualifies, ask early so we can give feedback.</p>"},{"location":"poster_session/#summary-approximate-deadlines","title":"Summary &amp; Approximate Deadlines","text":"<p>Below is a quick reference for key milestones and their weight toward your final grade:</p> Component Points Approximate Deadline Research Proposal 0 June\u00a06, 2025 (11:59\u202fPM) Video Presentation 0.3 June\u00a024, 2025 (11:59\u202fPM) Poster Design 0.3 June\u00a024, 2025 (11:59\u202fPM) In-Person Presentation 0.4 June\u00a026\u201327, 2025 (Event) Bonus\u00a01: English Video Presentation 0.3 June\u00a024, 2025 (11:59\u202fPM) Bonus\u00a02: Blog Post (RL Journal Club) 0.4 June\u00a024, 2025 (11:59\u202fPM) Bonus\u00a03: Extra Creativity 0.3+ June\u00a026\u201327, 2025 (Event) <p>We can't wait to see what you come up with. Let's make this poster session informative, fun, and inspiring for everyone involved\u2014happy researching and good luck!</p>"},{"location":"poster_session/sp24/","title":"Spring 2024","text":"<ul> <li> <p> Advancements in Offline Reinforcement Learning </p> </li> <li> <p> Automated Reinforcement Learning (AutoRL) </p> </li> <li> <p> Causal Bandits </p> </li> <li> <p> Combinatorial Causal Bandits </p> </li> <li> <p> Diffusion+RL </p> </li> <li> <p> Explainable Reinforcement Learning </p> </li> <li> <p> Goal-Conditioned Reinforcement Learning </p> </li> <li> <p> Hierarchical Reinforcement Learning </p> </li> <li> <p> Introduction to RL-based Recommender Systems </p> </li> <li> <p> Learning to Communication in MARL </p> </li> <li> <p> LLM RL Alignment </p> </li> <li> <p> Meta Reinforcement Learning </p> </li> <li> <p> Modular Reinforcement Learning </p> </li> <li> <p> Multi-Task Model-Based Reinforcement Learning </p> </li> <li> <p> Non-Stationary Reinforcement Learning </p> </li> <li> <p> Towards Robust and Safe Reinforcement Learning </p> </li> <li> <p> Unsupervised Reinforcement Learning </p> </li> </ul>"},{"location":"poster_session/sp25/","title":"Spring 2025","text":""},{"location":"prerequisites/","title":"Introduction","text":""},{"location":"prerequisites/deep_learning/","title":"Deep Learning","text":""},{"location":"prerequisites/game_theory/","title":"Game Theory","text":""},{"location":"prerequisites/game_theory/#what-is-game-theory","title":"What is Game Theory","text":"<p>Game theory originated to analyze strategic interactions between rational decision-makers. Its early foundations can be traced back to Augustin Cournot\u2019s work on imperfect competition in 1838, but it was formally developed in 1944 by John von Neumann and Oskar Morgenstern in Theory of Games and Economic Behavior. A breakthrough came later with John Nash\u2019s introduction of Nash Equilibrium, which became a cornerstone of modern game theory.</p> <p>At its core, game theory provides a structured way to study decision-making in competitive and cooperative environments. It helps explain how rational agents, whether individuals, companies, or even AI systems, make choices when their outcomes depend on the actions of others. A rational agent is assumed to act logically, weighing available information, potential risks, and expected rewards to achieve the best possible outcome.</p> <p>Game theory applies to a wide range of real-world scenarios, from economics and business strategy to politics and artificial intelligence. It is particularly useful in multi-agent systems, where multiple entities interact strategically, making it a fundamental tool for Multi-Agent Reinforcement Learning (MARL).</p>"},{"location":"prerequisites/game_theory/#game","title":"Game","text":"<p>A game is a mathematical model of strategic interactions where multiple decision-making entities (players) choose from a set of possible actions (strategies) and receive payoffs based on their choices. The simplest form of a game consists of three key elements:</p> <ol> <li>Players: The decision-making agents in the game. Players can be autonomous robots, AI agents in a multi-agent system, competing machine learning models, or even human participants. In an N-player game, players are typically labeled as Player 1, Player 2, ..., Player n, and any arbitrary player is referred to as Player i.</li> <li> <p>Strategies: The available choices or actions each player can take. A strategy can be a single action (e.g., moving left or right in a robotic navigation task) or a set of possible actions (e.g., an AI choosing between different learning rates in an optimization problem). If a player i has a strategy set \ud835\udc46<sub>\ud835\udc56</sub>, then any individual strategy within that set is denoted as s<sub>\ud835\udc56</sub>.</p> <ul> <li>Example: In a multi-agent reinforcement learning (MARL) setting for self-driving cars, each vehicle must decide whether to change lanes, maintain speed, or slow down based on other cars\u2019 positions.</li> </ul> </li> <li> <p>Payoffs: The rewards or outcomes each player receives based on the chosen strategies. In multi-agent reinforcement learning (MARL), payoffs often represent a reward function, which could be the distance traveled, fuel efficiency, or a safety score. The payoff function for player iii is denoted as: $$ U_i(s_1, s_2, \\dots, s_n) $$</p> </li> </ol> <p>where \\(s_1, s_2, \\dots, s_n\\) are the strategies chosen by all players.</p>"},{"location":"prerequisites/game_theory/#information-in-game","title":"Information in Game","text":"<p>Apart from players, strategies, and payoffs, a game also specifies the information available to each player at the time of decision-making. The concept of common knowledge is often assumed, meaning:</p> <ul> <li>All players know the rules of the game.</li> <li>Each player knows what others know about the game.</li> </ul> <p>However, different games vary in how much information is available at decision time, which significantly impacts strategy selection.</p>"},{"location":"prerequisites/game_theory/#types-of-games-based-on-timing-and-information","title":"Types of Games Based on Timing and Information","text":"<p>Simultaneous-Move (Static) Games:</p> <p>Players choose their actions at the same time (or without knowing what the others have chosen).</p> <ul> <li>Example: Packet Routing in Networks \u2013 When multiple routers decide simultaneously which path to forward packets without knowledge of others\u2019 decisions, leading to congestion or efficient data transfer.</li> </ul> <p>Sequential-Move (Dynamic) Games:</p> <p>Players take turns making decisions, and later players observe previous moves before making their choices.</p> <ul> <li>Example: Robot Navigation in a Warehouse \u2013 Imagine multiple robots moving through a warehouse to collect and transport items. The first robot moves and chooses a path toward its target shelf. The second robot, before making its decision, can see the first robot\u2019s movement and adjust its path to avoid congestion or optimize efficiency. The third robot then observes both previous movements before planning its route. The sequence of moves continues until all robots reach their destinations efficiently. This is a sequential game because each robot\u2019s decision depends on the prior actions of others, making planning and adaptation critical.</li> </ul>"},{"location":"prerequisites/game_theory/#why-game-theory-matters-in-multi-agent-systems","title":"Why Game Theory Matters in Multi-Agent Systems","text":"<p>Game theory provides a mathematical foundation for analyzing and designing multi-agent AI systems, such as:</p> <ul> <li>Autonomous vehicle coordination: How self-driving cars decide when to merge or yield in traffic.</li> <li>AI competition in adversarial environments: How AI agents play against each other in games like StarCraft II or Dota 2.</li> <li>Resource allocation in distributed systems: How cloud computing services decide to allocate CPU and memory resources among multiple users.</li> </ul>"},{"location":"prerequisites/game_theory/#types-of-game","title":"Types of Game","text":""},{"location":"prerequisites/game_theory/#cooperative-vs-non-cooperative-games","title":"Cooperative vs. Non-Cooperative Games","text":"<p>Game theory is broadly divided into cooperative and non-cooperative games. The key difference between the two lies in whether players can form binding agreements.</p> <p>Cooperative Game Theory:</p> <ul> <li> <p>Players form groups or coalitions and make binding agreements to coordinate their actions for mutual benefit.</p> </li> <li> <p>The goal is to maximize the collective reward, and the payoff is shared among members.</p> <ul> <li>Example in Computer Science: In distributed computing, multiple processors working together to execute a large-scale computation form a cooperative system, where resources and workloads are allocated fairly among them.</li> </ul> </li> </ul> <p>Non-Cooperative Game Theory:</p> <ul> <li> <p>Players act individually, focusing on their own self-interest.</p> </li> <li> <p>No binding contracts exist, meaning players independently decide their strategies, often leading to competition.</p> </li> <li> <p>The outcome is determined by the strategies chosen by all players, and each player's welfare depends on others' actions.</p> <ul> <li>Example in Computer Science: In network routing, Internet Service Providers (ISPs) manage their own traffic flow and bandwidth allocation independently, often competing for optimal routing paths while still interacting in the shared network.</li> </ul> </li> </ul>"},{"location":"prerequisites/game_theory/#games-of-complete-and-incomplete-information","title":"Games of Complete and Incomplete Information","text":"<p>In game theory, games are classified based on how much information players have about the game and their opponents.</p> <p>Complete Information Games:</p> <p>In a game of complete information, all players have full knowledge of:</p> <ol> <li>The strategies are available to all players.</li> <li>The payoff functions for every possible outcome.</li> <li>The types of players involved (i.e., whether they are rational, cooperative, or competitive).</li> </ol> <p>Since players know all relevant information, they can plan their moves strategically to maximize their expected reward. The solution concepts used in complete information games include:</p> <ul> <li>Nash Equilibrium (for static/simultaneous-move games).</li> <li> <p>Subgame Perfect Nash Equilibrium (for dynamic/sequential-move games).</p> <ul> <li>Example: Chess as a Complete Information Game: In chess, both players see the entire board, know the rules, and can predict possible future moves. There is no hidden information, everything is visible, making chess a classic example of a complete information game in AI.</li> </ul> </li> </ul> <p>Incomplete Information Games:</p> <p>In a game of incomplete information, some players have private information that others do not know. This means that players must estimate or infer their opponents' strategies, payoffs, or even goals.</p> <p>For instance, in an auction, each bidder knows their own valuation of an item but does not know how much the other bidders are willing to pay. Players must guess and strategize based on limited knowledge.</p> <p>The solution concepts used for incomplete information games include:</p> <ul> <li>Bayesian Nash Equilibrium (for static/simultaneous-move games).</li> <li> <p>Perfect Bayesian Equilibrium (for dynamic/sequential-move games).</p> <ul> <li>Example: When autonomous vehicles interact on the road, they do not know the exact goals of other drivers (e.g., whether a car will merge into another lane or speed up). They must estimate these intentions based on observed behavior.</li> </ul> </li> </ul>"},{"location":"prerequisites/game_theory/#zero-sum-vs-non-zero-sum-games","title":"Zero-Sum vs. Non-Zero-Sum Games","text":"<p>Zero-Sum Game:</p> <p>A zero-sum game is a type of game where one player\u2019s gain is exactly equal to the loss of another player. The total sum of payoffs in the game always equals zero\u2014meaning one player\u2019s success comes entirely at the expense of the other.</p> <ul> <li>Example: In chess, poker, or strategic board games, when one AI agent wins, the opponent necessarily loses by an equal amount.</li> <li>Example: In Generative Adversarial Networks (GANs), a generator tries to create realistic images, while a discriminator tries to distinguish real from fake images. The better the generator gets, the harder the discriminator's job becomes, and vice versa\u2014making it a zero-sum interaction.</li> </ul> <p>Non-zero-sum game:</p> <p>A non-zero-sum game, in contrast, is where players\u2019 gains do not necessarily come at the expense of others. Players can either benefit (cooperative outcomes) or both suffer (mutual losses).</p> <ul> <li>Example: In autonomous vehicle coordination, two self-driving cars merging onto a highway benefit from cooperation\u2014if they signal and adjust speeds optimally, they both avoid collisions and reduce traffic delays.</li> <li>Example: In distributed computing, multiple AI models can share processing power efficiently, benefiting all users instead of competing destructively for resources.</li> </ul>"},{"location":"prerequisites/game_theory/#simultaneous-move-vs-sequential-move-games","title":"Simultaneous-Move vs. Sequential-Move Games","text":"<p>The order of moves plays a crucial role in game theory, as it directly affects the strategies and outcomes of the game. Players can either make decisions simultaneously or act sequentially, with each scenario leading to different strategic considerations.</p> <p>Simultaneous-Move Games:</p> <p>In simultaneous-move games, all agents take actions at the same time, without knowledge of the others' choices. This requires strategies based on expectations rather than direct observation of the opponent\u2019s move.</p> <ul> <li> <p>Example: In multi-agent reinforcement learning (MARL) for competitive games like Atari Pong or RoboCup Soccer, both AI agents choose their actions at the same time (e.g., moving left, right, attacking, or defending). Since neither agent knows what the other will do at that moment, they must learn policies that anticipate opponent behavior.</p> </li> <li> <p>Example: Autonomous traffic lights must simultaneously decide their signal timings without knowing how other intersections will change theirs. If all intersections turn green for a major road at the same time, it improves flow, but if they conflict, it may lead to congestion. RL agents learn traffic policies that balance efficiency under simultaneous uncertainty.</p> </li> </ul> <p>Sequential-Move Games:</p> <p>In sequential-move games, actions happen in a turn-based manner, meaning later agents can observe and adapt to earlier actions.</p> <ul> <li> <p>Example: In robotic task planning, a high-level agent first selects a sub-goal (e.g., \"pick up the object\"), then a low-level agent observes this decision and executes finer actions (e.g., \"move gripper, adjust force, lift object\"). The low-level agent\u2019s decision depends on what the high-level agent has already done, making this a sequential-move game.</p> </li> <li> <p>Example: In Autonomous Drone Navigation in Obstacle Avoidance, A leading drone in a formation moves first, and following drones adjust their trajectories based on the leader's action. The first drone\u2019s movement is not influenced by others, but the second and third drones must adapt to what has already happened, making this a sequential decision-making RL problem.</p> </li> </ul> <p>Simultaneous-Move Games \u2192 Require opponent modeling and strategy anticipation.</p> <p>Sequential-Move Games \u2192 Allow adaptation and learning from prior moves.</p>"},{"location":"prerequisites/game_theory/#game-representations","title":"Game Representations","text":"<p>In game theory, there are two primary ways to represent a game:</p> <ol> <li>Normal (Strategic) Form</li> <li>Extensive Form</li> </ol>"},{"location":"prerequisites/game_theory/#normal-strategic-form","title":"Normal (Strategic) Form","text":"<p>The normal form represents a game using a payoff matrix, where rows and columns denote different strategies chosen by players.</p> <p>By convention, in a two-player game:</p> <ul> <li> <p>Rows correspond to Player 1\u2019s strategies.</p> </li> <li> <p>Columns correspond to Player 2\u2019s strategies.</p> </li> <li> <p>Each cell in the matrix shows the payoff each player receives for a given strategy combination.</p> </li> </ul>"},{"location":"prerequisites/game_theory/#extensive-form","title":"Extensive Form","text":"<p>The extensive form is a pictorial representation of a game using a game tree, which illustrates:</p> <ul> <li>The order in which players make decisions.</li> <li>The choices available at each decision point.</li> <li>The payoffs for different sequences of choices.</li> </ul> <p>In an extensive form representation:</p> <ul> <li>The game starts at an initial node, usually controlled by Player 1.</li> <li>Each branch represents a possible action.</li> <li>When a player makes a decision, the game moves along a branch to another node, where Player 2 (or another player) makes their decision.</li> <li>This continues until reaching a terminal node, where each player receives a payoff.</li> </ul> <p>In the following we will illustrate both of these presentation forms using the very famous example of Prisoner\u2019s Dilemma game.</p> <p>Prisoner\u2019s Dilemma: A well-known example of a non-cooperative and a game of complete information is the Prisoners\u2019 Dilemma game. Consider the following set-up of the game: A crime is committed for which there is no eyewitness. Suspects 1 and 2 are caught and imprisoned in two separate cells. Thus, each prisoner is in a solitary confinement with no means of communicating with each other. The magistrate speaks to each prisoner separately and asks them to act as an informer. If one of them confesses the crime, he will be freed but the other one will spend 4 years in prison. If both confess, each will spend 3 years in prison. If both stay quiet and do not confess, the crime cannot be probed, so they will get nominal punishment by spending only one year in prison. Thus, each player then has two possible strategies: Not confess (N) or Confess \\(C\\) and they decide simultaneously.</p> <p>Prisoner\u2019s Dilemma in Normal Form:</p> <ul> <li>The first number in each cell represents Player 1\u2019s payoff, and the second represents Player 2\u2019s payoff.</li> <li>The game is played simultaneously, meaning neither player knows what the other will do.</li> </ul> Player 1 \u2193 / Player 2 \u2192 Not Confess Confess Not Confess (-1, -1) (-4, 0) Confess (0, -4) (-3, -3) <p>Payoffs are given in the format (Player 1, Player 2)</p> <p>Prisoner\u2019s Dilemma in Extensive Form:</p> <p></p> <p>To reflect the fact that the Prisoners\u2019 Dilemma is a simultaneous move game, a dotted oval is drawn around Player 2\u2019s decision nodes to reflect the fact that Player 2 does not know which of the two decision nodes he is at since he does not observe which action Player 1 has chosen, that is, he does not know whether the first decision by Player 1 was to confess or not confess. This dotted oval around the two nodes of Player 2 indicates his lack of specific information. An information set of a Player is a collection of nodes such that the same player (here Player 2) moves at each of these nodes; and the same moves (here Not Confess, Confess) are available at each of these nodes.</p> <p>If we were to use the game tree to illustrate the above game as a sequential one in which Player 1 moves first which then is observed and reacted upon by Player 2, then the game would be more correctly drawn without the ellipse as:</p> <p></p> <p>The two nodes (say A and B) which signify the move of Player 2 represent the information set of Player 2. Clearly the information base of Player 2 at A and at B are not same in this case of sequential-move game.</p> <p>Choosing Between Normal and Extensive Form</p> <ul> <li> <p>Normal Form is best suited for simultaneous-move games, where players act without knowing the other\u2019s choice.</p> </li> <li> <p>Extensive Form is typically used for sequential-move games, but it can also represent simultaneous-move games using information sets (denoting that a player does not know which node they are in).</p> </li> </ul> <p>Key Points:</p> <p>The Normal Form and Extensive Form representations in game theory share fundamental similarities with multi-agent reinforcement learning (MARL) and decision-making in AI.</p> <p>Normal Form and RL</p> <ul> <li> <p>In simultaneous-move games, players choose their actions at the same time, like how RL agents in multi-agent settings must act without knowing the exact decisions of others.</p> </li> <li> <p>In self-play reinforcement learning, where two agents compete and improve through repeated interactions (e.g., AlphaZero), their learned Q-values can be structured as a payoff matrix, just like in normal-form games.</p> </li> </ul> <p>Extensive Form and RL</p> <ul> <li> <p>In sequential-move games, players make decisions one after another, similar to step-by-step decision-making in RL.</p> </li> <li> <p>This is directly related to Markov Decision Processes (MDPs), where each action transitions the agent to a new state, leading to a final reward at the end of an episode.</p> </li> <li> <p>Just as payoffs are assigned at the terminal nodes of a game tree, in episodic RL, rewards are only received after completing the full sequence of actions.</p> <ul> <li>Example: In an RL-based robotic arm learning to pick up an object, the robot makes a series of moves (grasp, adjust, lift), and only at the end does it receive the final reward (successful or failed pickup).</li> </ul> </li> </ul> <p>Thus, extensive-form games in game theory and decision-making models in RL share a deep structural similarity, making game theory a crucial tool for designing multi-agent learning algorithms.</p>"},{"location":"prerequisites/game_theory/#solving-a-game-theory-problem","title":"Solving A Game Theory Problem","text":"<p>After covering different representation forms of a game theory problem in the previous section, let us now discuss how to find solution of game theoretic problem.</p>"},{"location":"prerequisites/game_theory/#nash-equilibrium","title":"Nash Equilibrium","text":"<p>A Nash Equilibrium (NE) is a strategy profile where each player chooses the best possible response given the strategies of others. At Nash equilibrium, no player has an incentive to deviate unilaterally because changing their strategy would not improve their payoff, assuming all other players maintain their strategies.</p> <p>Formal Definition A strategy profile \\((s_1^*, s_2^*, s_3^*, \\dots, s_n^*)\\) is a Nash equilibrium if, for each player \\(i\\), their chosen strategy (\\(s_i^*\\)) is the best response to the strategies of the other \\(n - 1\\) players. Mathematically, this is expressed as:</p> \\[ U_i(s_i^*, s_{-i}^*) \\geq U_i(s_i, s_{-i}^*) \\quad \\forall s_i \\] <p>where:</p> <ul> <li>\\(U_i\\) is the payoff function of player \\(i\\).</li> <li>\\(s_i^*\\) is player \\(i\\)\u2019s optimal strategy.</li> <li>\\(s_{-i}^*\\) represents the Nash equilibrium strategies of all other players.</li> </ul> <p>This condition ensures that if any player unilaterally deviates from their equilibrium strategy, their payoff will not improve.</p> <p>Now, let us attempt to find the Nash equilibrium in case of the Prisoners\u2019 Dilemma game.</p> <p>To determine the Nash equilibrium, we analyze each player\u2019s best response to the other\u2019s strategy:</p> <p>If Player 2 plays \"Stay Silent\", Player 1 gets:</p> <ul> <li>-1 year of prison by also staying silent.</li> <li>0 years of prison by confessing (better option).</li> <li>Best Response: Confess.</li> </ul> <p>If Player 2 plays \"Confess\", Player 1 gets:</p> <ul> <li>-3 years of prison by confessing.</li> <li>-4 years of prison by staying silent (worse option).</li> <li>Best Response: Confess.</li> </ul> <p>Since both players reach the same conclusion, the Nash equilibrium occurs at (Confess, Confess) with a payoff of (-3, -3).</p> <p>Why Nash Equilibrium Is Not Always the Best Outcome?</p> <p>Notice that if both players stayed silent, their payoffs would be (-1, -1), which is a better outcome than (-3, -3). However, this (Stay Silent, Stay Silent) outcome is not a Nash equilibrium because each player has an incentive to deviate (confessing gives a better payoff).</p> <p>This leads to the concept of Pareto Optimality, where a different strategy profile could be better for all players, but is not an equilibrium unless cooperation is enforced. (We will discuss this in the following section)</p>"},{"location":"prerequisites/game_theory/#multiple-nash-equilibria","title":"Multiple Nash Equilibria","text":"<p>Nash equilibrium is a powerful concept because it guarantees a stable solution for strategic interactions. However, a challenge arises when a game has multiple Nash equilibria, making it difficult to predict a unique outcome.</p> <p>To illustrate this, consider the classic Battle of the Sexes game.</p> <p>Battle of the Sexes</p> <p>This game involves two players, a husband and a wife, who are planning an evening out.</p> <ul> <li>They both prefer to be together rather than going out alone.</li> <li> <p>However, they have different preferences:</p> </li> <li> <p>The wife wants to attend an Opera.</p> </li> <li> <p>The husband prefers to watch a Boxing match.</p> </li> </ul> <p>The payoff matrix for their choices is as follows:</p>"},{"location":"prerequisites/game_theory/#battle-of-the-sexes-game","title":"Battle of the Sexes Game","text":"Player 1 (wife) \u2193 / Player 2 (husband) \u2192 Opera Boxing Opera (3, 1) (0, 0) Boxing (0, 0) (1, 3) <p>Payoffs are in the format (Wife, Husband)</p> <p>Finding the Nash Equilibria:</p> <p>The best response for each player is to follow the other\u2019s decision:</p> <ul> <li> <p>If the wife chooses Opera, the husband\u2019s best response is Opera.</p> </li> <li> <p>If the husband chooses Boxing, the wife\u2019s best response is Boxing.</p> </li> </ul> <p>This results in two pure-strategy Nash equilibria:</p> <ul> <li> <p>(Opera, Opera) with payoffs (3,1)</p> </li> <li> <p>(Boxing, Boxing) with payoffs (1,3)</p> </li> </ul> <p>Since both equilibria offer different advantages to different players, the game presents a coordination problem: which equilibrium should they choose?</p> <p>Challenges with the Multiple Equilibria</p> <ul> <li> <p>Unlike games with a single Nash equilibrium, multiple equilibria create uncertainty because the game theory model alone cannot predict which outcome will occur.</p> </li> <li> <p>There is no clear Pareto superior solution because the equilibria are symmetrical\u2014each favor one player over the other.</p> </li> </ul>"},{"location":"prerequisites/game_theory/#pure-and-mixed-strategies-in-game-theory","title":"Pure and Mixed Strategies in Game Theory","text":"<p>Pure Strategies</p> <p>A pure strategy is when a player selects one specific action with certainty in a game. In other words, the player always chooses the same move when faced with the same situation.</p> <p>Pure Strategy in the Prisoner\u2019s Dilemma</p> <ul> <li>If a prisoner always chooses to confess, regardless of what the other prisoner does, this is a pure strategy.</li> <li>Similarly, if a player in Rock-Paper-Scissors always plays Rock, this is also a pure strategy.</li> </ul> <p>Pure strategies are deterministic, meaning there is no randomness in the player\u2019s decision-making.</p> <p>However, not all games have stable Nash equilibria in pure strategies. This is where mixed strategies become important.</p> <p>Mixed Strategies</p> <p>A mixed strategy is when a player randomly chooses between multiple strategies based on a probability distribution. This means that instead of always picking the same action, the player assigns probabilities to different choices and selects them accordingly.</p> <p>Mixed strategies are particularly useful in games where no pure strategy Nash equilibrium exists, meaning that predictable behavior leads to exploitation by the opponent.</p> <p>One of the best examples of mixed strategies in game theory is the Matching Pennies game.</p> <p>Matching Pennies</p> <p>There are two players, each with a coin (penny).</p> <p>Both players simultaneously choose to display their coins as Heads (H) or Tails (T).</p> <p>If the coins match (both heads or both tails), Player 2 gives a penny to Player 1.</p> <p>If the coins do not match (one is heads and the other is tails), Player 1 gives a penny to Player 2. The payoff matrix is:</p>"},{"location":"prerequisites/game_theory/#matching-pennies-game","title":"Matching Pennies Game","text":"Player 1 \u2193 / Player 2 \u2192 Head Tail Head (1, -1) (-1, 1) Tail (-1, 1) (1, -1) <p>This is a zero-sum game, meaning the gain of one player is exactly equal to the loss of the other.</p> <p>Checking for Pure Strategy Nash Equilibrium:</p> <p>Suppose Player 2 always plays Head (H):</p> <ul> <li> <p>Player 1\u2019s best response is also Head (H) (to win the penny).</p> </li> <li> <p>If Player 1 always plays Head, then Player 2 would switch to Tail (T) to win instead.</p> </li> </ul> <p>This process keeps repeating, meaning no pure strategy Nash equilibrium exists.</p> <p>Since there is no stable outcome in pure strategies, we must use mixed strategies where both players randomize their choices.</p> <p>In a mixed strategy equilibrium, each of the players must be indifferent between the actions which they choose to play. If a player was not indifferent between the available actions, this would imply that one particular action yields a higher payoff than the others, and the player would play that action with probability 1 rather than mixing strategies with certain probability distribution.</p> <p>To solve the Mixed Strategy Nash equilibrium, Let\u2019s define:</p> <ul> <li>Player 1 plays Head with probability q and Tail with probability 1\u2212q.</li> <li>Player 2 plays Head with probability p and Tail with probability 1\u2212p.</li> </ul> <p>The expected payoff of Player 1 for playing a pure strategy of Head when Player 2 plays a mixed strategy with a probability distribution p and (1-p): If Player 1 chooses Head (H), their expected payoff is:</p> \\[ E_1(H) = p(1) + (1 - p)(-1) = 2p - 1 \\] <p>If Player 1 chooses Tail (T), their expected payoff is:</p> \\[ E_1(T) = p(-1) + (1 - p)(1) = -2p + 1 \\] <p>For Player 1 to be indifferent (i.e., have no incentive to favor Head or Tail), these payoffs must be equal:</p> \\[ E_1(H) = E_1(T) \\Rightarrow 2p - 1 = -2p + 1 \\Rightarrow p = \\frac{1}{2} \\] <p>Thus, Player 2 must choose Head and Tail with equal probability (50%-50%) to keep Player 1 from exploiting a predictable strategy.</p> <p>Similarly, the expected payoff of Player 2 for playing a pure strategy of Head when Player 1 plays a mixed strategy with a probability distribution \\(q\\) and \\((1 - q)\\):</p> \\[ E_2(H) = q(-1) + (1 - q)(1) = -2q + 1 \\] \\[ E_2(T) = q(1) + (1 - q)(-1) = 2q - 1 \\] \\[ E_2(H) = E_2(T) \\Rightarrow 2q - 1 = -2q + 1 \\Rightarrow q = \\frac{1}{2} \\] <p>Thus, Player 1 must also choose Head and Tail with equal probability (50%-50%).</p> <p>The only Nash equilibrium in this game is where both players play Heads and Tails with equal probability \\(\\left(\\frac{1}{2}, \\frac{1}{2}\\right)\\). Thus, the equilibrium strategy can be written as:</p> \\[ p^* = \\frac{1}{2}, \\quad q^* = \\frac{1}{2} \\] <p>This means that each player randomizes their choice to ensure that the opponent cannot predict and exploit their strategy.</p>"},{"location":"prerequisites/game_theory/#sequential-games","title":"Sequential Games","text":"<p>So far, we have considered simultaneous-move games, where all players make their decisions at the same time, without prior knowledge of the actions chosen by their opponents. In such games, time does not play a role, so they are often called static games.</p> <p>However, many games involve sequential moves, where one player moves before another. These are known as sequential-move games, where:</p> <ul> <li>Players take actions at well-defined turns (over time).</li> <li>The second player observes the first player\u2019s action before making their own decision.</li> <li>Players have perfect information about previous moves.</li> </ul> <p>Let\u2019s consider the Battle of the Sexes, but now assume it is played sequentially instead of simultaneously:</p> <ul> <li>The wife moves first, choosing either Opera or Boxing.</li> <li>The husband moves second, making his choice after seeing his wife\u2019s decision.</li> </ul> <p>How Extensive Form Changes</p> <p>In the extensive form representation of this game:</p> <ul> <li>The oval around Player 2\u2019s decision nodes (which existed in the simultaneous-move version) is removed.</li> <li>This is because the husband (Player 2) knows the wife\u2019s choice before making his own move.</li> <li>The wife\u2019s possible strategies remain the same: Opera, Boxing.</li> <li> <p>However, the husband\u2019s set of possible strategies expands because he must specify an action for each of the wife\u2019s choices:</p> <ul> <li> <p>He must choose an action at node A (if the wife chooses Opera).</p> </li> <li> <p>He must also choose an action at node B (if the wife chooses Boxing).</p> </li> </ul> </li> </ul> <p>This means the husband is not in the same information set anymore, he makes a fully informed decision based on the wife\u2019s action. Illustration of the game is presented in the following extensive form:</p> <p></p> <p>Normal Form Representation of the Sequential Game</p> <p>To fully define Player 2\u2019s strategies, we must specify his action at each information set, even if a particular decision node is never reached in actual gameplay.</p> <p>Thus, Player 2\u2019s (husband\u2019s) strategy set now consists of four possible strategies:</p> <ol> <li>Always Opera \u2192 (Opera | Opera), (Opera | Boxing)</li> <li>Follows Wife \u2192 (Opera | Opera), (Boxing | Boxing)</li> <li>Opposite to Wife \u2192 (Boxing | Opera), (Opera | Boxing)</li> <li>Always Boxing \u2192 (Boxing | Opera), (Boxing | Boxing)</li> </ol> <p>Now, the normal-form representation of the game becomes:</p>"},{"location":"prerequisites/game_theory/#normal-form-representation","title":"Normal Form Representation","text":"Wife / Husband Always Opera Follows Wife Opposite to Wife Always Boxing Opera (3, 1) (3, 1) (0, 0) (0, 0) Boxing (0, 0) (1, 3) (0, 0) (1, 3) <p>Finding Nash Equilibria in the Sequential Game</p> <p>From the table, we observe that multiple Nash equilibria exist:</p> <ol> <li>Wife chooses Opera, Husband chooses (Opera | Opera), (Opera | Boxing)</li> <li>Wife chooses Opera, Husband chooses (Opera | Opera), (Boxing | Boxing)</li> <li>Wife chooses Boxing, Husband chooses (Boxing | Opera), (Boxing | Boxing)</li> </ol> <p>Now, let\u2019s analyze the credibility of these equilibria.</p> <p>Analyzing Credibility of the Nash Equilibria</p> <p>1st Equilibrium: (Opera, (Opera | Opera), (Opera | Boxing))</p> <ul> <li>The husband\u2019s strategy includes a threat that he will choose Opera even if the wife picks Boxing.</li> <li>However, this is an empty (non-credible) threat because if the wife actually chooses Boxing, the husband would get 1 instead of 3 by choosing Opera.</li> <li>Since the husband\u2019s rational choice at node B would actually be Boxing, this equilibrium is not credible.</li> </ul> <p>3rd Equilibrium: (Boxing, (Boxing | Opera), (Boxing | Boxing))</p> <ul> <li>Similarly, this equilibrium involves the husband threatening to play Boxing even if the wife chooses Opera.</li> <li>This is also non-credible because if the wife chooses Opera, the husband gets 1 by choosing Boxing instead of 3 by choosing Opera.</li> </ul> <p>Why Credibility Matters?</p> <ul> <li>Nash equilibrium alone does not consider whether threats are realistic.</li> <li>Subgame Perfect Nash Equilibrium (SPNE) eliminates equilibria based on non-credible threats.</li> <li>To find the true solution, we need to apply Backward Induction\u2014which we will explore next.</li> </ul>"},{"location":"prerequisites/game_theory/#sub-game-perfect-nash-equilibrium-spne","title":"Sub-game Perfect Nash Equilibrium (SPNE)","text":"<p>Nash equilibrium in sequential games can sometimes lead to implausible strategy profiles, especially when they involve non-credible threats. To refine our equilibrium concept, we use Subgame Perfect Nash Equilibrium (SPNE), which ensures that a strategy is rational at every stage of the game, not just overall.</p> <p>Defining SPNE</p> <p>A Subgame Perfect Nash Equilibrium (SPNE) is a strategy profile that:</p> <ol> <li>Forms a Nash equilibrium in the entire game.</li> <li>Is also a Nash equilibrium in every proper subgame.</li> </ol> <p>A subgame is a part of the extensive form representation that begins from any decision node and includes all subsequent actions branching from that node. A proper subgame must begin at a single decision node and not include any nodes that belong to an information set shared with another node.</p> <p>In the sequential Battle of the Sexes, we identify three subgames:</p> <ol> <li>The entire game itself (root node).</li> <li>The subgame beginning at the first decision node of the husband (when the wife chooses Opera).</li> <li>The subgame beginning at the second decision node of the husband (when the wife chooses Boxing).</li> </ol> <p>These subgames are shown in the dashed rectangles in the provided figure.</p> <p></p> <p>Solving the Proper Subgames</p> <ol> <li>Subgame (ii) (Wife chooses Opera \u2192 Husband chooses Opera or Boxing)</li> </ol> <p>The husband must choose between:</p> <ul> <li> <p>Opera, which gives him a payoff of 1.</p> </li> <li> <p>Boxing, which gives him a payoff of 0.</p> </li> </ul> <p>His best response is to choose Opera.</p> <ol> <li>Subgame (iii) (Wife chooses Boxing \u2192 Husband chooses Opera or Boxing)</li> </ol> <p>The husband must choose between:</p> <ul> <li> <p>Opera, which gives him a payoff of 0.</p> </li> <li> <p>Boxing, which gives him a payoff of 3.</p> </li> </ul> <p>His best response is to choose Boxing.</p> <p>Thus, the husband's optimal strategy is (Opera | Opera) (Boxing | Boxing)\u2014meaning he will:</p> <ul> <li>Choose Opera if the wife chooses Opera.</li> <li>Choose Boxing if the wife chooses Boxing.</li> </ul> <p>The other two strategy profiles like (Opera | Opera) (Opera | Boxing) and (Boxing | Opera) (Boxing | Boxing) results in him playing something that is not a Nash equilibrium on some proper subgame. Thus, among the three Nash equilibria we came across for the game, only the second one is subgame perfect Nash equilibrium while the first and the third are not.</p>"},{"location":"prerequisites/game_theory/#backward-induction","title":"Backward Induction","text":"<p>In the previous section, we solved the equilibrium in the sequential Battle of the Sexes game by finding the Nash equilibria using the normal form and then look for a subgame perfect Nash equilibrium among them. Another method providing a somewhat direct way of solving for the subgame perfect Nash equilibrium in such a setting is the method of Backward Induction. In this method we start with the subgames at the bottom of the extensive form, and determine the Nash equilibrium of these subgames. These subgames are then replaced by their respective Nash equilibrium. This process of replacing a subgame with the associated Nash equilibrium is then continued up to the next level of subgames till we reach a subgame perfect Nash equilibrium. The process is illustrated below:</p> <p></p> <p>In this figure the method of Backward Induction involves first solving the two subgames (ii and iii) for Nash equilibrium. In subgame (ii), given that wife opts for Opera, husband\u2019s best response would be to choose Opera for a payoff of 1 rather than going for Boxing match resulting in payoff of 0. Similarly, in subgame (iii), given that wife opts for Boxing, husband\u2019s best response will be to go for a Boxing match. Now, we replace the two subgames with their respective Nash equilibrium strategies to get a simple game where wife is to decide. Wife gets a payoff of 3 if she goes for Opera, while Boxing results in a payoff of 1. Nash equilibrium strategy yielding the higher payoff will be for her to go for Opera. So, the wife\u2019s best response is to go for Opera. Thus, we get the subgame perfect Nash equilibrium outcome as (Opera | Opera).</p> <p>In Earlier sections we have talked about cooperative vs non-cooperative games. Now we want to take a deeper look at these two concepts.</p>"},{"location":"prerequisites/game_theory/#pareto-optimality-and-social-welfare-in-game-theory","title":"Pareto Optimality and Social Welfare in Game Theory","text":"<p>In multi-agent systems and game theory, decision-making often involves balancing efficiency and fairness. Two important concepts that help evaluate outcomes in such settings are Pareto Optimality and Social Welfare.</p>"},{"location":"prerequisites/game_theory/#pareto-optimality","title":"Pareto Optimality","text":"<p>Pareto Optimality (or Pareto Efficiency) is a fundamental concept in game theory and economics that defines an optimal allocation of resources where no player can be made better off without making another player worse off.</p> <p>Definition of Pareto Optimality</p> <p>An outcome is Pareto Optimal if there is no alternative outcome where:</p> <ul> <li>At least one player is better off,</li> <li>Without making another player worse off.</li> </ul> <p>If such an alternative outcome exists, then the current outcome is Pareto inefficient because a better allocation is possible.</p> <ul> <li> <p>Example: Pareto Optimality in a Multi-Agent System</p> <p>Consider two autonomous delivery robots sharing a battery charging station.</p> Robot 1\u2019s Battery Level (%) Robot 2\u2019s Battery Level (%) 50% 50% 70% 30% <ul> <li>If Robot 1 gets more charge (70%) while Robot 2 gets less (30%), this is not necessarily Pareto inefficient, as long as Robot 2 cannot be made better off without decreasing Robot 1\u2019s charge.</li> <li>However, if there exists a way to increase Robot 2\u2019s charge without lowering Robot 1\u2019s charge, then the initial allocation was Pareto inefficient.</li> </ul> </li> </ul> <p>Pareto Improvement</p> <p>A Pareto Improvement is a change where at least one player benefits without hurting others.</p> <ul> <li> <p>If multiple Pareto improvements exist, we will keep improving until we reach a Pareto Optimal outcome where no further improvements are possible.</p> </li> <li> <p>Example: Pareto Improvement in Reinforcement Learning (RL)</p> <ul> <li> <p>Suppose two reinforcement learning agents are cooperating to navigate a grid.</p> </li> <li> <p>They find a policy where both reach their destinations, but one agent takes a longer path.</p> </li> <li> <p>If an alternative policy allows both agents to reach their destinations faster, then it is a Pareto Improvement.</p> </li> <li> <p>The policy that achieves the fastest time for both agents without worsening either one\u2019s outcome is Pareto Optimal.</p> </li> </ul> </li> </ul>"},{"location":"prerequisites/game_theory/#social-welfare","title":"Social Welfare","text":"<p>While Pareto Optimality focuses on efficiency, Social Welfare is about fairness and collective benefit.</p> <p>Definition of Social Welfare</p> <p>Social Welfare refers to the overall well-being of all players in the system. A high Social Welfare means that the total benefit across all players is maximized.</p> <p>Measuring Social Welfare</p> <p>There are different ways to measure Social Welfare in game theory and multi-agent systems:</p> <p>Utilitarian Social Welfare:</p> <ul> <li> <p>The total sum of all players' payoffs is maximized.</p> </li> <li> <p>Formula:</p> </li> </ul> \\[ W = U_1 + U_2 + \\cdots + U_n \\] <ul> <li>Example:</li> <li>If two AI agents receive rewards 5 and 8, the total social welfare is \\(5 + 8 = 13\\).</li> <li>A policy that increases their rewards to 6 and 10 would be preferred because total welfare increases to 16.</li> </ul> <p>Egalitarian Social Welfare:</p> <ul> <li> <p>Ensures fairness by maximizing the minimum utility among all players.</p> </li> <li> <p>Formula:</p> </li> </ul> \\[ W = \\min(U_1, U_2, \\dots, U_n) \\] <ul> <li>Example:</li> <li>If three reinforcement learning agents have rewards 3, 6, and 10, the egalitarian welfare is \\(\\min(3, 6, 10) = 3\\).</li> <li>A policy improving the worst-off agent\u2019s reward to 5 (even if the total sum decreases) may be preferred under egalitarian principles.</li> </ul> <p>Nash Social Welfare:</p> <ul> <li>Balances efficiency and fairness by maximizing the geometric mean of payoffs:</li> </ul> \\[ W = (U_1 \\times U_2 \\times \\cdots \\times U_n)^{1/n} \\] <ul> <li>Used in multi-agent learning where proportional fairness matters.</li> </ul>"},{"location":"prerequisites/game_theory/#pareto-optimality-and-social-welfare-in-ai-multi-agent-systems","title":"Pareto Optimality and Social Welfare in AI &amp; Multi-Agent Systems","text":"Application Role of Pareto Optimality Role of Social Welfare Multi-Agent Reinforcement Learning (MARL) Ensures agents do not waste rewards. Ensures fair reward distribution among agents. Resource Allocation (Cloud AI, IoT) Avoids inefficient use of resources. Ensures fair and proportional access to resources. AI Fairness in Decision-Making Prevents wasteful policies. Ensures AI does not discriminate against disadvantaged groups. Autonomous Vehicle Coordination Ensures vehicles optimize traffic flow. Prevents unfair delays for certain cars. <p>&lt;![endif]--&gt;</p>"},{"location":"prerequisites/game_theory/#cooperative-vs-non-cooperative-games_1","title":"Cooperative vs Non-Cooperative Games","text":""},{"location":"prerequisites/game_theory/#cooperative-games","title":"Cooperative Games","text":"<p>In game theory, strategic interactions between players can be categorized into two broad classes: cooperative and non-cooperative games. Cooperative game theory focuses on situations where players can form binding agreements and collaborate to achieve better outcomes collectively. Unlike non-cooperative games, where players act independently and make decisions to maximize their individual payoffs, cooperative games emphasize coalition formation and how the collective payoff should be distributed among participants.</p> <p>What is a Cooperative Game?</p> <p>A cooperative game is defined by:</p> <ol> <li>A set of players who can form coalitions (groups).</li> <li>A value function (v) that assigns a collective payoff to each coalition.</li> <li>A method for distributing payoffs among coalition members in a fair and stable manner.</li> </ol> <p>Players in cooperative games can negotiate, form alliances, and share resources to improve their outcomes compared to acting alone. These games are often modeled using the characteristic function form, which describes how much value any subset of players (coalition) can generate collectively.</p> <ul> <li> <p>Example: Cooperative vs. Non-Cooperative Games</p> <ul> <li> <p>Non-Cooperative Game: Two companies competing in a market decide their prices independently.</p> </li> <li> <p>Cooperative Game: Multiple companies form a cartel to maximize joint profits and then distribute the gains among members.</p> </li> </ul> </li> </ul> <p>Cooperative games arise in many real-world scenarios, such as team-based decision-making, profit-sharing among firms, coalition politics, and multi-agent systems in artificial intelligence.</p> <p>Why Cooperative Game Theory Matters in AI and Multi-Agent Systems?</p> <p>Cooperative game theory is highly relevant in multi-agent reinforcement learning (MARL), where:</p> <ul> <li>Agents must collaborate to achieve a common goal (e.g., robots working together in a warehouse).</li> <li>The system needs to fairly distribute rewards among agents based on their contributions.</li> <li>Stability and fairness must be ensured in coalition-based AI systems (e.g., cloud computing resource allocation).</li> </ul> <p>To address these challenges, game theorists developed solution concepts that determine how to fairly distribute rewards in cooperative settings. One of the most important concepts is the Shapley Value, which provides a fair way to allocate payoffs based on each player's contribution.</p> <p>Since cooperative games involve forming coalitions, an important question arises:</p> <p>\"How should we fairly distribute the total gains among the players in a way that reflects their individual contributions?\"</p> <p>The Shapley Value answers this question by providing a mathematically fair distribution of rewards among coalition members. It ensures that:</p> <ul> <li>Players who contribute more receive a higher share.</li> <li>The distribution is consistent and stable.</li> <li>The solution satisfies fairness principles, such as symmetry, efficiency, and additivity.</li> </ul> <p>Now, let\u2019s explore the Shapley Value in detail, including how it is computed and why it is important.</p> <p>Shapley Value:</p> <p>In cooperative games, players form coalitions to achieve a collective goal, but an important question arises:</p> <p>\"How should the total reward be fairly distributed among the players based on their contributions?\"</p> <p>The Shapley Value, introduced by Lloyd Shapley in 1953, provides a mathematically fair way to allocate payoffs among players in a coalition based on their marginal contributions.</p> <p>Why Do We Need Shapley Value?</p> <p>In many real-world cooperative settings, multiple agents contribute to a shared outcome, but their contributions may not be equal. Consider the following scenarios:</p> <ul> <li>A multi-agent reinforcement learning (MARL) team is completing a task together, where some agents contribute more than others.</li> <li>Companies forming an alliance, where some firms provide more resources or technology.</li> <li>A group of researchers working on a project, where some contribute more ideas or effort.</li> </ul> <p>A fair reward distribution must ensure that:</p> <ol> <li>Players who contribute more get a larger share.</li> <li>The total value is distributed among all players.</li> <li>The reward distribution remains stable and consistent.</li> </ol> <p>The Shapley Value satisfies these conditions and is widely used in game theory, AI, and economics.</p> <p>Defining the Shapley Value</p> <p>For a cooperative game with:</p> <ul> <li>A set of \\(N\\) players, denoted as \\(N = \\{1, 2, \\dots, n\\}\\).</li> <li>A characteristic function \\(v(S)\\) that assigns a value to each coalition \\(S\\) (where \\(v(\\varnothing) = 0\\)).</li> </ul> <p>The Shapley Value of a player \\(i\\) is given by:</p> \\[ \\phi_i(v) = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!(n - |S| - 1)!}{n!} \\left[ v(S \\cup \\{i\\}) - v(S) \\right] \\] <p>Where:</p> <ul> <li>\\(S\\) is a subset of players excluding \\(i\\).</li> <li>\\(v(S \\cup \\{i\\}) - v(S)\\) represents player \\(i\\)\u2019s marginal contribution to coalition \\(S\\).</li> <li>The term \\(\\frac{|S|!(n - |S| - 1)!}{n!}\\) is a weight factor, ensuring fairness by considering all possible ways in which players could join the coalition.</li> </ul>"},{"location":"prerequisites/game_theory/#shapley-value-calculation","title":"Shapley Value Calculation","text":"<p>Scenario: A Team Project</p> <p>Three students (Alice, Bob, and Charlie) are working on a project together:</p> <ul> <li>Alice alone can complete 30% of the project.</li> <li>Bob alone can complete 20%.</li> <li>Charlie alone can complete 50%.</li> <li>Together, they complete 100%.</li> </ul> <p>The characteristic function \\(v(S)\\) is defined as: - \\(v(\\{A\\}) = 30\\), \\(v(\\{B\\}) = 20\\), \\(v(\\{C\\}) = 50\\) - \\(v(\\{A, B\\}) = 50\\), \\(v(\\{A, C\\}) = 80\\), \\(v(\\{B, C\\}) = 70\\) - \\(v(\\{A, B, C\\}) = 100\\)</p> <p>Step 1: Compute Marginal Contributions</p> <p>For Alice (A):</p> <ul> <li> <p>Joining empty coalition: \\(v(\\{A\\}) - v(\\varnothing) = 30 - 0 = 30\\)</p> </li> <li> <p>Joining \\(\\{B\\}\\): \\(v(\\{A, B\\}) - v(\\{B\\}) = 50 - 20 = 30\\)</p> </li> <li> <p>Joining \\(\\{C\\}\\): \\(v(\\{A, C\\}) - v(\\{C\\}) = 80 - 50 = 30\\)</p> </li> <li> <p>Joining \\(\\{B, C\\}\\): \\(v(\\{A, B, C\\}) - v(\\{B, C\\}) = 100 - 70 = 30\\)</p> </li> </ul> <p>Similarly, we compute for Bob (B) and Charlie \\(C\\).</p> <p>Step 2: Apply Shapley Value Formula</p> <p>Using the formula and computing weighted averages, we find:</p> <ul> <li> <p>Alice\u2019s Shapley Value = 32%</p> </li> <li> <p>Bob\u2019s Shapley Value = 18%</p> </li> <li> <p>Charlie\u2019s Shapley Value = 50%</p> </li> </ul> <p>Thus, the final reward distribution is:</p> <ul> <li> <p>Alice gets 32% of the total reward</p> </li> <li> <p>Bob gets 18%.</p> </li> <li> <p>Charlie gets 50%.</p> </li> </ul> <p>This fairly distributes the payoff based on marginal contributions.</p> <p>Properties of the Shapley Value</p> <p>The Shapley Value satisfies the following fairness conditions:</p> <ol> <li>Efficiency: The total value is fully distributed among all players.</li> <li>Symmetry: If two players contribute equally, they receive the same payoff.</li> <li>Additivity: If two games are combined, the Shapley Value for each player remains consistent.</li> <li>Null Player Property: A player who contributes nothing receives zero.</li> </ol> <p>These properties make it an ideal solution for fair reward allocation.</p>"},{"location":"prerequisites/game_theory/#non-cooperative-games","title":"Non-Cooperative Games","text":"<p>While cooperative game theory focuses on coalitions and binding agreements, non-cooperative game theory analyzes situations where players act independently, often in competition with one another.</p> <p>In non-cooperative games, players make decisions strategically, anticipating the actions of others, but they cannot form enforceable agreements. Instead, each player maximizes their own payoff, which may or may not align with the interests of others.</p> <p>Characteristics of Non-Cooperative Games</p> <p>A non-cooperative game consists of:</p> <ol> <li>A set of players: The decision-makers in the game.</li> <li>A set of strategies: The possible actions each player can take.</li> <li>A payoff function: Defines the reward each player receives based on the combination of strategies chosen.</li> </ol> <p>Unlike cooperative games, where players can negotiate and share payoffs, non-cooperative games assume self-interested behavior, often leading to competitive interactions.</p> <ul> <li> <p>Examples of Non-Cooperative Games</p> <ul> <li> <p>Prisoner's Dilemma: Two criminals must decide whether to confess or remain silent. The best outcome collectively is for both to stay silent, but since they act selfishly, they both confess and receive a worse outcome.</p> </li> <li> <p>In reinforcement learning, agents trained independently often develop competitive strategies, such as in self-play training for AI models (e.g., AlphaZero, OpenAI Five).</p> </li> </ul> </li> </ul> <p>Important Solution Concepts in Non-Cooperative Games</p> <p>Several solution concepts help analyze optimal decision-making in non-cooperative settings:</p> <p>Nash Equilibrium:</p> <ul> <li>A stable state where no player has an incentive to unilaterally change their strategy.</li> <li>Found in self-play AI (AlphaZero) and game-theoretic RL models.</li> </ul> <p>Mixed Strategy Nash Equilibrium:</p> <ul> <li>Used when no pure strategy equilibrium exists.</li> <li>Involves randomizing actions to prevent exploitation, such as in security and adversarial AI.</li> </ul> <p>Subgame Perfect Nash Equilibrium (SPNE):</p> <ul> <li>Ensures optimal decision-making at every stage of a sequential game.</li> <li>Found using Backward Induction, commonly applied in hierarchical AI and planning algorithms.</li> </ul>"},{"location":"prerequisites/game_theory/#authors","title":"Author(s)","text":"<ul> <li> <p>Faezeh Sadeghi</p> <p>Teaching Assistant</p> <p>fz.saadeghi@gmail.com</p> <p> </p> </li> </ul>"},{"location":"prerequisites/information_theory/","title":"Information Theory","text":""},{"location":"prerequisites/linear_algebra/","title":"Linear Algebra","text":""},{"location":"prerequisites/numerical_optimization/","title":"Numerical Optimization","text":""},{"location":"prerequisites/numerical_optimization/#fundamentals","title":"Fundamentals","text":""},{"location":"prerequisites/numerical_optimization/#taylors-theorem","title":"Taylor\u2019s Theorem","text":"<p>Suppose that \\( f : \\mathbb{R}^n \\to \\mathbb{R} \\) is continuously differentiable and that \\( p \\in \\mathbb{R}^n \\). Then we have that</p> \\[ f(x + p) \\approx f(x) + \\nabla f(x + tp)^T p \\] <p>for some \\( t \\in (0, 1) \\). Moreover, if \\( f \\) is twice continuously differentiable, we have that</p> \\[ f(x + p) \\approx f(x) + \\int_0^1 \\nabla^2 f(x + tp) p \\, dt \\] <p>and that</p> \\[ f(x + p) \\approx f(x) + \\nabla f(x)^T p + \\frac{1}{2} p^T \\nabla^2 f(x + tp) p \\] <p>for some \\( t \\in (0, 1) \\).</p> <p>Necessary conditions for optimality are derived by assuming that \\( x^* \\) is a local minimizer and then proving facts about \\( \\nabla f(x^*) \\) and \\( \\nabla^2 f(x^*) \\).</p>"},{"location":"prerequisites/numerical_optimization/#first-order-necessary-conditions","title":"First-Order Necessary Conditions","text":"<p>If \\( x^* \\) is a local minimizer and \\( f \\) is continuously differentiable in an open neighborhood of \\( x^* \\), then</p> \\[ \\nabla f(x^*) = 0. \\]"},{"location":"prerequisites/numerical_optimization/#second-order-sufficient-conditions","title":"Second-Order Sufficient Conditions","text":"<p>Suppose that \\( \\nabla^2 f \\) is continuous in an open neighborhood of \\( x^* \\), that \\( \\nabla f(x^*) = 0 \\), and \\( \\nabla^2 f(x^*) \\) is positive definite. Then, \\( x^* \\) is a strict local minimizer of \\( f \\).</p>"},{"location":"prerequisites/numerical_optimization/#theorem","title":"Theorem","text":"<p>When \\( f \\) is convex, any local minimizer \\( x^* \\) is a global minimizer of \\( f \\). If, in addition, \\( f \\) is differentiable, then any stationary point \\( x^* \\) is a global minimizer of \\( f \\).</p>"},{"location":"prerequisites/numerical_optimization/#definition","title":"Definition","text":"<p>A convex function is a function where, for any two points \\( x_1 \\) and \\( x_2 \\) in its domain, and any \\( \\lambda \\in [0, 1] \\), the function value at the point \\( \\lambda x_1 + (1-\\lambda) x_2 \\) is less than or equal to the weighted average of the function values at \\( x_1 \\) and \\( x_2 \\):</p> \\[ f(\\lambda x_1 + (1-\\lambda) x_2) \\leq \\lambda f(x_1) + (1-\\lambda) f(x_2) \\]"},{"location":"prerequisites/numerical_optimization/#theorem_1","title":"Theorem","text":"<p>Let \\( C \\subseteq \\mathbb{R}^n \\) be a convex set and let \\( f: \\mathbb{R}^n \\to \\mathbb{R} \\) be differentiable over \\( \\mathbb{R}^n \\). (a) The function \\( f \\) is convex over \\( C \\) iff \\( f(z) \\geq f(x) + (z - x) \\cdot \\nabla f(x) \\), for all \\( x, z \\in C \\). (b) If the inequality is strict whenever \\( x \\neq z \\), then \\( f \\) is strictly convex over \\( C \\).</p> <p></p>"},{"location":"prerequisites/numerical_optimization/#rate-of-convergence","title":"Rate of Convergence","text":"<p>Let \\( \\{ x_k \\mid x_k \\in \\mathbb{R}^n \\} \\) be a sequence such that \\( x^* = \\lim_{k \\to \\infty} x_k \\).</p> <p>The convergence is: - Q-linear if for a constant \\( r \\in (0,1) \\):</p> \\[\\| x_{k+1} - x^* \\| \\leq r \\| x_k - x^* \\|\\] <ul> <li>Q-superlinear if:</li> </ul> \\[\\lim_{k \\to \\infty} \\frac{\\| x_{k+1} - x^* \\|}{\\| x_k - x^* \\|} = 0\\] <ul> <li>\\( p \\) Q-order if for a constant \\( M \\):</li> </ul> \\[\\| x_{k+1} - x^* \\| \\leq M \\| x_k - x^* \\|^p\\]"},{"location":"prerequisites/numerical_optimization/#line-search","title":"Line Search","text":"<p>Often, in the methods we use for optimizing nonlinear functions, finding descent directions at the current point and moving in those directions to find a point with a lower value is done iteratively. However, the issue that arises is the step size, which itself becomes an optimization problem called Line Search.</p> <p>Which its problem is as follows:</p> \\[\\min_{\\alpha &gt; 0} f(x_k + \\alpha p_k)\\] <p>we would derive the maximum benefit from the direction \\( p_k \\).</p>"},{"location":"prerequisites/numerical_optimization/#conditions","title":"Conditions","text":"<p>The ideal choice would be the global minimizer of the univariate function \\( \\varphi(\\cdot) \\) defined by</p> \\[\\varphi(\\alpha) = f(x_k + \\alpha p_k), \\quad \\alpha &gt; 0,\\] <p></p> <p>The sufficient decrease and curvature conditions are known collectively as the Wolfe conditions</p> <ul> <li>\\( f(x_k + \\alpha p_k) \\leq f(x_k) + c_1 \\alpha \\nabla f_k^T p_k \\) (sufficient decrease)</li> <li>\\( \\nabla f(x_k + \\alpha_k p_k)^T p_k \\geq c_2 \\nabla f_k^T p_k \\) (curvature condition)</li> </ul> <p>with \\( 0 &lt; c_1 &lt; c_2 &lt; 1 \\).</p> <p></p> <p>Figure: Sufficient decrease condition</p> <p></p> <p>Figure: The curvature condition</p>"},{"location":"prerequisites/numerical_optimization/#theorem_2","title":"Theorem:","text":"<p>Consider any iteration of the form \\( x_{k+1} = x_k + \\alpha_k p_k \\), where \\( p_k \\) is a descent direction and \\( \\alpha_k \\) satisfies the Wolfe conditions. Suppose that \\( f \\) is bounded below in \\( \\mathbb{R}^n \\) and that \\( f \\) is continuously differentiable in an open set \\( \\mathcal{N} \\) containing the level set \\( \\mathcal{L} \\equiv \\{ x : f(x) \\leq f(x_0) \\} \\), where \\( x_0 \\) is the starting point of the iteration. Assume also that the gradient \\( \\nabla f \\) is Lipschitz continuous on \\( \\mathcal{N} \\), that is, there exists a constant \\( L &gt; 0 \\) such that</p> \\[\\| \\nabla f(x) - \\nabla f(\\tilde{x}) \\| \\leq L \\| x - \\tilde{x} \\|, \\quad \\text{for all } x, \\tilde{x} \\in \\mathcal{N}.\\] <p>Then</p> \\[\\sum_{k \\geq 0} \\cos^2 \\theta_k \\| \\nabla f_k \\|^2 &lt; \\infty.\\] <p>Since \\( |\\cos \\theta_k| \\geq c &gt; 0 \\), we can see that the gradients \\( \\| \\nabla f_k \\| \\) must shrink to zero as the iterations progress. Otherwise, the sum would not be finite, which leads to the conclusion that \\( \\nabla f_k \\) converges to zero.</p>"},{"location":"prerequisites/numerical_optimization/#trust-region","title":"Trust Region","text":"<p>The Trust Region method uses the information gathered about \\( f \\) to construct a model function \\( m_k \\) whose behavior near the current point \\( x_k \\) is similar to that of the actual objective function \\( f \\). Because the model \\( m_k \\) may not be a good approximation of \\( f \\) when \\( x \\) is far from \\( x_k \\), we restrict the search for a minimizer of \\( m_k \\) to some region around \\( x_k \\).</p> \\[\\min_{p} \\, m_k(x_k + p), \\, \\text{where} \\, x_k + p \\, \\text{lies inside the trust region.}\\] <p>Usually, the trust region is a ball defined by \\( \\|p\\|_2 \\leq \\delta \\), where the scalar \\( \\delta &gt; 0 \\) is called the trust-region radius. For example, one of the functions that can be suitable for \\( m_k \\) is a quadratic function, which is obtained using the Taylor expansion of the function \\( f \\) around the point \\( x_k \\):</p> \\[m_k(x_k + p) = f_k + p^T \\nabla f_k + \\frac{1}{2} p^T B_k p\\] <p>Where the matrix \\( B_k \\) is either the Hessian \\( H_k = \\nabla^2 f_k \\) or some approximation to it.</p> <p>Now, using the mentioned \\( m_k \\), and optimizing it (taking the gradient with respect to \\( p \\)), we reach two descent directions: one is the Newton method and the other is the quasi-Newton method, which is used when directly computing the Hessian matrix is costly.</p>"},{"location":"prerequisites/numerical_optimization/#newton-direction","title":"Newton direction:","text":"\\[\\nabla_p \\, m_k(x_k+p) = 0 \\xrightarrow[\\exists \\, \\text{inverse}]{} p_N = - (\\nabla^2 f_k)^{-1} \\nabla f_k\\]"},{"location":"prerequisites/numerical_optimization/#quasi-newton-direction","title":"Quasi-Newton direction:","text":"\\[\\nabla_p \\, m_k(x_k+p) = 0 \\xrightarrow[\\exists \\, \\text{inverse}]{} p_{qN} = - (B_k)^{-1} \\nabla f_k\\]"},{"location":"prerequisites/numerical_optimization/#recursive-method-for-quasi-newton","title":"Recursive method for quasi-Newton","text":"\\[B_{k+1} = B_k + \\frac{(y_k - B_k s_k)(y_k - B_k s_k)^T}{(y_k - B_k s_k)^T s_k}\\] <p>We can rewrite it in the following form (or BFGS formula)</p> \\[B_{k+1} = B_k - \\frac{B_k s_k s_k^T}{s_k^T B_k s_k} + \\frac{y_k y_k^T}{y_k^T s_k}\\] <p>where</p> \\[s_k = x_{k+1} - x_k, \\quad y_k = \\nabla f_{k+1} - \\nabla f_k\\]"},{"location":"prerequisites/numerical_optimization/#theory-of-constrained-optimization","title":"Theory of Constrained Optimization","text":"<p>In some optimization problems we have, the domain we search for the optimal value is not the entire Euclidean space, but rather, under certain conditions, we are looking for the optimal function. For this reason, we use the theory of Constrained Optimization.</p> <p>A general formulation:</p> \\[\\min_{x \\in \\mathbb{R}^n} f(x) \\quad \\text{subject to} \\quad  \\begin{cases} c_i(x) = 0, &amp; \\text{for } i \\in \\mathcal{E}, \\\\ c_i(x) \\geq 0, &amp; \\text{for } i \\in \\mathcal{I}, \\end{cases}\\] <p>where \\( f \\) and the functions \\( c_i \\) are all smooth, real-valued functions on a subset of \\( \\mathbb{R}^n \\), and \\( \\mathcal{I} \\) and \\( \\mathcal{E} \\) are two finite sets of indices. As before, we call \\( f \\) the objective function, while \\( c_i \\),</p> <p>\\( i \\in \\mathcal{E} \\) are the equality constraints and \\( c_i \\), \\( i \\in \\mathcal{I} \\) are the inequality constraints.</p> <p>Also we can define the feasible set \\( \\Omega \\) to be the set of points \\( x \\) that satisfy the constraints; that is,</p> \\[\\Omega = \\{ x \\mid c_i(x) = 0, \\; i \\in \\mathcal{E}; \\; c_i(x) \\geq 0, \\; i \\in \\mathcal{I} \\}.\\] <p>so that we can rewrite:</p> \\[\\min_{x \\in \\Omega} f(x).\\]"},{"location":"prerequisites/numerical_optimization/#definitions","title":"Definitions:","text":"<ul> <li>Standard form problem (not necessarily convex)</li> </ul> <p>Minimize \\( f(x) \\)</p> <p>subject to</p> \\[ 0 \\leq g_i(x), \\quad i = 1, \\dots, m \\] \\[ h_i(x) = 0, \\quad i = 1, \\dots, p \\] <p>Variable \\( x \\in \\mathbb{R}^n \\), domain \\( D \\), optimal value \\( p^\\star \\)</p> <ul> <li>Lagrangian:</li> </ul> <p>\\( L : \\mathbb{R}^n \\times \\mathbb{R}^m \\times \\mathbb{R}^p \\rightarrow \\mathbb{R}, \\quad \\text{with } \\text{dom } L = D \\times \\mathbb{R}^m \\times \\mathbb{R}^p \\)</p> \\[ L(x, \\lambda, \\nu) = f(x) - \\sum_{i=1}^{m} \\lambda_i g_i(x) + \\sum_{i=1}^{p} \\nu_i h_i(x) \\] <ul> <li>Active/Inactive Constraint:</li> </ul> <p>An inequality constraint \\( h_i(x) \\geq 0 \\) is called active at \\( x^* \\in \\Omega \\) if \\( h_i(x^*) = 0 \\) and otherwise inactive.</p> <ul> <li>Active Set:</li> </ul> <p>The index set \\( A(x^*) \\subset \\{1, \\dots, q\\} \\) of active constraints is called the active set.</p> <p>Remark: Inactive constraints do not influence \\( T_\\Omega(x^*) \\).</p> <ul> <li>LICQ (linear independence constraint qualification):</li> </ul> <p>The linear independence constraint qualification (LICQ) holds at \\( x^* \\in \\Omega \\) iff all vectors \\( \\nabla g_i(x^*) \\) for \\( i \\in \\{1, \\dots, m\\} \\) and \\( \\nabla h_i(x^*) \\) for \\( i \\in A(x^*) \\) are linearly independent.</p> <p>Remark: This is a technical condition, and is usually satisfied.</p> <ul> <li>Linearized Feasible Cone:</li> </ul> <p>\\(F(x^*) = \\{ p \\mid \\nabla g_i(x^*)^T p = 0, \\; i = 1, \\dots, m \\; \\text{and} \\; \\nabla h_i(x^*)^T p \\geq 0, \\; i(x^*\\}\\)</p> <p>is called the linearized feasible cone at \\( x^* \\in \\Omega \\).</p>"},{"location":"prerequisites/numerical_optimization/#example-1","title":"Example 1:","text":"\\[\\min_{x} \\, f(x) = x_1 + x_2\\] \\[\\text{s.t.}\\] \\[c_1(x) = x_1^2 + x_2^2 - 2 = 0.\\]"},{"location":"prerequisites/numerical_optimization/#solution","title":"Solution:","text":"<p>The optimal solution is \\( x^* = (-1, -1)^T \\). From any other point on the circle, one can move in directions that maintain feasibility (stay on the circle) while decreasing \\( f(x) \\). For instance, from the point \\( x = (2, 0)^T \\), any clockwise movement on the circle decreases \\( f(x) \\).</p> <p>At the solution \\( x^* \\), the gradient of the objective function \\( \\nabla f(x) \\) is parallel to the gradient of the constraint \\( \\nabla c_1(x) \\), satisfying the Lagrange multiplier condition:</p> \\[\\nabla f(x^*) = \\lambda^* \\nabla c_1(x^*),\\] <p>where \\( \\lambda^* = -\\frac{1}{2} \\).</p> <p>To ensure feasibility, small steps \\( s \\) must satisfy the condition:</p> \\[\\nabla c_1(x)^T s = 0.\\] <p>To decrease the objective function, the step must satisfy:</p> \\[\\nabla f(x)^T s &lt; 0.\\] <p>We can express this in terms of first-order Taylor series approximations. To retain feasibility with respect to the constraint, the first-order approximation gives:</p> \\[c_1(x + s) \\approx c_1(x) + \\nabla c_1(x)^T s = 0,\\] <p>which simplifies to:</p> \\[\\nabla c_1(x)^T s = 0.\\] <p>Similarly, to decrease the objective function \\( f(x) \\), the first-order approximation is:</p> \\[f(x + s) \\approx f(x) + \\nabla f(x)^T s,\\] <p>and for \\( f(x) \\) to decrease, we require:</p> \\[\\nabla f(x)^T s &lt; 0.\\] <p>The optimal step \\( d \\) can be calculated as:</p> \\[d = - \\frac{\\nabla c_1(x) \\nabla f(x)^T}{\\|\\nabla c_1(x)\\|^2} \\nabla f(x),\\] <p>which ensures feasibility and decreases the objective function.</p> <p>By introducing the Lagrangian function</p> \\[\\mathcal{L}(x, \\lambda_1) = f(x) - \\lambda_1 c_1(x),\\] <p>and noting that</p> \\[\\nabla_x \\mathcal{L}(x, \\lambda_1) = \\nabla f(x) - \\lambda_1 \\nabla c_1(x),\\] <p>we can state the condition equivalently as follows: At the solution \\( x^* \\), there is a scalar \\( \\lambda_1^* \\) such that</p> \\[\\nabla_x \\mathcal{L}(x^*, \\lambda_1^*) = 0.\\]"},{"location":"prerequisites/numerical_optimization/#example-2","title":"Example 2:","text":"<p>\\( \\min \\: \\: \\: \\: \\: x_1^2 + x_2^2 \\)</p> \\[ \\text{s.t.} \\] \\[ x_2 - 1 - x_1^2 \\geq 0, \\] \\[ x_1 - 1 \\geq 0. \\] <p>where \\( x \\in \\mathbb{R}^2 \\).</p> <p></p>"},{"location":"prerequisites/numerical_optimization/#theorem-first-order-necessary-conditions","title":"Theorem (First-Order Necessary Conditions):","text":"<p>Suppose that \\( x^* \\) is a local solution of the optimization problem:</p> <p>\\( \\min_{x \\in \\mathbb{R}^n} f(x) \\)</p> <p>subject to:</p> \\[ c_i(x) = 0, \\quad i \\in \\mathcal{E}, \\quad c_i(x) \\geq 0, \\quad i \\in \\mathcal{I}, \\] <p>where \\( x^* \\) is the solution, and \\( \\lambda^* \\) is the corresponding Lagrange multiplier vector. Then, the following conditions hold at \\( x^* \\):</p> <ul> <li>(a) \\( \\nabla_x \\mathcal{L}(x^*, \\lambda^*) = 0 \\)</li> <li>(b) \\( c_i(x^*) = 0 \\quad \\text{for all} \\quad i \\in \\mathcal{E} \\)</li> <li>(c) \\( c_i(x^*) \\geq 0 \\quad \\text{for all} \\quad i \\in \\mathcal{I} \\)</li> <li>(d) \\( \\lambda_i^* \\geq 0 \\quad \\text{for all} \\quad i \\in \\mathcal{I} \\)</li> <li>(e) \\( \\lambda_i^* c_i(x^*) = 0 \\quad \\text{for all} \\quad i \\in \\mathcal{E} \\cup \\mathcal{I} \\)</li> </ul> <p>These conditions are known as the Karush-Kuhn-Tucker (KKT) conditions. The last condition is a complementarity condition, meaning that for any constraint \\( i \\), either \\( c_i(x^*) = 0 \\) (active) or \\( \\lambda_i^* = 0 \\) (inactive).</p> <p>Finally, the necessary condition can be rewritten as:</p> \\[ 0 = \\nabla_x \\mathcal{L}(x^*, \\lambda^*) = \\nabla f(x^*) - \\sum_{i \\in A(x^*)} \\lambda_i^* \\nabla c_i(x^*). \\] <p>Definition 12.5 (Strict Complementarity):</p> <p>Given a local solution \\( x^* \\) of the optimization problem and a vector \\( \\lambda^* \\) satisfying the first-order necessary conditions, we say that the strict complementarity condition holds if exactly one of \\( \\lambda_i^* \\) and \\( c_i(x^*) \\) is zero for each index \\( i \\in \\mathcal{I} \\). In other words, we have that \\( \\lambda_i^* &gt; 0 \\) for each \\( i \\in \\mathcal{I} \\cap A(x^*) \\).</p> <p>Example:</p> <p>Consider the feasible region illustrated in Figure below and described by the four constraints. By restating the constraints in the standard form of the optimization problem and including an objective function, the problem becomes:</p> \\[ \\min_x \\left( \\left( x_1 - \\frac{3}{2} \\right)^2 + \\left( x_2 - \\frac{1}{2} \\right)^4 \\right) \\] <p>subject to \\( \\begin{bmatrix} 1 - x_1 - x_2 \\\\ 1 - x_1 + x_2 \\\\ 1 + x_1 - x_2 \\\\ 1 + x_1 + x_2 \\end{bmatrix} \\geq 0. \\)</p> <p>It is fairly clear from Figure below that the solution is \\( x^* = (1, 0)^T \\). The first and second constraints are active at this point. Denoting them by \\( c_1 \\) and \\( c_2 \\) (and the inactive constraints by \\( c_3 \\) and \\( c_4 \\)), we have</p> <p>\\( \\nabla f(x^*) = \\begin{bmatrix} -1 \\\\ -1 \\end{bmatrix}, \\quad \\nabla c_1(x^*) = \\begin{bmatrix} -1 \\\\ -1 \\end{bmatrix}, \\quad \\nabla c_2(x^*) = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}. \\)</p> <p>Therefore, the KKT conditions are satisfied when we set</p> \\[ \\lambda^* = \\begin{bmatrix} \\frac{3}{4} \\\\ \\frac{1}{4} \\\\ 0 \\\\ 0 \\end{bmatrix}. \\] <p></p> <p>Lemma</p> <p>Let \\( x^* \\) be a feasible point. The following two statements are true:</p> <ul> <li> <p>(i) \\( T_\\Omega(x^*) \\subset F(x^*) \\).</p> </li> <li> <p>(ii) If the LICQ condition is satisfied at \\( x^* \\), then \\( F(x^*) = T_\\Omega(x^*) \\).</p> </li> </ul> <p>Theorem</p> <p>If \\( x^* \\) is a local solution of the optimization problem, then we have</p> \\[ \\nabla f(x^*)^T d \\geq 0, \\quad \\text{for all} \\quad d \\in T_\\Omega(x^*). \\] <p>The most important step in proving the previous theorem is a classical result known as Farkas' Lemma.</p> <p>Farkas' Lemma</p> <p></p> <p>This lemma defines a cone \\( K \\) as:</p> \\[ K = \\{ By + Cw \\mid y \\geq 0 \\}. \\] <p>For a vector \\( g \\in \\mathbb{R}^n \\), Farkas' Lemma states that one of the following alternatives is true:</p> <ol> <li>\\( g \\in K \\) (as shown on the left side of the figure),</li> <li>There exists a vector \\( d \\in \\mathbb{R}^n \\) such that:    \\( g^T d &lt; 0 , \\quad B^Td \\geq 0, \\quad C^Td = 0.\\)</li> </ol> <p>These two cases are illustrated in the above figure, where \\( B \\) has three columns, \\( C \\) is null, and \\( n = 2 \\).</p> <p>Let the cone \\( K \\) be defined as in the equation above. Given any vector \\( g \\in \\mathbb{R}^n \\), either \\( g \\in K \\) or there exists a vector \\( d \\in \\mathbb{R}^n \\) satisfying the condition above, but not both.</p>"},{"location":"prerequisites/numerical_optimization/#theorem-second-order-necessary-conditions","title":"Theorem (Second-Order Necessary Conditions).","text":"<p>Suppose that \\( x^* \\) is a local solution and that the LICQ condition is satisfied. Let \\( \\lambda^* \\) be the Lagrange multiplier vector for which the KKT conditions are satisfied. Then</p> \\[ w^T \\nabla^2_{xx} \\mathcal{L}(x^*, \\lambda^*) w \\geq 0, \\quad \\text{for all} \\ w \\in C(x^*, \\lambda^*). \\]"},{"location":"prerequisites/numerical_optimization/#theorem-second-order-sufficient-conditions","title":"Theorem (Second-Order Sufficient Conditions).","text":"<p>Suppose that for some feasible point \\( x^* \\in \\mathbb{R}^n \\) there is a Lagrange multiplier vector \\( \\lambda^* \\) such that the KKT conditions (12.34) are satisfied. Suppose also that</p> \\[ w^T \\nabla^2_{xx} \\mathcal{L}(x^*, \\lambda^*) w &gt; 0, \\quad \\text{for all } w \\in C(x^*, \\lambda^*), w \\neq 0. \\]"},{"location":"prerequisites/numerical_optimization/#duality","title":"Duality","text":"<p>Duality in optimization provides a method to derive an alternative problem, the dual, related to the original primal problem. Solving the dual can sometimes be easier and provides bounds on the primal solution.</p>"},{"location":"prerequisites/numerical_optimization/#standard-form-problem","title":"Standard Form Problem","text":"<p>Consider the optimization problem:</p> \\[ \\min f(x) \\] <p>subject to:</p> \\[ c_i(x) \\geq 0, \\quad i = 1, \\dots, m \\] \\[ h_i(x) = 0, \\quad i = 1, \\dots, p \\] <p>where \\( x \\in \\mathbb{R}^n \\) is the variable to optimize.</p>"},{"location":"prerequisites/numerical_optimization/#lagrangian","title":"Lagrangian","text":"<p>The Lagrangian is:</p> \\[ L(x, \\lambda, \\nu) = f_0(x) - \\sum_{i=1}^{m} \\lambda_i c_i(x) + \\sum_{i=1}^{p} \\nu_i h_i(x) \\] <p>where \\( \\lambda_i \\) and \\( \\nu_i \\) are the Lagrange multipliers associated with the inequality and equality constraints, respectively.</p>"},{"location":"prerequisites/numerical_optimization/#dual-function-and-problem","title":"Dual Function and Problem","text":"<p>The dual function is:</p> \\[ g(\\lambda, \\nu) = \\inf_{x \\in D} L(x, \\lambda, \\nu) \\] <p>The dual problem is:</p> \\[ \\max_{\\lambda \\geq 0, \\nu} g(\\lambda, \\nu) \\] <p>This dual formulation provides a lower bound on the optimal value of the primal problem and can often be easier to solve.</p>"},{"location":"prerequisites/numerical_optimization/#theorem_3","title":"Theorem","text":"<p>The function \\( g(\\lambda, \\nu) \\) defined by the dual function is concave, and its domain \\( D \\) is convex.</p>"},{"location":"prerequisites/numerical_optimization/#proof","title":"Proof:","text":"<p>For any \\( \\lambda^0 \\) and \\( \\lambda^1 \\) in \\( \\mathbb{R}^m \\), and any \\( \\nu^0 \\) and \\( \\nu^1 \\) in \\( \\mathbb{R}^p \\), and any \\( \\alpha \\in [0, 1] \\), we have:</p> \\[ L(x, (1 - \\alpha) \\lambda^0 + \\alpha \\lambda^1, (1 - \\alpha) \\nu^0 + \\alpha \\nu^1) = (1 - \\alpha) L(x, \\lambda^0, \\nu^0) + \\alpha L(x, \\lambda^1, \\nu^1) \\] <p>By taking the infimum of both sides in this expression, using the definition of \\( g(\\lambda, \\nu) \\), and applying the result that the infimum of a sum is greater than or equal to the sum of infima, we obtain:</p> \\[ g((1 - \\alpha) \\lambda^0 + \\alpha \\lambda^1, (1 - \\alpha) \\nu^0 + \\alpha \\nu^1) \\geq (1 - \\alpha) g(\\lambda^0, \\nu^0) + \\alpha g(\\lambda^1, \\nu^1) \\] <p>This confirms the concavity of \\( g(\\lambda, \\nu) \\).</p> <p>If both \\( \\lambda^0 \\) and \\( \\lambda^1 \\) belong to \\( D \\), and both \\( \\nu^0 \\) and \\( \\nu^1 \\) belong to \\( D \\), this inequality implies that:</p> \\[ g((1 - \\alpha) \\lambda^0 + \\alpha \\lambda^1, (1 - \\alpha) \\nu^0 + \\alpha \\nu^1) \\geq -\\infty \\] <p>and therefore, \\( (1 - \\alpha) \\lambda^0 + \\alpha \\lambda^1 \\in D \\) and \\( (1 - \\alpha) \\nu^0 + \\alpha \\nu^1 \\in D \\), verifying the convexity of the domain \\( D \\).</p>"},{"location":"prerequisites/numerical_optimization/#example","title":"Example:","text":"<p>Consider the problem:</p> \\[ \\min_{(x_1, x_2)} \\, 0.5(x_1^2 + x_2^2) \\quad \\text{subject to} \\quad x_1 - 1 \\geq 0 \\] <p>The Lagrangian for this problem is:</p> \\[ L(x_1, x_2, \\lambda_1) = 0.5(x_1^2 + x_2^2) - \\lambda_1(x_1 - 1) \\] <p>Holding \\( \\lambda_1 \\) fixed, we minimize the Lagrangian with respect to \\( x_1 \\) and \\( x_2 \\). The partial derivatives with respect to \\( x_1 \\) and \\( x_2 \\) give:</p> \\[ x_1 = \\lambda_1, \\quad x_2 = 0 \\] <p>Substituting these values into the Lagrangian, the dual function becomes:</p> \\[ q(\\lambda_1) = 0.5(\\lambda_1^2 + 0) - \\lambda_1(\\lambda_1 - 1) = -0.5\\lambda_1^2 + \\lambda_1 \\] <p>The dual problem is:</p> \\[ \\max_{\\lambda_1 \\geq 0} \\left( -0.5\\lambda_1^2 + \\lambda_1 \\right) \\] <p>Solving this, we find that \\( \\lambda_1 = 1 \\).</p>"},{"location":"prerequisites/numerical_optimization/#theorem-weak-duality","title":"Theorem (Weak Duality)","text":"<p>For any \\( \\bar{x} \\) feasible for the primal problem and any \\( \\lambda \\geq 0 \\), we have:</p> \\[ g(\\lambda, \\nu) \\leq f(\\bar{x}) \\]"},{"location":"prerequisites/numerical_optimization/#proof_1","title":"Proof","text":"<p>The dual function \\( g(\\lambda, \\nu) \\) is defined as:</p> \\[ g(\\lambda, \\nu) = \\inf_x \\left[ f(x) - \\lambda^T c(x) - \\nu^T h(x) \\right] \\] <p>Thus,</p> \\[ g(\\lambda, \\nu) = \\inf_x \\left[ f(x) - \\lambda^T c(x) - \\nu^T h(\\bar{x}) \\right] \\leq f(\\bar{x}) - \\lambda^T c(\\bar{x}) - \\nu^T h(\\bar{x}) \\leq f(\\bar{x}) \\] <p>where the final inequality follows from \\( \\lambda \\geq 0 \\), \\( h(\\bar{x}) = 0 \\), and \\( c(\\bar{x}) \\geq 0 \\).</p> <p>Definition: (Dual Norm):</p> <p>The dual norm of a vector \\( \\nu \\) in the dual space \\( V^* \\) is defined as:</p> \\[ \\| \\nu \\|_* = \\sup_{\\| x \\| \\leq 1} \\, \\nu^T x \\] <p>It measures the maximum value of \\( \\nu^T x \\) over all vectors \\( x \\) with \\( \\| x \\| \\leq 1 \\).</p>"},{"location":"prerequisites/numerical_optimization/#example-equality-constrained-norm-minimization","title":"Example: Equality Constrained Norm Minimization","text":"<p>Consider the problem:</p> \\[ \\min \\| x \\| \\quad \\text{subject to} \\quad A x = b \\] <p>The dual function is:</p> \\[ g(\\nu) =  \\begin{cases}  b^T \\nu &amp; \\text{if} \\quad \\| A^T \\nu \\|_* \\leq 1 \\\\ -\\infty &amp; \\text{otherwise} \\end{cases} \\] <p>where \\( \\| \\nu \\|_* \\) is the dual norm of \\( \\nu \\).</p> <p>If \\( \\| y \\|_* \\leq 1 \\), then \\( \\| x \\| - y^T x \\geq 0 \\) for all \\( x \\), with equality when \\( x = 0 \\). If \\( \\| y \\|_* &gt; 1 \\), the function tends to \\( -\\infty \\) as \\( t \\) approaches infinity.</p> <p>The lower bound property is:</p> \\[ p^* \\geq b^T \\nu \\quad \\text{if} \\quad \\| A^T \\nu \\|_* \\leq 1 \\]"},{"location":"prerequisites/numerical_optimization/#example-quadratic-program","title":"Example: Quadratic Program","text":"<p>Consider the following quadratic program:</p>"},{"location":"prerequisites/numerical_optimization/#primal-problem-assume-p-in-sn_","title":"Primal Problem (assume \\( P \\in S^{n}_{++} \\))","text":"\\[ \\min x^T P x \\] \\[ \\text{s.t.} \\] \\[ A x \\leq b \\] <p>The dual function is given by:</p> \\[ g(\\lambda) = \\inf_x \\left( x^T P x + \\lambda^T (A x - b) \\right) = -\\frac{1}{4} \\lambda^T A P^{-1} A^T \\lambda - b^T \\lambda \\] <p>The dual problem is:</p> \\[ \\max \\left( -\\frac{1}{4} \\lambda^T A P^{-1} A^T \\lambda - b^T \\lambda \\right) \\] \\[ \\text{s.t.} \\] \\[ \\lambda \\geq 0 \\]"},{"location":"prerequisites/numerical_optimization/#theorem_4","title":"Theorem","text":"<p>Suppose that \\( \\bar{x} \\) is a solution of the primal problem and that \\( f \\) and \\( -c_i \\), for \\( i = 1, 2, \\dots, m \\), are convex functions on \\( \\mathbb{R}^n \\) that are differentiable at \\( \\bar{x} \\). Then any \\( \\lambda \\) for which \\( (\\bar{x}, \\lambda) \\) satisfies the KKT conditions is a solution of the primal optimization problem.</p>"},{"location":"prerequisites/numerical_optimization/#references","title":"References","text":"<ol> <li> <p>Nocedal, J., &amp; Wright, S. J. (2006). Numerical Optimization (2nd ed.). Springer.</p> </li> <li> <p>Bertsekas, D. P. (2009). Convex Optimization Theory. Athena Scientific.</p> </li> <li> <p>Boyd, S., &amp; Vandenberghe, L. (2004). Convex Optimization. Cambridge University Press.</p> </li> <li> <p>Diehl, M. (2006). Lecture Notes on Numerical Optimization (Preliminary Draft). Retrieved from [link or source if applicable].</p> </li> </ol> <p> </p>"},{"location":"prerequisites/numerical_optimization/#authors","title":"Author(s)","text":"<ul> <li> <p>Abdollah Zohrabi</p> <p>Teaching Assistant</p> <p>abdollahzz1381@gmail.com</p> <p> </p> </li> </ul>"},{"location":"prerequisites/stochastic_processes/","title":"Stochastic Process","text":""},{"location":"prerequisites/stochastic_processes/#preliminary-definitions","title":"Preliminary Definitions","text":"<p>Definition: A sample space, \\( \\Omega \\), is a set of possible outcomes of a random experiment.</p> <p>Definition: A stochastic process is a family of random variables,</p> \\[ \\{ X(t) : t \\in T \\}, \\] <p>where \\( t \\) usually denotes time. That is, at every time \\( t \\) in the set \\( T \\), a random number \\( X(t) \\) is observed.</p> <p>Definition: \\( \\{ X(t) : t \\in T \\} \\) is a discrete-time process if the set \\( T \\) is finite or countable.</p> <p>In practice, this generally means \\( T = \\{0, 1, 2, 3, \\ldots\\} \\).</p> <p>Thus a discrete-time process is \\( \\{ X(0), X(1), X(2), X(3), \\ldots \\} \\): a random number associated with every time \\( 0, 1, 2, 3, \\ldots \\).</p> <p>Definition: \\( \\{ X(t) : t \\in T \\} \\) is a continuous-time process if \\( T \\) is not finite or countable.</p> <p>In practice, this generally means \\( T = [0, \\infty) \\), or \\( T = [0, K] \\) for some \\( K \\).</p> <p>Thus a continuous-time process \\( \\{ X(t) : t \\in T \\} \\) has a random number \\( X(t) \\) associated with every instant in time.</p> <p>(Note that \\( X(t) \\) need not change at every instant in time, but it is allowed to change at any time; i.e., not just at \\( t = 0, 1, 2, \\ldots \\), like a discrete-time process.)</p> <p>Definition: The state space, \\( S \\), is the set of real values that \\( X(t) \\) can take.</p> <p>Every \\( X(t) \\) takes a value in \\( \\mathbb{R} \\), but \\( S \\) will often be a smaller set: \\( S \\subseteq \\mathbb{R} \\). For example, if \\( X(t) \\) is the outcome of a coin tossed at the time \\( t \\), then the state space is \\( S = \\{0, 1\\} \\).</p> <p>Definition: The state space \\( S \\) is discrete if it is finite or countable. Otherwise it is continuous.</p>"},{"location":"prerequisites/stochastic_processes/#law-of-large-numbers","title":"Law of Large Numbers","text":""},{"location":"prerequisites/stochastic_processes/#the-weak-law-of-large-numbers-wlln","title":"The Weak Law of Large Numbers (WLLN)","text":"<p>Let \\( X_1, X_2, \\ldots \\) be a sequence of independent and identically distributed random variables having mean \\( \\mu \\). Then, for any \\( \\epsilon &gt; 0 \\),</p> \\[ P\\left( \\left| \\frac{X_1 + \\cdots + X_n}{n} - \\mu \\right| &gt; \\epsilon \\right) \\to 0 \\quad \\text{as} \\quad n \\to \\infty \\]"},{"location":"prerequisites/stochastic_processes/#the-central-limit-theorem-clt","title":"The Central Limit Theorem (CLT)","text":"<p>Let \\( X_1, X_2, \\ldots \\) be a sequence of independent and identically distributed random variables having finite mean \\( \\mu \\) and finite variance \\( \\sigma^2 \\). Then</p> \\[ \\lim_{n \\to \\infty} P\\left( \\frac{X_1 + \\cdots + X_n - n\\mu}{\\sigma \\sqrt{n}} &lt; x \\right) = \\Phi(x) \\] \\[ \\Phi(x) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{x} e^{-t^2/2} \\, dt, \\quad -\\infty &lt; x &lt; \\infty \\]"},{"location":"prerequisites/stochastic_processes/#borel-cantelli","title":"Borel-Cantelli","text":"<p>The Borel-Cantelli Lemma Let \\( E_1, E_2, \\dots \\) denote a sequence of events. If [ \\sum_{i=1}^{\\infty} P(E_i) &lt; \\infty, ] then [ P(\\text{an infinite number of the } E_i \\text{ occur}) = 0. ]</p> <p>Converse to the Borel-Cantelli Lemma If \\( E_1, E_2, \\dots \\) are independent events such that [ \\sum_{n=1}^{\\infty} P(E_n) = \\infty, ] then [ P(\\text{an infinite number of the } E_n \\text{ occur}) = 1. ]</p>"},{"location":"prerequisites/stochastic_processes/#conditional-expectation","title":"Conditional Expectation","text":""},{"location":"prerequisites/stochastic_processes/#definition","title":"Definition","text":"<p>For jointly discrete random variables \\( X \\) and \\( Y \\), the conditional expectation \\( E[X | Y = y] \\) is:</p> \\[ E[X | Y = y] = \\sum_x x P(X = x | Y = y) = \\sum_x x \\frac{P(X = x, Y = y)}{P(Y = y)}. \\] <p>For jointly continuous \\( X \\) and \\( Y \\) with joint density \\( f(x, y) \\):</p> \\[ E[X | Y = y] = \\frac{\\int_{-\\infty}^{\\infty} x f(x, y) \\, dx}{\\int_{-\\infty}^{\\infty} f(x, y) \\, dx}. \\] <p>Note that \\( E[X | Y] \\) is a random variable, representing \\( E[X | Y = y] \\) as a function of \\( Y \\).</p>"},{"location":"prerequisites/stochastic_processes/#proposition-law-of-iterated-expectation","title":"Proposition: Law of Iterated Expectation","text":"\\[ E[E[X | Y]] = E[X]. \\] <p>For discrete \\( Y \\):</p> \\[ E[X] = \\sum_y E[X | Y = y] P(Y = y). \\]"},{"location":"prerequisites/stochastic_processes/#markov-chains","title":"Markov Chains","text":"<p>Markov Chain Definition</p> <p>A sequence of random variables \\( (X_0, X_1, X_2, \\ldots) \\) is a Markov chain with state space \\( \\Omega \\) and transition matrix \\( P \\) if, for all \\( n \\geq 0 \\), and all sequences \\( (X_0, X_1, \\ldots, X_n, X_{n+1}) \\), we have:</p> \\[ \\mathbb{P}[X_{n+1} = x_{n+1} \\mid X_0 = x_0, \\ldots, X_n = x_n] = \\mathbb{P}[X_{n+1} = x_{n+1} \\mid X_n = x_n] = P(x_n, x_{n+1}). \\]"},{"location":"prerequisites/stochastic_processes/#example-1-gamblers-ruin","title":"Example 1: Gambler's Ruin","text":"<p>Consider a gambling game where on any turn you win $1 with probability \\( p = 0.4 \\) or lose $1 with probability \\( 1 - p = 0.6 \\). You adopt the rule to quit playing if your fortune reaches $N. If your fortune reaches $0, the casino stops you.</p> <p>Let \\( X_n \\) be your fortune after \\( n \\) plays. This process has the Markov property: given the current state, past states are irrelevant for predicting the next state \\( X_{n+1} \\). If you are still playing at time \\( n \\) (i.e., your fortune \\( X_n = i \\) with \\( 0 &lt; i &lt; N \\)), then for any history of your wealth \\( i_{n-1}, i_{n-2}, \\ldots, i_0 \\),</p> \\[ P(X_{n+1} = i + 1 \\mid X_n = i, X_{n-1} = i_{n-1}, \\ldots, X_0 = i_0) = 0.4 \\]"},{"location":"prerequisites/stochastic_processes/#example-2-ehrenfest-chain-and-transition-matrix","title":"Example 2: Ehrenfest Chain and Transition Matrix","text":"<p>In the Ehrenfest chain, two urns have \\( N \\) balls. At each step, a ball is picked at random and moved to the other urn. Let \\( X_n \\) be the number of balls in the \"left\" urn after \\( n \\) draws. This has the Markov property:</p> \\[ P(X_{n+1} = i + 1 \\mid X_n = i) = \\frac{N - i}{N}, \\quad P(X_{n+1} = i - 1 \\mid X_n = i) = \\frac{i}{N}, \\] <p>with \\( P(i, j) = 0 \\) otherwise. For \\( N = 4 \\), the transition matrix is:</p> \\[ \\begin{array}{c|ccccc}  &amp; 0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 \\\\ \\hline 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 1/4 &amp; 0 &amp; 3/4 &amp; 0 &amp; 0 \\\\ 2 &amp; 0 &amp; 2/4 &amp; 0 &amp; 2/4 &amp; 0 \\\\ 3 &amp; 0 &amp; 0 &amp; 3/4 &amp; 0 &amp; 1/4 \\\\ 4 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ \\end{array} \\] <p>A transition matrix \\( P(i, j) \\) defines a Markov chain if: (i) \\( P(i, j) \\geq 0 \\) (probabilities). (ii) \\( \\sum_j P(i, j) = 1 \\) (next state is certain).</p>"},{"location":"prerequisites/stochastic_processes/#example-3-wright-fisher-model","title":"Example 3: Wright-Fisher Model","text":"<p>Consider a population of \\( N/2 \\) diploid individuals (or \\( N \\) haploid individuals) with genes of type \\( A \\) or \\( a \\). The population at time \\( n+1 \\) is obtained by drawing with replacement from the population at time \\( n \\). Let \\( X_n \\) be the number of \\( A \\) alleles at time \\( n \\). Then \\( X_n \\) is a Markov chain with transition probability:</p> \\[ p(i, j) = \\binom{N}{j} \\left( \\frac{i}{N} \\right)^j \\left( 1 - \\frac{i}{N} \\right)^{N-j}, \\] <p>where the right-hand side is the binomial distribution for \\( N \\) independent trials with success probability \\( i/N \\).</p>"},{"location":"prerequisites/stochastic_processes/#multistep-transition-probabilities","title":"Multistep Transition Probabilities","text":"<p>The transition probability \\( p(i, j) = P(X_{n+1} = j \\mid X_n = i) \\) gives the probability of going from state \\( i \\) to state \\( j \\) in one step. Our goal is to compute the probability of going from \\( i \\) to \\( j \\) in \\( m &gt; 1 \\) steps:</p> \\[ p^m(i, j) = P(X_{n+m} = j \\mid X_n = i). \\] <p>This \\( p^m(i, j) \\) is the \\( m \\)-th power of the transition matrix \\( p \\).</p>"},{"location":"prerequisites/stochastic_processes/#theorem-chapman-kolmogorov-equation","title":"Theorem: (Chapman-Kolmogorov Equation)","text":"<p>The \\( m \\)-step transition probability satisfies:</p> \\[ p^{m+n}(i, j) = \\sum_k p^m(i, k) p^n(k, j). \\] <p>For \\( n = 1 \\), this becomes \\( p^{m+1}(i, j) = \\sum_k p^m(i, k) p(k, j) \\), meaning the \\( m+1 \\)-step probability is the \\( m \\)-step probability times \\( p \\).</p> <p></p>"},{"location":"prerequisites/stochastic_processes/#proof","title":"Proof","text":"<p>Consider the probability of transitioning from state \\( i \\) to state \\( j \\) in \\( m+n \\) steps, and split it at time \\( m \\):</p> \\[ P(X_{m+n} = j \\mid X_0 = i) = \\sum_k P(X_{m+n} = j, X_m = k \\mid X_0 = i). \\] <p>Apply conditional probability to the joint event:</p> \\[ P(X_{m+n} = j, X_m = k \\mid X_0 = i) = P(X_m = k \\mid X_0 = i) \\cdot P(X_{m+n} = j \\mid X_m = k, X_0 = i). \\] <p>Using the Markov property, the future depends only on the current state at time \\( m \\):</p> \\[ P(X_{m+n} = j \\mid X_m = k, X_0 = i) = P(X_{m+n} = j \\mid X_m = k) = p^n(k, j). \\] <p>Also, \\( P(X_m = k \\mid X_0 = i) = p^m(i, k) \\). Thus:</p> \\[ P(X_{m+n} = j \\mid X_0 = i) = \\sum_k p^m(i, k) p^n(k, j), \\] <p>which is \\( p^{m+n}(i, j) \\), completing the proof.</p>"},{"location":"prerequisites/stochastic_processes/#example-gamblers-ruin","title":"Example: Gambler's Ruin","text":"<p>The transition probability for the gambler's ruin with \\( N = 4 \\) (from Example 1) is:</p> \\[ \\begin{array}{c|ccccc}  &amp; 0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 \\\\ \\hline 0 &amp; 1.0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0.6 &amp; 0 &amp; 0.4 &amp; 0 &amp; 0 \\\\ 2 &amp; 0 &amp; 0.6 &amp; 0 &amp; 0.4 &amp; 0 \\\\ 3 &amp; 0 &amp; 0 &amp; 0.6 &amp; 0 &amp; 0.4 \\\\ 4 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1.0 \\\\ \\end{array} \\] <p>Compute \\( p^2 \\): - \\( p^2(0, 0) = 1 \\), \\( p^2(4, 4) = 1 \\) (absorbing states). - \\( p^2(1, 3) = (0.4)^2 = 0.16 \\), going up twice. - \\( p^2(1, 1) = (0.4)(0.6) = 0.24 \\), from 1 to 2 to 1. - \\( p^2(1, 0) = 0.6 \\), first jump to 0.</p> <p>Thus:</p> \\[ p^2 = \\begin{pmatrix} 1.0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0.6 &amp; 0.24 &amp; 0 &amp; 0.16 &amp; 0 \\\\ 0 &amp; 0.36 &amp; 0.48 &amp; 0 &amp; 0.16 \\\\ 0 &amp; 0 &amp; 0.36 &amp; 0.24 &amp; 0.4 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{pmatrix} \\] <p>The limiting matrix is:</p> \\[ \\lim_{n \\to \\infty} p^n = \\begin{pmatrix} 1.0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 57/65 &amp; 0 &amp; 0 &amp; 0 &amp; 8/65 \\\\ 45/65 &amp; 0 &amp; 0 &amp; 0 &amp; 20/65 \\\\ 27/65 &amp; 0 &amp; 0 &amp; 0 &amp; 38/65 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{pmatrix} \\]"},{"location":"prerequisites/stochastic_processes/#classification-of-states","title":"Classification of States","text":"<p>Definition: Probability Under Initial Condition</p> <p>For a stochastic process with state \\( X_0 \\), the notation \\( P_x(A) \\) represents the probability of event \\( A \\) given that the process starts in state \\( x \\):</p> \\[ P_x(A) = P(A \\mid X_0 = x). \\] <p>Definition: Stopping Time A random variable \\( T \\) is a stopping time if the event \\( \\{ T = n \\} \\) (the occurrence of \"we stop at time \\( n \\)\") can be determined by the values of the process up to time \\( n \\): \\( X_0, \\ldots, X_n \\). For \\( T_y = \\min\\{ n: X_n = y \\} \\),</p> \\[ \\{ T_y = n \\} = \\{ X_1 \\neq y, \\ldots, X_{n-1} \\neq y, X_n = y \\}. \\]"},{"location":"prerequisites/stochastic_processes/#theorem-strong-markov-property","title":"Theorem (Strong Markov Property):","text":"<p>Suppose \\( T \\) is a stopping time. Given \\( T = n \\) and \\( X_T = y \\), the future states \\( X_{T+k} \\) (for \\( k \\geq 0 \\)) behave like a Markov chain with initial state \\( y \\), independent of \\( X_0, \\ldots, X_T \\).</p> <p>Definition (Communication Between States):</p> <p>We say that state \\( x \\) communicates with state \\( y \\), and write \\( x \\to y \\), if there is a positive probability of reaching \\( y \\) starting from \\( x \\):</p> \\[ \\rho_{xy} = P_x(T_y &lt; \\infty) &gt; 0, \\] <p>where \\( T_y \\) is the first time the chain reaches state \\( y \\).</p>"},{"location":"prerequisites/stochastic_processes/#lemma-on-transitivity-of-communication","title":"Lemma on Transitivity of Communication","text":"<p>If \\( x \\to y \\) and \\( y \\to z \\), then \\( x \\to z \\).</p> <p>Proof:</p> <p>Since \\( x \\to y \\), there exists an \\( m \\) such that \\( p^m(x, y) &gt; 0 \\). Similarly, there exists an \\( n \\) such that \\( p^n(y, z) &gt; 0 \\). By the Chapman-Kolmogorov equation, \\( p^{m+n}(x, z) \\geq p^m(x, y) p^n(y, z) &gt; 0 \\), so \\( x \\to z \\).</p>"},{"location":"prerequisites/stochastic_processes/#theorem-on-transience","title":"Theorem on Transience","text":"<p>If \\( \\rho_{xy} &gt; 0 \\) but \\( \\rho_{yx} &lt; 1 \\), then \\( x \\) is transient.</p> <p>Proof Let \\( K = \\min\\{ k : p^k(x, y) &gt; 0 \\} \\), the smallest number of steps to get from \\( x \\) to \\( y \\). Since \\( p^K(x, y) &gt; 0 \\), there exists a sequence \\( y_1, \\ldots, y_{K-1} \\) such that:</p> \\[ p(x, y_1) p(y_1, y_2) \\cdots p(y_{K-1}, y) &gt; 0. \\] <p>Since \\( K \\) is minimal, all \\( y_i \\neq y \\), and we have:</p> \\[ P_x(T_x = \\infty) \\geq p(x, y_1) p(y_1, y_2) \\cdots p(y_{K-1}, y) (1 - \\rho_{yx}) &gt; 0, \\] <p>so \\( x \\) is transient.</p>"},{"location":"prerequisites/stochastic_processes/#lemma-on-recurrence-and-communication","title":"Lemma on Recurrence and Communication","text":"<p>If \\( x \\) is recurrent and \\( \\rho_{xy} &gt; 0 \\), then \\( \\rho_{yx} = 1 \\).</p> <p>Proof:</p> <p>If \\( \\rho_{yx} &lt; 1 \\), then by the Theorem on Transience, \\( x \\) would be transient, a contradiction.</p>"},{"location":"prerequisites/stochastic_processes/#remark","title":"Remark","text":"<p>The Theorem on Transience allows us to identify all transient states when the state space is finite. </p> <p>Definition of a Closed Set: A set \\( A \\) is closed if it is impossible to get out, i.e., if \\( i \\in A \\) and \\( j \\notin A \\) then \\( p(i, j) = 0 \\).</p> <p>In this Example with Specific States \\( \\{1, 5\\} \\) and \\( \\{4, 6, 7\\} \\) are closed sets. Their union, \\( \\{1, 4, 5, 6, 7\\} \\) is also closed. One can add 3 to get another closed set \\( \\{1, 3, 4, 5, 6, 7\\} \\). Finally, the whole state space \\( \\{1, 2, 3, 4, 5, 6, 7\\} \\) is always a closed set.</p> <p></p> <p>Definition of an Irreducible Set: A set \\( B \\) is called irreducible if whenever \\( i, j \\in B \\), \\( i \\) communicates with \\( j \\).</p> <p>The irreducible closed sets in the Example on Gambler's Ruin with Specific States are \\( \\{1, 5\\} \\) and \\( \\{4, 6, 7\\} \\). The next result explains our interest in irreducible closed sets.</p>"},{"location":"prerequisites/stochastic_processes/#theorem-recurrence-in-finite-irreducible-sets","title":"Theorem (Recurrence in Finite Irreducible Sets)","text":"<p>If \\( C \\) is a finite closed and irreducible set, then all states in \\( C \\) are recurrent.</p>"},{"location":"prerequisites/stochastic_processes/#theorem-decomposition-of-finite-state-space","title":"Theorem (Decomposition of Finite State Space)","text":"<p>If the state space \\( S \\) is finite, then \\( S \\) can be written as a disjoint union \\( T \\cup R_1 \\cup \\cdots \\cup R_k \\), where \\( T \\) is a set of transient states and the \\( R_i \\), \\( 1 \\leq i \\leq k \\), are closed irreducible sets of recurrent states.</p>"},{"location":"prerequisites/stochastic_processes/#recurrence-properties","title":"Recurrence Properties","text":"<ul> <li> <p>If \\( x \\) is recurrent and \\( x \\to y \\), then \\( y \\) is recurrent.</p> </li> <li> <p>In a finite closed set there has to be at least one recurrent state.</p> </li> <li> <p>\\( y \\) is recurrent if and only if</p> </li> </ul> \\[ \\sum_{n=1}^{\\infty} p^n(y, y) = E_y N(y) = \\infty \\] <p>where \\( E_y N(y) \\) is the expected number of visits to \\( y \\) starting from \\( y \\).</p>"},{"location":"prerequisites/stochastic_processes/#stationary-distributions","title":"Stationary Distributions","text":"<p>Suppose \\( \\vec{\\pi} \\) is a limiting probability vector, i.e., for some initial probability vector \\( \\vec{v} \\),</p> \\[ \\vec{\\pi} = \\lim_{n \\to \\infty} \\vec{v} \\mathbf{P}^n. \\] <p>Then</p> \\[ \\vec{\\pi} = \\lim_{n \\to \\infty} \\vec{v} \\mathbf{P}^{n+1} = \\left( \\lim_{n \\to \\infty} \\vec{v} \\mathbf{P}^n \\right) \\mathbf{P} = \\vec{\\pi} \\mathbf{P}. \\] <p>We call a probability vector \\( \\vec{\\pi} \\) an invariant probability distribution for \\( \\mathbf{P} \\) if</p> \\[ \\vec{\\pi} = \\vec{\\pi} \\mathbf{P}. \\] <p>Such a \\( \\vec{\\pi} \\) is also called a stationary, equilibrium, or steady-state probability distribution. Note that an invariant probability vector is a left eigenvector of \\( \\mathbf{P} \\) with eigenvalue 1.</p>"},{"location":"prerequisites/stochastic_processes/#lemma","title":"Lemma","text":"<p>If a stationary distribution exists, then [ \\lim_{n \\to \\infty} \\mathbf{P}^n = \\begin{bmatrix} \\vec{\\pi} \\ \\vec{\\pi} \\ \\vdots \\ \\vec{\\pi} \\end{bmatrix}, ]</p>"},{"location":"prerequisites/stochastic_processes/#example-general-two-state-transition-probability","title":"Example: (General two state transition probability)","text":"<p>Let us start by considering the two-state Markov chain with</p> \\[ \\mathbf{P} = \\begin{bmatrix} 1-p &amp; p \\\\ q &amp; 1-q \\end{bmatrix}, \\] <p>where \\( 0 &lt; p, q &lt; 1 \\). This matrix has eigenvalues 1 and \\( 1-p-q \\). We can diagonalize \\( \\mathbf{P} \\),</p> \\[ \\mathbf{D} = \\mathbf{Q}^{-1} \\mathbf{P} \\mathbf{Q}, \\] <p>where</p> \\[ \\mathbf{Q} = \\begin{bmatrix} 1 &amp; -p \\\\ 1 &amp; q \\end{bmatrix}, \\quad \\mathbf{Q}^{-1} = \\begin{bmatrix} q/(p+q) &amp; p/(p+q) \\\\ -1/(p+q) &amp; 1/(p+q) \\end{bmatrix}, \\] \\[ \\mathbf{D} = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1-p-q \\end{bmatrix}. \\] <p>The columns of \\( \\mathbf{Q} \\) are right eigenvectors of \\( \\mathbf{P} \\), and the rows of \\( \\mathbf{Q}^{-1} \\) are left eigenvectors. The eigenvectors are unique up to a multiplicative constant.</p> <p>We choose the left eigenvector for eigenvalue 1 to be a probability vector. \\( \\vec{\\pi} = (q/(p+q), p/(p+q)) \\) is the unique invariant probability distribution for \\( \\mathbf{P} \\). Once \\( \\mathbf{P} \\) is diagonalized, we can compute powers:</p> \\[ \\mathbf{P}^n = (\\mathbf{Q} \\mathbf{D} \\mathbf{Q}^{-1})^n = \\mathbf{Q} \\mathbf{D}^n \\mathbf{Q}^{-1} = \\mathbf{Q} \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; (1-p-q)^n \\end{bmatrix} \\mathbf{Q}^{-1}. \\] <p>This simplifies to:</p> \\[ \\mathbf{P}^n = \\begin{bmatrix} q + p(1-p-q)^n &amp; p - p(1-p-q)^n \\\\ q - q(1-p-q)^n &amp; p + q(1-p-q)^n \\end{bmatrix} / (p+q). \\] <p>Since \\( |1-p-q| &lt; 1 \\), we have:</p> \\[ \\lim_{n \\to \\infty} \\mathbf{P}^n = \\begin{bmatrix} q/(p+q) &amp; p/(p+q) \\\\ q/(p+q) &amp; p/(p+q) \\end{bmatrix} = \\begin{bmatrix} \\vec{\\pi} \\\\ \\vec{\\pi} \\end{bmatrix}. \\] <p>The key is that the second eigenvalue \\( 1-p-q \\) has absolute value less than 1, so the dominant contribution to \\( \\mathbf{P}^n \\) comes from the eigenvalue 1, i.e., the invariant probability distribution.</p>"},{"location":"prerequisites/stochastic_processes/#irreducible","title":"Irreducible","text":"<p>A transition matrix \\( \\mathbf{P} \\) is called irreducible, if for any \\( x, y \\in \\Omega \\), there exists a number \\( n \\) (possibly depending on \\( x, y \\)) such that</p> \\[ P^n(x, y) &gt; 0. \\] <p>Definition</p> <p>For any \\( x \\in \\Omega \\), define \\( T(x) = \\{ n \\geq 1 : P^n(x, x) &gt; 0 \\} \\). The period of state \\( x \\) is the greatest common divisor of \\( T(x) \\), denoted by \\( \\gcd(T(x)) \\).</p> <p>Lemma</p> <p>If \\( \\mathbf{P} \\) is irreducible, then \\( \\gcd(T(x)) = \\gcd(T(y)) \\) for all \\( x, y \\in \\Omega \\).</p>"},{"location":"prerequisites/stochastic_processes/#aperiodic","title":"Aperiodic","text":"<p>Definition</p> <p>For an irreducible chain, the period of the chain is defined to be the period which is common to all states. The chain is aperiodic if all states have period 1.</p> <p>Example</p> <p>Consider a simple random walk on an \\( N \\)-cycle where \\( N \\) is odd. Then the walk is irreducible and aperiodic.</p>"},{"location":"prerequisites/stochastic_processes/#theorem","title":"Theorem","text":"<p>If \\( \\mathbf{P} \\) is irreducible and aperiodic, then there exists an integer \\( r \\) such that</p> \\[ P^n(x, y) &gt; 0, \\quad \\forall x, y \\in \\Omega, \\forall n \\geq r. \\] <p>Definition</p> <p>For \\( x \\in \\Omega \\), define</p> \\[ \\tau_x = \\inf \\{ n \\geq 0 : X_n = x \\}, \\quad \\tau_x^+ = \\inf \\{ n \\geq 1 : X_n = x \\}. \\] <p>1- \\( \\tau_x \\): the hitting time for \\( x \\).</p> <p>2- \\( \\tau_x^+ \\): the first return time when \\( X_0 = x \\).</p>"},{"location":"prerequisites/stochastic_processes/#lemma_1","title":"Lemma","text":"<p>Suppose that \\( P \\) is irreducible. Then, for any \\( x, y \\in \\Omega \\), we have</p> \\[ \\mathbb{E}_x[\\tau_y^+] &lt; \\infty \\]"},{"location":"prerequisites/stochastic_processes/#perron-frobenius-theorem-for-finite-state-markov-chains","title":"Perron-Frobenius Theorem (for Finite-State Markov Chains)","text":"<p>Suppose that \\( P \\) is irreducible, then there exists a probability measure \\( \\pi \\) such that \\( \\pi = \\pi P \\) and \\( \\pi(x) &gt; 0 \\) for all \\( x \\in \\Omega \\).</p>"},{"location":"prerequisites/stochastic_processes/#theorem_1","title":"Theorem","text":"<p>Suppose that \\( P \\) is irreducible. Then there exists a unique stationary distribution. Moreover,</p> \\[ \\pi(x) = \\frac{1}{\\mathbb{E}_x[\\tau_x^+]}, \\quad \\forall x \\in \\Omega. \\]"},{"location":"prerequisites/stochastic_processes/#theorem-convergence-theorem","title":"Theorem (Convergence Theorem)","text":"<p>Suppose that \\( p \\) is irreducible, aperiodic, and has a stationary distribution \\( \\pi \\). Then as \\( n \\to \\infty \\):  [ p^n(x, y) \\to \\pi(y). ]</p> <p>Definition:</p> <p>Given a graph \\( G = (V, E) \\), we define simple random walk on \\( G \\) to be the Markov chain with state space \\( V \\) and transition matrix:</p> \\[ P(x, y) = \\begin{cases}  \\frac{1}{\\deg(x)} &amp; \\text{if } y \\sim x \\\\ 0 &amp; \\text{else} \\end{cases}. \\]"},{"location":"prerequisites/stochastic_processes/#theorem_2","title":"Theorem:","text":"<p>Let</p> \\[ \\pi(x) = \\frac{\\deg(x)}{2|E|}, \\quad \\forall x \\in V. \\] <p>Then \\( \\pi \\) is a stationary distribution for the simple random walk on the graph.</p>"},{"location":"prerequisites/stochastic_processes/#countable-markov-chain","title":"Countable Markov Chain","text":"<p>Definition: Let \\( \\Omega \\) be a countable state space and \\( P \\) the transition matrix. A stochastic process \\( (X_n)_{n \\geq 0} \\) is a Markov chain if for all states \\( x, y \\in \\Omega \\),</p> \\[ \\mathbb{P}[X_{n+1} = y \\mid X_1 = x_1, \\dots, X_n = x] = \\mathbb{P}[X_{n+1} = y \\mid X_n = x] = P(x, y), \\] <p>i.e., the future state depends only on the present state.</p> <p>Chapman-Kolmogorov Equation (Countable Markov Chain): Let \\( S \\) be a countable state space. The n-step transition probabilities are defined by:</p> \\[ p_n(x, y) = \\mathbb{P}\\{X_n = y \\mid X_0 = x\\}. \\] <p>For \\( 0 &lt; m, n &lt; \\infty \\), the Chapman-Kolmogorov equation states:</p> \\[ p_{m+n}(x, y) = \\sum_{z \\in S} p_m(x, z) \\, p_n(z, y). \\] <p>This expresses that the probability of transitioning from state \\( x \\) to \\( y \\) in \\( m+n \\) steps equals the sum over all intermediate states \\( z \\in S \\), of the probability of transitioning from \\( x \\) to \\( z \\) in \\( m \\) steps and then from \\( z \\) to \\( y \\) in \\( n \\) steps.</p> <p>Example 1 (Random Walk with Partially Reflecting Boundary at 0): Let \\( 0 &lt; p &lt; 1 \\), and let \\( S = \\{0, 1, 2, \\dots\\} \\). The transition probabilities are:</p> \\[ p(x, y) = \\begin{cases} 1 - p &amp; \\text{if } y = x - 1,\\ x &gt; 0 \\text{ or } x = y = 0, \\\\ p     &amp; \\text{if } y = x + 1, \\\\ 0     &amp; \\text{otherwise}. \\end{cases} \\] <p>This describes a random walk on \\( S \\) with a partially reflecting boundary at 0.</p> <p></p> <p>Example 2 (Simple Random Walk on the Integer Lattice): Let \\( \\mathbb{Z}^d \\) be the d-dimensional integer lattice:</p> \\[ \\mathbb{Z}^d = \\{ (z_1, \\dots, z_d) : z_i \\in \\mathbb{Z} \\}. \\] <p>Each \\( x \\in \\mathbb{Z}^d \\) has \\( 2d \\) nearest neighbors at distance 1. A simple random walk on \\( \\mathbb{Z}^d \\) is a Markov chain \\( (X_n) \\) with state space \\( S = \\mathbb{Z}^d \\), where the process moves uniformly to one of the \\( 2d \\) nearest neighbors. The transition probabilities are:</p> \\[ p(x, y) = \\begin{cases} \\frac{1}{2d} &amp; \\text{if } \\lVert x - y \\rVert = 1, \\\\ 0           &amp; \\text{otherwise}. \\end{cases} \\] <p></p>"},{"location":"prerequisites/stochastic_processes/#recurrence-and-transience","title":"Recurrence and Transience","text":"<p>Let \\( X_n \\) be an irreducible Markov chain with countably infinite state space \\( S \\) and transition probabilities \\( p(x, y) \\).</p> <p>Definition (Recurrence): The chain \\( X_n \\) is said to be recurrent if, for each state \\( x \\in S \\),</p> \\[ \\mathbb{P}^x\\{X_n = x \\text{ for infinitely many } n\\} = 1. \\] <p>That is, the chain returns to the state \\( x \\) infinitely often with probability 1.</p> <p>Definition (Transience): If a state \\( x \\in S \\) is not recurrent, it is called transient. In this case, the chain visits \\( x \\) only a finite number of times almost surely.</p> <p>Now, fix a state \\( x \\) and assume \\( X_0 = x \\). Define the random variable \\( R \\) to be the total number of visits to state \\( x \\), including the initial visit:</p> \\[ R = \\sum_{n = 0}^{\\infty} I\\{X_n = x\\}, \\] <p>where \\( I\\{\\cdot\\} \\) denotes the indicator function. Then the expected number of visits to state \\( x \\), assuming \\( X_0 = x \\), is given by:</p> \\[ \\mathbb{E}(R) = \\sum_{n = 0}^{\\infty} \\mathbb{P}(X_n = x) = \\sum_{n = 0}^{\\infty} p_n(x, x). \\] <p>Let \\( T = \\min\\{n &gt; 0 : X_n = x\\} \\) denote the time of first return to state \\( x \\). Then:</p> <p>1- If \\( \\mathbb{P}^x\\{T &lt; \\infty\\} = 1 \\), the chain always returns to \\( x \\) and is recurrent.</p> <p>2- If \\( \\mathbb{P}^x\\{T &lt; \\infty\\} = q &lt; 1 \\), the chain returns with probability less than 1 and is transient.</p> <p>In the transient case, the number of returns \\( R \\) is a geometric random variable with success probability \\( 1 - q \\), and its expectation is:</p> \\[ \\mathbb{E}(R) = \\sum_{m = 1}^{\\infty} m \\cdot q^{m - 1}(1 - q) = \\frac{1}{1 - q} &lt; \\infty. \\] <p>Fact: An irreducible Markov chain is transient if and only if the expected number of returns to a state is finite. That is,</p> \\[ \\sum_{n = 0}^{\\infty} p_n(x, x) &lt; \\infty. \\]"},{"location":"prerequisites/stochastic_processes/#example-recurrence-of-the-simple-random-walk-on-mathbbzd","title":"Example: Recurrence of the Simple Random Walk on \\( \\mathbb{Z}^d \\)","text":""},{"location":"prerequisites/stochastic_processes/#case-d-1","title":"Case \\( d = 1 \\)","text":"<p>We begin with the one-dimensional case. The state space is \\( \\mathbb{Z} \\), and the transition probabilities are given by:</p> \\[ p(x, x + 1) = p(x, x - 1) = \\frac{1}{2}. \\] <p>Assuming the walk starts at the origin (\\( X_0 = 0 \\)), and noting that the chain has period 2, we have:</p> \\[ p_n(0, 0) = 0 \\quad \\text{for all odd } n. \\] <p>To compute \\( p_{2n}(0, 0) \\), observe that the walker must take exactly \\( n \\) steps to the right and \\( n \\) steps to the left. The number of such paths is \\( \\binom{2n}{n} \\), each with probability \\( (1/2)^{2n} \\). Therefore:</p> \\[ p_{2n}(0, 0) = \\binom{2n}{n} \\left( \\frac{1}{2} \\right)^{2n} = \\frac{(2n)!}{n! \\cdot n!} \\left( \\frac{1}{2} \\right)^{2n}. \\] <p>Applying Stirling\u2019s approximation,</p> \\[ n! \\sim \\sqrt{2\\pi n} \\cdot n^n e^{-n}, \\] <p>we obtain the asymptotic estimate:</p> \\[ p_{2n}(0, 0) \\sim \\frac{1}{\\sqrt{\\pi n}}. \\] <p>Hence, the total return probability is:</p> \\[ \\sum_{n=0}^{\\infty} p_{2n}(0, 0) \\sim \\sum_{n=1}^{\\infty} \\frac{1}{\\sqrt{n}} = \\infty. \\] <p>Conclusion: The simple random walk on \\( \\mathbb{Z} \\) is recurrent.</p>"},{"location":"prerequisites/stochastic_processes/#case-d-1_1","title":"Case \\( d &gt; 1 \\)","text":"<p>Now consider the simple random walk on \\( \\mathbb{Z}^d \\) for \\( d &gt; 1 \\), with transition probabilities:</p> \\[ p(x, y) = \\frac{1}{2d}, \\quad \\text{if } |x - y| = 1. \\] <p>Assume the walk starts at the origin. After \\( 2n \\) steps, by the law of large numbers, approximately \\( 2n/d \\) of the steps are expected in each coordinate direction.</p> <p>From the one-dimensional result, the probability that a single coordinate returns to zero is approximately:</p> \\[ \\left( \\pi(n/d) \\right)^{-1/2}. \\] <p>Assuming independence across dimensions, the total return probability becomes:</p> \\[ p_{2n}(0, 0) \\sim \\left( \\frac{1}{2} \\right)^{d - 1} \\left( \\frac{d}{\\pi n} \\right)^{d/2}. \\] <p>To determine recurrence, we analyze:</p> \\[ \\sum_{n=0}^{\\infty} p_{2n}(0, 0) \\sim \\sum_{n=1}^{\\infty} n^{-d/2}. \\] <p>This series:</p> \\[ \\begin{cases} \\text{diverges} &amp; \\text{if } d = 1, 2, \\\\ \\text{converges} &amp; \\text{if } d \\geq 3. \\end{cases} \\] <p>We conclude:</p> \\[ \\sum_{n=0}^{\\infty} p_{2n}(0, 0) \\begin{cases} = \\infty, &amp; \\text{if } d = 1, 2 \\quad \\Rightarrow \\text{Recurrent}, \\\\ &lt; \\infty, &amp; \\text{if } d \\geq 3 \\quad \\Rightarrow \\text{Transient}. \\end{cases} \\] <p>Therefore, the simple random walk on \\( \\mathbb{Z}^d \\) is:</p> <ul> <li>Recurrent for \\( d = 1, 2 \\) </li> <li>Transient for \\( d \\geq 3 \\)</li> </ul>"},{"location":"prerequisites/stochastic_processes/#continuous-time-markov-chains","title":"Continuous Time Markov Chains","text":"<p>There is another type of Markov process, and that is the continuous Markov process.</p> <p>Definition: Continuous-Time Markov Chain</p> <p>Let \\( (X_t)_{t \\geq 0} \\) be a continuous-time Markov chain. We say that \\( (X_t)_{t \\geq 0} \\) is a continuous-time Markov chain if for all times \\( 0 \\leq t_1 \\leq t_2 \\leq \\cdots \\leq t_{n+1} \\) and all \\( x_1, x_2, \\dots, x_{n+1} \\in \\Omega \\), we have:</p> \\[ \\mathbb{P}[X_{t_{n+1}} = x_{n+1} \\mid X_{t_1} = x_1, \\dots, X_{t_n} = x_n] = \\mathbb{P}[X_{t_{n+1}} = x_{n+1} \\mid X_{t_n} = x_n]. \\] <p>Moreover, the right-hand side depends only on \\( (t_{n+1} - t_n) \\).</p> <p>Definition: Semigroup of the Chain</p> <p>Suppose that \\( (X_t)_{t \\geq 0} \\) is a continuous-time Markov chain. Define the transition probabilities as:</p> \\[ P_t(x, y) = \\mathbb{P}[X_t = y \\mid X_0 = x]. \\] <p>The semigroup \\( (P_t)_{t \\geq 0} \\) of the chain is defined as follows:</p> <ul> <li>\\( P_0 = I \\) (identity matrix),</li> <li>\\( P_t \\) is a stochastic matrix,</li> <li>\\( P_{t+s} = P_t P_s \\).</li> </ul> <p>Example 1: Poisson Process is Markovian</p> <p>The Poisson process is a continuous-time Markov chain with transition probabilities:</p> \\[ P_s(x, y) = e^{-\\lambda s} \\frac{(\\lambda s)^{y - x}}{(y - x)!}. \\] <p>Example 2: Discrete-Time Markov Chain as Continuous-Time Process</p> <p>Let \\( (\\hat{X}_n)_{n \\geq 0} \\) be a discrete-time Markov chain with transition matrix \\( Q \\), and let \\( (N_t)_{t \\geq 0} \\) be an independent Poisson process with intensity \\( \\lambda &gt; 0 \\). Define \\( X_t = \\hat{X}_{N_t}, t \\geq 0 \\). Then \\( (X_t)_{t \\geq 0} \\) is a continuous-time Markov chain with transition probabilities:</p> \\[ P_s(x, y) = e^{-\\lambda s} \\sum_{k=0}^{\\infty} \\frac{(\\lambda s)^k}{k!} Q^k(x, y). \\]"},{"location":"prerequisites/stochastic_processes/#theorem-holding-time-exponentially-distributed","title":"Theorem (Holding Time Exponentially Distributed):","text":"<p>Let \\( X_0 = x \\), and define the holding time at \\( x \\) as:</p> \\[ S_x = \\inf\\{ t \\geq 0 : X_t \\neq x \\}. \\] <p>Then \\( S_x \\) has an exponential distribution.</p>"},{"location":"prerequisites/stochastic_processes/#theorem-memoryless-property-of-exponential-distribution","title":"Theorem (Memoryless Property of Exponential Distribution)","text":"<p>Let \\( T \\) be a positive random variable. \\( T \\) has the memoryless property if:</p> \\[ \\mathbb{P}[T &gt; t + s \\mid T &gt; s] = \\mathbb{P}[T &gt; t], \\] <p>if and only if \\( T \\) has an exponential distribution.</p>"},{"location":"prerequisites/stochastic_processes/#references","title":"References","text":"<ol> <li> <p>Lawler, Gregory F. (2006). Introduction to Stochastic Processes.  CRC Press.</p> </li> <li> <p>Durrett, Rick (2010). Essentials of Stochastic Processes.  2nd Edition, Version Beta.  Cambridge University Press.</p> </li> <li> <p>MIT OpenCourseWare (2015). Introduction to Stochastic Processes - Spring 2015. Lecture Notes - MIT OCW.</p> </li> <li> <p>Ross, Sheldon M. (2014). Stochastic Processes.  Second Edition, University of California, Berkeley.  </p> </li> <li> <p>University of Auckland (2020). COURSE NOTES: STATS 325 - Stochastic Processes.  Department of Statistics, University of Auckland.  </p> </li> </ol>"},{"location":"prerequisites/stochastic_processes/#authors","title":"Author(s)","text":"<ul> <li> <p>Abdollah Zohrabi</p> <p>Teaching Assistant</p> <p>abdollahzz1381@gmail.com</p> <p> </p> </li> </ul>"},{"location":"resources/","title":"Resources","text":"<p>Ref 1: Reinforcement Learning: An Overview by K. Murphy, 2024</p> <p>Ref 2: Reinforcement Learning: An introduction by R. Sutton and A. Barto, 2nd Edition, 2020.</p> <p>Ref 3: Deep Reinforcement Learning by A. Plaat, 2022.</p>"},{"location":"screen/","title":"Screen Records","text":"<p>Session 1: Introduction to RL- part 1</p> <p>Session 2: Introduction to RL- part 2</p>"}]}